<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="Yu-shui">
<meta property="og:url" content="http://yoursite.com/page/3/index.html">
<meta property="og:site_name" content="Yu-shui">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Yu-shui">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/3/">





  <title>Yu-shui</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
    <a href="https://github.com/Yu-shui" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yu-shui</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>
            
            日程表
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/22/简单神经网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/22/简单神经网络/" itemprop="url">简单神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-22T14:29:45+08:00">
                2019-04-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简单神经网络"><a href="#简单神经网络" class="headerlink" title="简单神经网络"></a>简单神经网络</h1><h3 id="1-文本表示"><a href="#1-文本表示" class="headerlink" title="1.文本表示"></a>1.文本表示</h3><h5 id="1-one-hot"><a href="#1-one-hot" class="headerlink" title="(1).one-hot"></a>(1).one-hot</h5><p>one-hot即独立热词，词语被表示成一个维度为词表大小的向量，这个向量中<strong>只有一个维度是1其他位置都是0</strong>.假如词表中只有四个个词“奥巴马”、“特朗普”、“宣誓”、“就职”，那么他们将被表示为：</p>
<p><img src="/2019/04/22/简单神经网络/nn1.PNG" alt=""></p>
<p>one-hot表示的优点是简单易用，但是有着致命的缺点：</p>
<ul>
<li>如果一个词表有十万个单词，那么就需要十万维的向量来表示每一个单词，而每个向量只有一位是1，在储存上造成大量浪费，在深度学习场景中容易受到维度灾难的困扰。</li>
<li>奥巴马和特朗普都是美国总统，而one-hot表示出的两个向量是正交向量，也就是他们之间毫无关系，这显然是丢失了相关语义信息。<br> 因此我们需要更好的表示方法。</li>
</ul>
<p><strong>实例例举</strong>：</p>
<p><img src="/2019/04/22/简单神经网络/nn2.PNG" alt=""></p>
<h5 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h5><p><strong>word2vec</strong>就是将词表征为实数值向量的一种高效的算法模型，其利用深度学习的思想，可以通过训练，把对文本内容的处理简化为 K 维向量空间中的向量运算，而向量空间上的相似度可以用来表示文本语义上的相似。当我们在说word2vec算法或模型的时候，其实指的是其背后用于计算word vector的CBoW模型和Skip-gram模型。很多人以为word2vec指的是一个算法或模型，这也是一种谬误。</p>
<ul>
<li><p><strong>引子</strong>：<strong>Distributed representation词向量表示</strong></p>
<p>低维实数向量。向量的距离可以用最传统的欧氏距离来衡量，也可以用 cos 夹角来衡量。它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。</p>
</li>
<li><p><strong>Word2vec</strong> 使用的词向量不是我们上述提到的One-hot Representation那种词向量，而是 Distributed representation 的词向量表示方式。其基本思想是 <strong>通过训练将每个词映射成 K 维实数向量</strong>（K 一般为模型中的超参数），通过词之间的距离（比如 cosine 相似度、欧氏距离等）来判断它们之间的语义相似度.其采用一个 <strong>三层的神经网络</strong> ，输入层-隐层-输出层。有个核心的技术是 <strong>根据词频用Huffman编码</strong> ，使得所有词频相似的词隐藏层激活的内容基本一致，出现频率越高的词语，他们激活的隐藏层数目越少，这样有效的降低了计算的复杂度。而Word2vec大受欢迎的一个原因正是其高效性。这个三层神经网络本身是 <strong>对语言模型进行建模</strong> ，但也同时 <strong>获得一种单词在向量空间上的表示</strong> ，而这个副作用才是Word2vec的真正目标。</p>
</li>
<li><p>输入是one-hot,，Hidden Layer没有激活函数，也就是线性的单元。Output Layer维度跟Input Layer的维度一样，用的是Softmax回归。当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵。</p>
</li>
</ul>
<p><strong>Word2Vec</strong>实际上是两种不同的方法：Continuous Bag of Words (CBOW) 和 Skip-gram。CBOW的目标是根据上下文来预测当前词语的概率。Skip-gram刚好相反：根据当前词语来预测上下文的概率（如下图所示）。这两种方法都利用人工神经网络作为它们的分类算法。起初，每个单词都是一个随机 N 维向量。经过训练之后，该算法利用 CBOW 或者 Skip-gram 的方法获得了每个单词的最优向量。</p>
<p><img src="/2019/04/22/简单神经网络/nn3.PNG" alt=""></p>
<p>　取一个适当大小的窗口当做语境，输入层读入窗口内的词，将它们的向量（K维，初始随机）加和在一起，形成隐藏层K个节点。输出层是一个巨大的二叉 树，叶节点代表语料里所有的词（语料含有V个独立的词，则二叉树有|V|个叶节点）。而这整颗二叉树构建的算法就是Huffman树。这样，对于叶节点的 每一个词，就会有一个全局唯一的编码，形如”010011”，不妨记左子树为1，右子树为0。接下来，隐层的每一个节点都会跟二叉树的内节点有连边，于是 对于二叉树的每一个内节点都会有K条连边，每条边上也会有权值。</p>
<p><img src="/2019/04/22/简单神经网络/nn4.PNG" alt=""></p>
<p>对于语料库中的某个词w_t，对应着二叉树的某个叶子节点，因此它必然有一个二进制编码，如”010011”。在训练阶段，当给定上下文，要预测后 面的词w_t的时候，我们就从二叉树的根节点开始遍历，这里的目标就是预测这个词的二进制编号的每一位。即对于给定的上下文，我们的目标是使得预测词的二 进制编码概率最大。形象地说，我们希望在根节点，词向量和与根节点相连经过 logistic 计算得到 bit=1 的概率尽量接近 0，在第二层，希望其 bit=1 的概率尽量接近1，这么一直下去，我们把一路上计算得到的概率相乘，即得到目标词w_t在当前网络下的概率P(w_t)，那么对于当前这个 sample的残差就是1-P(w_t)，于是就可以使用梯度下降法训练这个网络得到所有的参数值了。显而易见，按照目标词的二进制编码计算到最后的概率 值就是归一化的。</p>
<p>　　最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大,word2vec使用霍夫曼树来代替传统的DNN算法（因为DNN算法处理过程非常耗时）。 而内部节点则起到隐藏层神经元的作用。其实借助了分类问题中，使用一连串二分类近似多分类的思想。例如我们是把所有的词都作为输出，那么“桔 子”、“汽车”都是混在一起。给定w_t的上下文，先让模型判断w_t是不是名词，再判断是不是食物名，再判断是不是水果，再判断是不是“桔子”。</p>
<p>　　但是在训练过程中，模型会赋予这些抽象的中间结点一个合适的向量，这个向量代表了它对应的所有子结点。因为真正的单词公用了这些抽象结点的向量，所 以Hierarchical Softmax方法和原始问题并不是等价的，但是这种近似并不会显著带来性能上的损失同时又使得模型的求解规模显著上升。</p>
<p>　　没有使用这种二叉树，而是直接从隐层直接计算每一个输出的概率——即传统的Softmax，就需要对|V|中的每一个词都算一遍，这个过程时间复杂 度是O(|V|)的。而使用了二叉树（如Word2vec中的Huffman树），其时间复杂度就降到了O(log2(|V|))，速度大大地加快了。</p>
<p>　　现在这些词向量已经捕捉到上下文的信息。我们可以利用基本代数公式来发现单词之间的关系（比如，“国王”-“男人”+“女人”=“王后”）。这些词向量可 以代替词袋用来预测未知数据的情感状况。该模型的优点在于不仅考虑了语境信息还压缩了数据规模（通常情况下，词汇量规模大约在300个单词左右而不是之前 模型的100000个单词）。因为神经网络可以替我们提取出这些特征的信息，所以我们仅需要做很少的手动工作。但是由于文本的长度各异，我们可能需要利用 所有词向量的平均值作为分类算法的输入值，从而对整个文本文档进行分类处理。</p>
<p><strong>注：以下内容转载自<a href="https://www.cnblogs.com/pinard/p/7243513.html" target="_blank" rel="noopener">网络</a></strong></p>
<h3 id="基于Hierarchical-Softmax的模型概述"><a href="#基于Hierarchical-Softmax的模型概述" class="headerlink" title=". 基于Hierarchical Softmax的模型概述"></a>. 基于Hierarchical Softmax的模型概述</h3><p>　　　　我们先回顾下传统的神经网络词向量语言模型，里面一般有三层，输入层（词向量），隐藏层和输出层（softmax层）。里面最大的问题在于从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值。这个模型如下图所示。其中<em>V</em>V  是词汇表的大小，</p>
<p><img src="/2019/04/22/简单神经网络/nn5.PNG" alt=""></p>
<p>　　　　word2vec对这个模型做了改进，首先，对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。比如输入的是三个4维词向量：(1,2,3,4),(9,6,11,8),(5,10,7,12)(1,2,3,4),(9,6,11,8),(5,10,7,12)  ,那么我们word2vec映射后的词向量就是(5,6,7,8)(5,6,7,8)  。由于这里是从多个词向量变成了一个词向量。</p>
<p>　　　　第二个改进就是从隐藏层到输出的softmax层这里的计算量个改进。为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。我们在上一节已经介绍了霍夫曼树的原理。如何映射呢？这里就是理解word2vec的关键所在了。</p>
<p>　　　　由于我们把之前所有都要计算的从输出softmax层的概率计算变成了一颗二叉霍夫曼树，那么我们的softmax概率计算只需要沿着树形结构进行就可以了。如下图所示，我们可以沿着霍夫曼树从根节点一直走到我们的叶子节点的词<em>w</em>2w2  。</p>
<p><img src="/2019/04/22/简单神经网络/nn6.PNG" alt=""></p>
<p>　　　　和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种softmax取名为”Hierarchical Softmax”。</p>
<p>　　　　如何“沿着霍夫曼树一步步完成”呢？在word2vec中，我们采用了二元逻辑回归的方法，即规定沿着左子树走，那么就是负类(霍夫曼树编码1)，沿着右子树走，那么就是正类(霍夫曼树编码0)。判别正类和负类的方法是使用sigmoid函数，即：</p>
<p><em>P</em>(+)=<em>σ</em>(<em>x<strong>T</strong>w**θ</em>)=11+<em>e</em>−<em>x<strong>T</strong>w**θ</em>P(+)=σ(xwTθ)=11+e−xwTθ</p>
<p>　　　　其中<em>x**w</em>xw  是当前内部节点的词向量，而<em>θ</em>θ  则是我们需要从训练样本求出的逻辑回归的模型参数。</p>
<p>　　　　使用霍夫曼树有什么好处呢？首先，由于是二叉树，之前计算量为<em>V</em>V  ,现在变成了<em>l<strong>o</strong>g</em>2<em>V</em>log2V  。第二，由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到，这符合我们的贪心优化思想。</p>
<p>　　　　容易理解，被划分为左子树而成为负类的概率为<em>P</em>(−)=1−<em>P</em>(+)P(−)=1−P(+)  。在某一个内部节点，要判断是沿左子树还是右子树走的标准就是看<em>P</em>(−),<em>P</em>(+)P(−),P(+)  谁的概率值大。而控制<em>P</em>(−),<em>P</em>(+)P(−),P(+)  谁的概率值大的因素一个是当前节点的词向量，另一个是当前节点的模型参数<em>θ</em>θ  。</p>
<p>　　　　对于上图中的<em>w</em>2w2  ，如果它是一个训练样本的输出，那么我们期望对于里面的隐藏节点<em>n</em>(<em>w</em>2,1)n(w2,1)  的<em>P</em>(−)P(−)  概率大，<em>n</em>(<em>w</em>2,2)n(w2,2)  的<em>P</em>(−)P(−)  概率大，<em>n</em>(<em>w</em>2,3)n(w2,3)  的<em>P</em>(+)P(+)  概率大。</p>
<p>　　　　回到基于Hierarchical Softmax的word2vec本身，我们的目标就是找到合适的所有节点的词向量和所有内部节点<em>θ</em>θ  , 使训练样本达到最大似然。那么如何达到最大似然呢？</p>
<h3 id="2-基于Hierarchical-Softmax的模型梯度计算"><a href="#2-基于Hierarchical-Softmax的模型梯度计算" class="headerlink" title="2. 基于Hierarchical Softmax的模型梯度计算"></a>2. 基于Hierarchical Softmax的模型梯度计算</h3><p>　　　我们使用最大似然法来寻找所有节点的词向量和所有内部节点<em>θ</em>θ  。先拿上面的<em>w</em>2w2  例子来看，我们期望最大化下面的似然函数：</p>
<p>∏<em>i</em>=13<em>P</em>(<em>n</em>(<em>w**i</em>),<em>i</em>)=(1−11+<em>e</em>−<em>x<strong>T</strong>w**θ</em>1)(1−11+<em>e</em>−<em>x<strong>T</strong>w**θ</em>2)11+<em>e</em>−<em>x<strong>T</strong>w**θ</em>3∏i=13P(n(wi),i)=(1−11+e−xwTθ1)(1−11+e−xwTθ2)11+e−xwTθ3</p>
<p>　　　　对于所有的训练样本，我们期望最大化所有样本的似然函数乘积。</p>
<p>　　　　为了便于我们后面一般化的描述，我们定义输入的词为<em>w</em>w  ,其从输入层词向量求和平均后的霍夫曼树根节点词向量为<em>x**w</em>xw  , 从根节点到<em>w</em>w  所在的叶子节点，包含的节点总数为<em>l**w</em>lw  , <em>w</em>w  在霍夫曼树中从根节点开始，经过的第<em>i</em>i  个节点表示为<em>p<strong>w</strong>i</em>piw  ,对应的霍夫曼编码为<em>d<strong>w</strong>i</em>∈{0,1}diw∈{0,1}  ,其中<em>i</em>=2,3,…<em>l**w</em>i=2,3,…lw  。而该节点对应的模型参数表示为<em>θ<strong>w</strong>i</em>θiw  , 其中<em>i</em>=1,2,…<em>l**w</em>−1i=1,2,…lw−1  ，没有<em>i</em>=<em>l**w</em>i=lw  是因为模型参数仅仅针对于霍夫曼树的内部节点。</p>
<p>　　　　定义<em>w</em>w  经过的霍夫曼树某一个节点j的逻辑回归概率为<em>P</em>(<em>d<strong>w</strong>j</em>|<em>x**w</em>,<em>θ<strong>w</strong>j</em>−1)P(djw|xw,θj−1w)  ，其表达式为：</p>
<p><em>P</em>(<em>d<strong>w</strong>j</em>|<em>x**w</em>,<em>θ<strong>w</strong>j</em>−1)={<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)1−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)<em>d<strong>w</strong>j</em>=0<em>d<strong>w</strong>j</em>=1P(djw|xw,θj−1w)={σ(xwTθj−1w)djw=01−σ(xwTθj−1w)djw=1</p>
<p>　　　　那么对于某一个目标输出词<em>w</em>w  ,其最大似然为：</p>
<p>∏<em>j</em>=2<em>l<strong>w</strong>P</em>(<em>d<strong>w</strong>j</em>|<em>x**w</em>,<em>θ<strong>w</strong>j</em>−1)=∏<em>j</em>=2<em>l**w</em>[<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)]1−<em>d<strong>w</strong>j</em>[1−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)]<em>d<strong>w</strong>j</em>∏j=2lwP(djw|xw,θj−1w)=∏j=2lw[σ(xwTθj−1w)]1−djw[1−σ(xwTθj−1w)]djw</p>
<p>　　　　在word2vec中，由于使用的是随机梯度上升法，所以并没有把所有样本的似然乘起来得到真正的训练集最大似然，仅仅每次只用一个样本更新梯度，这样做的目的是减少梯度计算量。这样我们可以得到<em>w</em>w  的对数似然函数<em>L</em>L  如下：</p>
<p><em>L</em>=<em>l<strong>o</strong>g</em>∏<em>j</em>=2<em>l<strong>w</strong>P</em>(<em>d<strong>w</strong>j</em>|<em>x**w</em>,<em>θ<strong>w</strong>j</em>−1)=∑<em>j</em>=2<em>l**w</em>((1−<em>d<strong>w</strong>j</em>)<em>l<strong>o</strong>g</em>[<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)]+<em>d<strong>w</strong>j<strong>l</strong>o**g</em>[1−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)])L=log∏j=2lwP(djw|xw,θj−1w)=∑j=2lw((1−djw)log[σ(xwTθj−1w)]+djwlog[1−σ(xwTθj−1w)])</p>
<p>　　　　要得到模型中<em>w</em>w  词向量和内部节点的模型参数<em>θ</em>θ  , 我们使用梯度上升法即可。首先我们求模型参数<em>θ<strong>w</strong>j</em>−1θj−1w  的梯度：</p>
<p>∂<em>L</em>∂<em>θ<strong>w</strong>j</em>−1=(1−<em>d<strong>w</strong>j</em>)(<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)(1−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)<em>x**w</em>−<em>d<strong>w</strong>j</em>(<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)(1−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)1−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)<em>x**w</em>=(1−<em>d<strong>w</strong>j</em>)(1−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1))<em>x**w</em>−<em>d<strong>w</strong>j**σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)<em>x**w</em>=(1−<em>d<strong>w</strong>j</em>−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1))<em>x**w</em>(1)(2)(3)(1)∂L∂θj−1w=(1−djw)(σ(xwTθj−1w)(1−σ(xwTθj−1w)σ(xwTθj−1w)xw−djw(σ(xwTθj−1w)(1−σ(xwTθj−1w)1−σ(xwTθj−1w)xw(2)=(1−djw)(1−σ(xwTθj−1w))xw−djwσ(xwTθj−1w)xw(3)=(1−djw−σ(xwTθj−1w))xw</p>
<p>　　　　如果大家看过之前写的<a href="http://www.cnblogs.com/pinard/p/6029432.html" target="_blank" rel="noopener">逻辑回归原理小结</a>，会发现这里的梯度推导过程基本类似。</p>
<p>　　　　同样的方法，可以求出<em>x**w</em>xw  的梯度表达式如下：</p>
<p>∂<em>L</em>∂<em>x**w</em>=∑<em>j</em>=2<em>l**w</em>(1−<em>d<strong>w</strong>j</em>−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1))<em>θ<strong>w</strong>j</em>−1∂L∂xw=∑j=2lw(1−djw−σ(xwTθj−1w))θj−1w</p>
<p>　　　　有了梯度表达式，我们就可以用梯度上升法进行迭代来一步步的求解我们需要的所有的<em>θ<strong>w</strong>j</em>−1θj−1w  和<em>x**w</em>xw  。</p>
<h3 id="3-基于Hierarchical-Softmax的CBOW模型"><a href="#3-基于Hierarchical-Softmax的CBOW模型" class="headerlink" title="3. 基于Hierarchical Softmax的CBOW模型"></a>3. 基于Hierarchical Softmax的CBOW模型</h3><p>　　　　由于word2vec有两种模型：CBOW和Skip-Gram,我们先看看基于CBOW模型时， Hierarchical Softmax如何使用。</p>
<p>　　　　首先我们要定义词向量的维度大小<em>M</em>M  ，以及CBOW的上下文大小2<em>c</em>2c  ,这样我们对于训练样本中的每一个词，其前面的<em>c</em>c  个词和后面的<em>c</em>c  个词作为了CBOW模型的输入,该词本身作为样本的输出，期望softmax概率最大。</p>
<p>　　　　在做CBOW模型前，我们需要先将词汇表建立成一颗霍夫曼树。</p>
<p>　　　　对于从输入层到隐藏层（投影层），这一步比较简单，就是对<em>w</em>w  周围的2<em>c</em>2c  个词向量求和取平均即可，即：</p>
<p><em>x**w</em>=12<em>c</em>∑<em>i</em>=12<em>c<strong>x</strong>i</em>xw=12c∑i=12cxi</p>
<p>　　　　第二步，通过梯度上升法来更新我们的<em>θ<strong>w</strong>j</em>−1θj−1w  和<em>x**w</em>xw  ，注意这里的<em>x**w</em>xw  是由2<em>c</em>2c  个词向量相加而成，我们做梯度更新完毕后会用梯度项直接更新原始的各个<em>x**i</em>(<em>i</em>=1,2,,,,2<em>c</em>)xi(i=1,2,,,,2c)  ，即：</p>
<p><em>θ<strong>w</strong>j</em>−1=<em>θ<strong>w</strong>j</em>−1+<em>η</em>(1−<em>d<strong>w</strong>j</em>−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1))<em>x**w</em>θj−1w=θj−1w+η(1−djw−σ(xwTθj−1w))xw</p>
<p><em>x**i</em>=<em>x**i</em>+<em>η</em>∑<em>j</em>=2<em>l**w</em>(1−<em>d<strong>w</strong>j</em>−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1))<em>θ<strong>w</strong>j</em>−1(<em>i</em>=1,2..,2<em>c</em>)xi=xi+η∑j=2lw(1−djw−σ(xwTθj−1w))θj−1w(i=1,2..,2c)</p>
<p>　　　　其中<em>η</em>η  为梯度上升法的步长。</p>
<p>　　　　这里总结下基于Hierarchical Softmax的CBOW模型算法流程，梯度迭代使用了随机梯度上升法：</p>
<p>　　　　输入：基于CBOW的语料训练样本，词向量的维度大小<em>M</em>M  ，CBOW的上下文大小2<em>c</em>2c  ,步长<em>η</em>η  </p>
<p>　　　　输出：霍夫曼树的内部节点模型参数<em>θ</em>θ  ，所有的词向量<em>w</em>w  </p>
<p>　　　　1. 基于语料训练样本建立霍夫曼树。</p>
<p>　　　　2. 随机初始化所有的模型参数<em>θ</em>θ  ，所有的词向量<em>w</em>w  </p>
<p>　　　　3. 进行梯度上升迭代过程，对于训练集中的每一个样本(<em>c<strong>o</strong>n<strong>t</strong>e<strong>x</strong>t</em>(<em>w</em>),<em>w</em>)(context(w),w)  做如下处理：</p>
<p>　　　　　　a)  e=0， 计算<em>x**w</em>=12<em>c</em>∑<em>i</em>=12<em>c<strong>x</strong>i</em>xw=12c∑i=12cxi  </p>
<p>　　　　　　b)  for j = 2 to <em>l**w</em>lw  , 计算：</p>
<p><em>f</em>=<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)f=σ(xwTθj−1w)</p>
<p><em>g</em>=(1−<em>d<strong>w</strong>j</em>−<em>f</em>)<em>η</em>g=(1−djw−f)η</p>
<p><em>e</em>=<em>e</em>+<em>g<strong>θ</strong>w**j</em>−1e=e+gθj−1w</p>
<p><em>θ<strong>w</strong>j</em>−1=<em>θ<strong>w</strong>j</em>−1+<em>g<strong>x</strong>w</em>θj−1w=θj−1w+gxw</p>
<p>　　　           c) 对于<em>c<strong>o</strong>n<strong>t</strong>e<strong>x</strong>t</em>(<em>w</em>)context(w)  中的每一个词向量<em>x**i</em>xi  (共2c个)进行更新：</p>
<p><em>x**i</em>=<em>x**i</em>+<em>e</em>xi=xi+e</p>
<p>　　　　　　d) 如果梯度收敛，则结束梯度迭代，否则回到步骤3继续迭代。</p>
<h3 id="4-基于Hierarchical-Softmax的Skip-Gram模型"><a href="#4-基于Hierarchical-Softmax的Skip-Gram模型" class="headerlink" title="4. 基于Hierarchical Softmax的Skip-Gram模型"></a>4. 基于Hierarchical Softmax的Skip-Gram模型</h3><p>　　　　现在我们先看看基于Skip-Gram模型时， Hierarchical Softmax如何使用。此时输入的只有一个词<em>w</em>w  ,输出的为2<em>c</em>2c  个词向量<em>c<strong>o</strong>n<strong>t</strong>e<strong>x</strong>t</em>(<em>w</em>)context(w)  。</p>
<p>　　　　我们对于训练样本中的每一个词，该词本身作为样本的输入， 其前面的<em>c</em>c  个词和后面的<em>c</em>c  个词作为了Skip-Gram模型的输出,，期望这些词的softmax概率比其他的词大。</p>
<p>　　　　Skip-Gram模型和CBOW模型其实是反过来的，在上一篇已经讲过。</p>
<p>　　　　在做CBOW模型前，我们需要先将词汇表建立成一颗霍夫曼树。</p>
<p>　　　　对于从输入层到隐藏层（投影层），这一步比CBOW简单，由于只有一个词，所以，即<em>x**w</em>xw  就是词<em>w</em>w  对应的词向量。</p>
<p>　　　　第二步，通过梯度上升法来更新我们的<em>θ<strong>w</strong>j</em>−1θj−1w  和<em>x**w</em>xw  ，注意这里的<em>x**w</em>xw  周围有2<em>c</em>2c  个词向量，此时如果我们期望<em>P</em>(<em>x**i</em>|<em>x**w</em>),<em>i</em>=1,2…2<em>c</em>P(xi|xw),i=1,2…2c  最大。此时我们注意到由于上下文是相互的，在期望<em>P</em>(<em>x**i</em>|<em>x**w</em>),<em>i</em>=1,2…2<em>c</em>P(xi|xw),i=1,2…2c  最大化的同时，反过来我们也期望<em>P</em>(<em>x**w</em>|<em>x**i</em>),<em>i</em>=1,2…2<em>c</em>P(xw|xi),i=1,2…2c  最大。那么是使用<em>P</em>(<em>x**i</em>|<em>x**w</em>)P(xi|xw)  好还是<em>P</em>(<em>x**w</em>|<em>x**i</em>)P(xw|xi)  好呢，word2vec使用了后者，这样做的好处就是在一个迭代窗口内，我们不是只更新<em>x**w</em>xw  一个词，而是<em>x**i</em>,<em>i</em>=1,2…2<em>c</em>xi,i=1,2…2c  共2<em>c</em>2c  个词。这样整体的迭代会更加的均衡。因为这个原因，Skip-Gram模型并没有和CBOW模型一样对输入进行迭代更新，而是对2<em>c</em>2c  个输出进行迭代更新。</p>
<p>　　　　这里总结下基于Hierarchical Softmax的Skip-Gram模型算法流程，梯度迭代使用了随机梯度上升法：</p>
<p>　　　　输入：基于Skip-Gram的语料训练样本，词向量的维度大小<em>M</em>M  ，Skip-Gram的上下文大小2<em>c</em>2c  ,步长<em>η</em>η  </p>
<p>　　　　输出：霍夫曼树的内部节点模型参数<em>θ</em>θ  ，所有的词向量<em>w</em>w  </p>
<p>　　　　1. 基于语料训练样本建立霍夫曼树。</p>
<p>　　　　2. 随机初始化所有的模型参数<em>θ</em>θ  ，所有的词向量<em>w</em>w  ,</p>
<p>　　　　3. 进行梯度上升迭代过程，对于训练集中的每一个样本(<em>w</em>,<em>c<strong>o</strong>n<strong>t</strong>e<strong>x</strong>t</em>(<em>w</em>))(w,context(w))  做如下处理：</p>
<p>　　　　　　a)  for i =1 to 2c:</p>
<p>　　　　　　　　i) e=0</p>
<p>　　　　　　　　ii)for j = 2 to <em>l**w</em>lw  , 计算：</p>
<p><em>f</em>=<em>σ</em>(<em>x<strong>T</strong>i<strong>θ</strong>w**j</em>−1)f=σ(xiTθj−1w)</p>
<p><em>g</em>=(1−<em>d<strong>w</strong>j</em>−<em>f</em>)<em>η</em>g=(1−djw−f)η</p>
<p><em>e</em>=<em>e</em>+<em>g<strong>θ</strong>w**j</em>−1e=e+gθj−1w</p>
<p><em>θ<strong>w</strong>j</em>−1=<em>θ<strong>w</strong>j</em>−1+<em>g<strong>x</strong>i</em>θj−1w=θj−1w+gxi   iii) </p>
<p><em>x**i</em>=<em>x**i</em>+<em>e</em>xi=xi+e</p>
<p>　　　　　　b)如果梯度收敛，则结束梯度迭代，算法结束，否则回到步骤a继续迭代。</p>
<h3 id="5-Hierarchical-Softmax的模型源码和算法的对应"><a href="#5-Hierarchical-Softmax的模型源码和算法的对应" class="headerlink" title="5. Hierarchical Softmax的模型源码和算法的对应　　　　"></a>5. Hierarchical Softmax的模型源码和算法的对应　　　　</h3><p>　　　　这里给出上面算法和<a href="https://github.com/tmikolov/word2vec/blob/master/word2vec.c" target="_blank" rel="noopener">word2vec源码</a>中的变量对应关系。</p>
<p>　　　　在源代码中，基于Hierarchical Softmax的CBOW模型算法在435-463行，基于Hierarchical Softmax的Skip-Gram的模型算法在495-519行。大家可以对着源代码再深入研究下算法。</p>
<p>　　　　在源代码中，neule对应我们上面的<em>e</em>e  , syn0对应我们的<em>x**w</em>xw  , syn1对应我们的<em>θ<strong>i</strong>j</em>−1θj−1i  , layer1_size对应词向量的维度，window对应我们的<em>c</em>c  。</p>
<p>　　　　另外，vocab[word].code[d]指的是，当前单词word的，第d个编码，编码不含Root结点。vocab[word].point[d]指的是，当前单词word，第d个编码下，前置的结点。</p>
<p>　　</p>
<p>　</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/21/神经网络基础/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/21/神经网络基础/" itemprop="url">神经网络基础</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-21T11:20:09+08:00">
                2019-04-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h1><h3 id="1-神经网络模型"><a href="#1-神经网络模型" class="headerlink" title="1.神经网络模型"></a>1.神经网络模型</h3><p>所谓神经网络就是将许多个单一“神经元”联结在一起，这样，一个“神经元”的输出就可以是另一个“神经元”的输入。例如，下图就是一个简单的神经网络： </p>
<p><img src="/2019/04/21/神经网络基础/nn1.PNG" alt=""></p>
<p>我们使用圆圈来表示神经网络的输入，标上“+1”的圆圈被称为”’偏置节点”’，也就是截距项。神经网络最左边的一层叫做 <strong>”‘输入层”’</strong>，最右的一层叫做<strong>”‘输出层’”</strong>（本例中，输出层只有一个节点）。中间所有节点组成的一层叫做<strong>”’隐藏层”’</strong>，因为我们不能在训练样本集中观测到它们的值。同时可以看到，以上神经网络的例子中有3个”’输入单元”’（偏置单元不计在内），3个”’隐藏单元”’及一个”’输出单元”’。</p>
<p>我们用nl 来表示网络的层数，本例中nl=3，我们将第l 层记为Ll ，于是L1是输入层，输出层Lni 。本例神经网络有参数(W,b)=(W(1),b(1),W(2),b(2)) ，其中W(l)ij（下面的式子中用到）是第l层第j 单元与第l+1 层第i 单元之间的联接参数（其实就是连接线上的权重，注意标号顺序），bi(l) 是第l+1 层第i 单元的偏置项。因此在本例中，W(1)∈ℜ3×3 ，W(2)∈ℜ1×3 。注意，没有其他单元连向偏置单元(即偏置单元没有输入)，因为它们总是输出+1。同时，我们用sl表示第ll 层的节点数（偏置单元不计在内）。</p>
<p>本例神经网络的计算步骤如下:</p>
<p><img src="/2019/04/21/神经网络基础/nn2.PNG" alt=""></p>
<p>我们用<em>z</em>(<em>l</em>)<em>i</em> 表示第<em>l</em> 层第<em>i</em>单元输入加权和（包括偏置单元）:</p>
<p><img src="/2019/04/21/神经网络基础/nn3.PNG" alt=""></p>
<p>再将激活函数进行广播,及可以得到前向传播过程：</p>
<p><img src="/2019/04/21/神经网络基础/nn4.PNG" alt=""></p>
<p>当然神经网络也可以具有多个隐藏层，其计算原理同之前类似。</p>
<p><img src="/2019/04/21/神经网络基础/nn5.PNG" alt=""></p>
<p>如果你想预测的输出是多个的，那这种神经网络很适用。（比如，在医疗诊断应用中，患者的体征指标就可以作为向量的输入值，而不同的输出值<em>y**i</em>yi 可以表示不同的疾病存在与否。）</p>
<h3 id="2-前馈神经网络-FF"><a href="#2-前馈神经网络-FF" class="headerlink" title="2.前馈神经网络(FF)"></a>2.前馈神经网络(FF)</h3><p><img src="/2019/04/21/神经网络基础/nn6.PNG" alt=""></p>
<p><strong>前馈神经网络（FF）</strong>，这是一个很古老的方法——这种方法起源于50年代。它的工作原理通常遵循以下规则：</p>
<p>1.所有节点都完全连接</p>
<p>2.激活从输入层流向输出，无回环</p>
<p>3.输入和输出之间有一层（隐含层）</p>
<p>在大多数情况下，这种类型的网络使用<strong>反向传播方法</strong>进行训练。</p>
<h3 id="3-感知机"><a href="#3-感知机" class="headerlink" title="3.感知机"></a>3.感知机</h3><h5 id="1-算法原理"><a href="#1-算法原理" class="headerlink" title="1.算法原理"></a>1.算法原理</h5><p>想法来自生物学的神经元的一些工作方式，多个生物信号 (input singals) 到达树突 (dentrites)并进入细胞核 (cell nucleus)，如果这些信号的效果累加达到一个阈值，那么通过轴突 (axon) 产生一个输出信号 (output signals)。在有监督学习与分类的背景下，这样的算法可被用来预测一个样本是否属于某个类别。</p>
<p><img src="/2019/04/21/神经网络基础/nn7.PNG" alt=""></p>
<p>正式地，我们可以把这个问题表述为一个二分类任务，并且为了简单起见，将这两个类别分别定义为 1 (正类) 与 -1（负类）。接着定义一个激活函数 (activation function) ,, 它输入的是 xx 与其对应的权重向量 ww 的一个线性组合.</p>
<p>对于一个指定样本 x(i)x(i), 如果 ϕ(z)ϕ(z) 的输出值大于预先定义的一个阈值 Θ, 那么就预测其类别 1. 否则，预测为类别 -1. 在感知器算法中，激活函数 是一个简单的单位阶跃函数 (unit step function)</p>
<p>需要注意一点的是只有当两个类别是<strong>线性可分</strong>时，感知器算法才能保证收敛。如果两个类别不是线性可分，那么我们可以在训练集上设置一个最大的迭代次数, 或是设置一个可接受的错误分类的阈值。</p>
<h5 id="2-感知机权重的学习过程"><a href="#2-感知机权重的学习过程" class="headerlink" title="2.感知机权重的学习过程"></a>2.感知机权重的学习过程</h5><p>采用随机梯度下降法：<br><img src="/2019/04/21/神经网络基础/nn8.PNG" alt=""></p>
<h3 id="推荐一个神经网络可视化的网站-tensorflow-游乐场"><a href="#推荐一个神经网络可视化的网站-tensorflow-游乐场" class="headerlink" title="推荐一个神经网络可视化的网站 :tensorflow 游乐场"></a>推荐一个神经网络可视化的网站 :<a href="https://www.jianshu.com/p/191d1e21f7ed" target="_blank" rel="noopener">tensorflow 游乐场</a></h3><h3 id="3-使用tensoeflow定义几层简单的神经网络"><a href="#3-使用tensoeflow定义几层简单的神经网络" class="headerlink" title="3.使用tensoeflow定义几层简单的神经网络"></a>3.使用tensoeflow定义几层简单的神经网络</h3><p>激活函数使用sigmoid,递归使用链式法则来实现反向传播:</p>
<ol>
<li><p>再次运用的时简单类定义进行神经网络的计算。</p>
</li>
<li><p>tensorflow版本待更</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import math</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">def rand(a,b):</span><br><span class="line">    return (b-a)*random.random()+a</span><br><span class="line"></span><br><span class="line">def make_matrix(m,n,fill=0.0):</span><br><span class="line">    mat=[]</span><br><span class="line">    for i in range(m):</span><br><span class="line">        mat.append([fill]*n)</span><br><span class="line">    return mat</span><br><span class="line"></span><br><span class="line">def sigmoid(x):</span><br><span class="line">    return 1.0/(1.0+math.exp(-x));</span><br><span class="line"></span><br><span class="line">def sigmod_derivate(x):</span><br><span class="line">    return x*(1-x)</span><br><span class="line"></span><br><span class="line">class BPNeuralNetWork:</span><br><span class="line">    </span><br><span class="line">    def _init_(self):</span><br><span class="line">        self.input_n=0</span><br><span class="line">        self.hidden_n=0</span><br><span class="line">        self.output_n=0</span><br><span class="line">        self.input_cells=[]</span><br><span class="line">        self.hidden_cells=[]</span><br><span class="line">        self.output_cells=[]</span><br><span class="line">        self.input_weights=[]</span><br><span class="line">        self.output_weights=[]</span><br><span class="line">        </span><br><span class="line">    def setup(self,ni,nh,no):</span><br><span class="line">        self.input_n=ni+1</span><br><span class="line">        self.hidden_n=nh</span><br><span class="line">        self.output_n=no</span><br><span class="line">        </span><br><span class="line">        self.input_cells=[1.0]*self.input_n</span><br><span class="line">        self.hidden_cells=[1.0]*self.hidden_n</span><br><span class="line">        self.output_cells=[1.0]*self.output_n</span><br><span class="line">        </span><br><span class="line">        self.input_weights=make_matrix(self.input_n,self.hidden_n)</span><br><span class="line">        self.output_weights=make_matrix(self.hidden_n,self.output_n)</span><br><span class="line">        for i in range(self.input_n):</span><br><span class="line">            for h in range(self.hidden_n):</span><br><span class="line">                self.input_weights[i][h]=rand(-0.2,0.2)</span><br><span class="line">        for h in range(self.hidden_n):</span><br><span class="line">            for j in range(self.output_n):</span><br><span class="line">                self.output_weights[h][j]=rand(-0.2,0.2)</span><br><span class="line"></span><br><span class="line">    def predict(self,inputs):</span><br><span class="line">        for i in range(self.input_n-1.0):</span><br><span class="line">            self.input_cells[i]=inputs[i]</span><br><span class="line">            </span><br><span class="line">        for j in range(self.hidden_n):</span><br><span class="line">            total+=self.input_cells[i]*self.input_weights[i][j]</span><br><span class="line">        self.hidden_cells[i]=sigmoid(total)#不要忘记去线性化</span><br><span class="line">        </span><br><span class="line">        for k in range(self.output_n):</span><br><span class="line">            total=0.0</span><br><span class="line">            for j in range(self.output_n):</span><br><span class="line">                total+=self.hidden_cells[j]*self.output_weights[j][k]</span><br><span class="line">                self.output_cells[k]=sigmoid(total)</span><br><span class="line">        return self.output_cells[:]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    def back_propagate(self,case,label,learn):</span><br><span class="line">        </span><br><span class="line">        self.predict(case)</span><br><span class="line">        </span><br><span class="line">        output_deltas=[0.0]*self.output_n</span><br><span class="line">        for k in range(self.output_n):</span><br><span class="line">            error=label[k]-self.output_cells[k]</span><br><span class="line">            output_deltas[k]=sigmod_derivate(self.output_cells[k])*error</span><br><span class="line">            </span><br><span class="line">        hidden_deltas=[0.0]*self.hidden_n</span><br><span class="line">        for j in range(self.input_n):</span><br><span class="line">            error=0.0</span><br><span class="line">            for k in range(self.output_n):</span><br><span class="line">                error+=output_deltas[k]*self.output_weights[j][k]</span><br><span class="line">            hidden_deltas[j]=sigmod_derivate(self.hidden_cells[j])*error</span><br><span class="line">            </span><br><span class="line">        for j in range(self.hidden_n):</span><br><span class="line">            for k in range(self.output_n):</span><br><span class="line">                self.output_weights[j][k]+=learn*output_deltas[k]*self.hidden_cells[j]</span><br><span class="line">        </span><br><span class="line">        for i in range(self.input_n):</span><br><span class="line">            for j in range(self.hidden_n):</span><br><span class="line">                self.input_weights[i][j]=learn*hidden_deltas[j]*self.input_cells[i]</span><br><span class="line">                </span><br><span class="line">                error=0</span><br><span class="line">        </span><br><span class="line">        for o in range(len(label)):</span><br><span class="line">            #L1实现</span><br><span class="line">            error+=0.5*(label[o]-self.output_cells[o])**2</span><br><span class="line">        return error</span><br><span class="line">    </span><br><span class="line">    def train(self,cases,labels,limit=100,learn=0.05):</span><br><span class="line">        for i in range(limit):</span><br><span class="line">            error=0</span><br><span class="line">            for i in range(len(cases)):</span><br><span class="line">                label=labels[i]</span><br><span class="line">                case=cases[i]</span><br><span class="line">                error+=self.back_propagate(case,label,learn)</span><br><span class="line">            pass</span><br><span class="line">        </span><br><span class="line">    def test(self):</span><br><span class="line">        cases=[</span><br><span class="line">            [0,0],</span><br><span class="line">            [0,1],</span><br><span class="line">            [1,0],</span><br><span class="line">            [1,1],</span><br><span class="line">        ]</span><br><span class="line">        labels=[[0],[1],[1],[0]]</span><br><span class="line">        dself.setup(2,5,1)</span><br><span class="line">        self.train(cases,labels,10000,0.05)</span><br><span class="line">        for case in cases:</span><br><span class="line">            print(self.predict(case))</span><br><span class="line">            </span><br><span class="line">if __name__==&apos;__mian__&apos;:</span><br><span class="line">    nn=BPNeuralNetWork()</span><br><span class="line">    nn.test()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="4-激活函数的种类以及各自的背景，优缺点"><a href="#4-激活函数的种类以及各自的背景，优缺点" class="headerlink" title="4.激活函数的种类以及各自的背景，优缺点"></a>4.激活函数的种类以及各自的背景，优缺点</h3><h5 id="1-定义"><a href="#1-定义" class="headerlink" title="1,定义"></a>1,定义</h5><p>在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数 Activation Function。</p>
<p>使用激活函数对线性值进行去线性化。</p>
<h5 id="2-为什么要使用激活函数-线性模型的局限性"><a href="#2-为什么要使用激活函数-线性模型的局限性" class="headerlink" title="2.为什么要使用激活函数(线性模型的局限性)"></a>2.为什么要使用激活函数(线性模型的局限性)</h5><p>如果不用激励函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中</p>
<h5 id="3-分类定义"><a href="#3-分类定义" class="headerlink" title="3.分类定义"></a>3.分类定义</h5><h6 id="1-sigmoid函数"><a href="#1-sigmoid函数" class="headerlink" title="(1).sigmoid函数"></a>(1).sigmoid函数</h6><p>公式：<br><img src="/2019/04/21/神经网络基础/nn10.PNG" alt=""></p>
<p>曲线:</p>
<p><img src="/2019/04/21/神经网络基础/nn11.PNG" alt=""></p>
<p>取值范围为(0,1)，用于<strong>隐层神经元输出</strong>。<br>它可以将一个实数映射到(0,1)的区间，可以用来做二分类。<br><strong>在特征相差比较复杂或是相差不是特别大时效果比较好。</strong><br><strong>sigmoid缺点</strong>：</p>
<p>激活函数计算量大，反向传播求误差梯度时，求导涉及除法。反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。</p>
<h4 id="2-Tanh函数"><a href="#2-Tanh函数" class="headerlink" title="(2) Tanh函数"></a>(2) Tanh函数</h4><p>公式：</p>
<p><img src="/2019/04/21/神经网络基础/nn12.PNG" alt=""></p>
<p>曲线:</p>
<p><img src="/2019/04/21/神经网络基础/nn13.PNG" alt=""></p>
<p>也称为双切正切函数<br>取值范围为[-1,1]。<br>tanh在特征相差明显时的效果会很好，在循环过程中会不断扩大特征果。与 sigmoid 的区别是，tanh 是 0 均值的，因此实际应用中 tanh 会比 sigmoid 更好。</p>
<h4 id="3-ReLU"><a href="#3-ReLU" class="headerlink" title="3) ReLU"></a>3) ReLU</h4><p>Rectified Linear Unit(ReLU) - 用于隐层神经元输出</p>
<p>公式：</p>
<p><img src="/2019/04/21/神经网络基础/nn14.PNG" alt=""></p>
<p>曲线</p>
<p><img src="/2019/04/21/神经网络基础/nn15.PNG" alt=""></p>
<p>输入信号 <0 时，输出都是0，="">0 的情况下，输出等于输入</0></p>
<p><strong>ReLU 的优点</strong>：使用 ReLU 得到的 SGD 的收敛速度会比 sigmoid/tanh 快很多</p>
<p><strong>ReLU 的缺点</strong>：<br> 训练的时候很”脆弱”，很容易就”die”了<br> 例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.<br> 如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。</p>
<h3 id="5-深度学习中的正则化"><a href="#5-深度学习中的正则化" class="headerlink" title="5.深度学习中的正则化"></a>5.深度学习中的正则化</h3><h5 id="1-定义-1"><a href="#1-定义-1" class="headerlink" title="1.定义"></a>1.定义</h5><p>《统计学习方法》中认为正则化是<strong>选择模型</strong>的一种方法。</p>
<p>正则化通过对学习算法的修改，旨在减少泛化误差而不是训练误差。目前有很多正则化策略，有些是向机器学习模型中添加限制参数值的额外约束，有些是向目标函数添加额外项来对参数值进行软约束。</p>
<h5 id="2-L1正则化"><a href="#2-L1正则化" class="headerlink" title="2.L1正则化"></a>2.L1正则化</h5><p>L1正则化对梯度的影响不是线性地缩放每个wi而是添加了一项与sign(wi)同号的常数。这种形式的梯度不一定能得到直接算术解。 </p>
<p><img src="/2019/04/21/神经网络基础/nn16.PNG" alt=""></p>
<p>相比L2正则化，L1正则化会产生更稀疏的解。这种稀疏性广泛用于特征选择机制，可以从可用的特征子集中选择出有意义的特征，化简机器学习问题。</p>
<p><strong>L1正则化可通过假设权重w的先验分布为拉普拉斯分布，由最大后验概率估计导出. </strong></p>
<h5 id="3-L2正则化"><a href="#3-L2正则化" class="headerlink" title="3.L2正则化"></a>3.L2正则化</h5><p>通过向目标函数添加一个L2范数平方项，使权重更加接近原点。<br><img src="/2019/04/21/神经网络基础/C:/blog\source\_posts\神经网络基础\nn17.PNG" alt=""></p>
<p>《Deep Learning》书中也提到：<strong>L2正则化能让学习算法“感知”到具有较高方差的输入xx，因此与输出目标的协方差较小的特征的权重将会收缩。</strong>L2正则化能够很好的防止过拟合：</p>
<ol>
<li>通过引入L2正则，使模型参数偏好比较小的值，这其实也限制的函数空间的大小，有针对的减小了模型容量。一般来说，大的参数值对应于波动剧烈的函数，小的参数值对应于比较平缓的参数，因为小参数对于输入的改变不会那么敏感。发生过拟合往往是因为顾及到了所有样本点，所以此时的函数波动会比较大，如下面右图所示。过拟合的模型往往具有比较大的参数，如果将这部分模型族从假设空间中剔除掉，发生过拟合的可能就变小。</li>
</ol>
<p>​       <img src="/2019/04/21/神经网络基础/C:/blog\source\_posts\神经网络基础\nn18.PNG" alt=""></p>
<ol start="2">
<li>L2正则化能让学习算法“感知”到具有较高方差的输入xx，因此与输出目标的协方差较小的特征的权重将会收缩。因此L2正则化总是倾向于对那些训练集样本共用的特征产生较大的响应，而减小对个别样本独有的特征产生的响应。因此L2正则有抑制那些“独有特征”的作用，这在一定程度上也减小了过拟合的风险。<br>　　<strong>L2正则化可通过假设权重ww的先验分布为高斯分布，由最大后验概率估计导出.</strong></li>
</ol>
<h5 id="4-数据集增强"><a href="#4-数据集增强" class="headerlink" title="4.数据集增强"></a>4.数据集增强</h5><ul>
<li>原因：一般而言，比较成功的神经网络需要大量的参数，许许多多的神经网路的参数都是数以百万计，而使得这些参数可以正确工作则需要大量的数据进行训练，而实际情况中数据并没有我们想象中的那么多</li>
<li>作用：<ol>
<li>增加训练的数据量，提高模型的泛化能力</li>
<li>增加噪声数据，提升模型的鲁棒性</li>
</ol>
</li>
<li>方法：<ol>
<li>增加数据集，一般较难实现</li>
<li>利用已有的数据比如翻转、平移或旋转，创造出更多的数据，来使得神经网络具有更好的泛化效果。</li>
</ol>
</li>
<li>分类：<ol>
<li><strong>离线增强</strong> ： 直接对数据集进行处理，数据的数目会变成增强因子 x 原数据集的数目 ，这种方法常常用于数据集很小的时候</li>
<li><strong>在线增强</strong> ： 这种增强的方法用于，获得 batch 数据之后，然后对这个 batch 的数据进行增强，如旋转、平移、翻折等相应的变化，由于有些数据集不能接受线性级别的增长，这种方法长用于大的数据集，很多机器学习框架已经支持了这种数据增强方式，并且可以使用 GPU 优化计算。</li>
</ol>
</li>
</ul>
<h5 id="5-噪声增加"><a href="#5-噪声增加" class="headerlink" title="5.噪声增加"></a>5.噪声增加</h5><p>​    简单提高网络抗噪能力的方法，就是在训练中加入随机噪声一起训练。我们可以在网络的不同位置加入噪声：输入层，隐藏层，输出层。<br>　　数据集增强在某种意义上也能看做是在输入层加入噪声，通过随机旋转、翻转，色彩变换，裁剪等操作人工扩充训练集大小。这样可以使得网络对于输入更加鲁棒。<br>　　当然如果你能保证数据集没错误，不向输出层加入噪声也没关系。解决这个问题常见的办法是<strong>标签平滑</strong>，通过把确切的分类目标从0和1替换成ϵk−1和1−ϵ，正则化具有k个输出的softmax函数的模型。标签平滑的优势是能够防止模型追求确切的概率而不能学习正确分类。<br>　　从优化过程的角度来看，对权重叠加方差极小噪声等价于对权重是假范数惩罚，可以被解释为关于权重的贝叶斯推断的随即实现（这一点我不是很理解）。换个角度就很结论就很明朗了，因为我们在权重中添加了一些随机扰动，这鼓励优化过程找到一个参数空间，该空间对微小参数变化引起的输出变化影像很小。将模型送入了一个对于微小变化不敏感的区域，不仅找到了最小值，还找到了一个宽扁的最小值区域。</p>
<h5 id="early-stop（提前终止"><a href="#early-stop（提前终止" class="headerlink" title="early stop（提前终止)"></a>early stop（提前终止)</h5><p>模型过拟合一般是发生在训练次数过多的情况下，那么只要我们在过拟合之前停止训练即可。这也是深度学习中最常用的正则化形式，主要是因为它的有效性和简单性。提前终止需要验证集损失作为观测指标，当验证集损失开始持续上升时，这时就该停止训练过程了。 </p>
<h5 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h5><p>先介绍集成学习的概念</p>
<ul>
<li><p>集成学习通过结合几个模型降低泛化误差的技术。分别训练几个不同的模型，然后让所有模型表决测试样例的输出，也被称为模型平均。模型平均奏效的原因是不同的模型通常不会再测试集上产生完全相同的误差。 </p>
<p>  假设我们有k个回归模型，每个模型在每个例子上的误差为ϵi，这个误差服从均值为0，方差E[ϵi^2]=v且协方差E[ϵiϵj]=c的多维正态分布。通过所有集成模型的平均预测所得误差是1k∑iϵi，则该集成预测期的平方误差的期望为：</p>
<p>  <img src="/2019/04/21/神经网络基础/C:/blog\source\_posts\神经网络基础\nn19.PNG" alt=""></p>
</li>
</ul>
<p>​      在误差完全相关即c=v的情况下，E[(1/k*∑iϵi)^2]=v，模型平均对提升结果没有任何帮       助；在误差完全不想关即c=0的情况下，集成模型的误差期望为E[(1/k∑iϵi)^2]=(1/k)v。用一  句话说，平均上，集成至少与它任何成员表现的一样好，并且如果成员误差是独立的，集成将显著地比其成员表现的更好。神经网络中随机初始化的差异、小批量的随机选择、超参数的差异或不同输出的非确定性实现往往足以使得集成中的不同成员具有部分独立的误差。 </p>
<ul>
<li><p>Dropout</p>
<p>Dropout是每次训练过程中随机舍弃一些神经元之间的连接</p>
<p><img src="/2019/04/21/神经网络基础/C:/blog\source\_posts\神经网络基础\nn20.PNG" alt=""></p>
</li>
</ul>
<p>Dropout提供了一种廉价的Bagging集成近似，能够训练和评估指数级数量的神经网络。Dropout和Bagging训练不太一样，Bagging所有模型都是独立的，而Dropout是所有模型共享参数，每个模型集成父神经网络参数的不同子集，这中参数共享的方式使得在有限可用内存下表示指数级数量的模型变得可能。<br>　　隐藏单元经过Dropout训练后，它必须学习与不同采样神经元的合作，使得神经元具有更强的健壮性，并驱使神经元通过自身获取到有用的特征，而不是依赖其他神经元去纠正自身的错误。这可以看错对输入内容的信息高度智能化，自适应破坏的一种形式，而不是对输入原始值的破坏。<br>　　Dropout的另一个重要方面是噪声是乘性的。如果是固定规模的加性噪声，那么加了噪声ϵ的ReLU可以简单的学会使hi变得很大（使增加的噪声ϵϵ变得不显著）。乘性噪声就不允许这样病态的去解决噪声鲁棒问题。</p>
<h3 id="深度模型的优化"><a href="#深度模型的优化" class="headerlink" title="深度模型的优化"></a>深度模型的优化</h3><h5 id="1-参数初始化策略"><a href="#1-参数初始化策略" class="headerlink" title="1.参数初始化策略"></a>1.参数初始化策略</h5><p>1.梯度下降算法</p>
<ul>
<li><p>批量梯度下降<strong>(Batch gradient descent)</strong> </p>
<p>批量梯度下降每次学习都使用整个训练集，因此其优点在于每次更新都会朝着正确的方向进行，最后能够保证收敛于极值点(凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点)，但是其缺点在于每次学习时间过长，并且如果训练集很大以至于需要消耗大量的内存，并且全量梯度下降不能进行在线模型参数更新。</p>
</li>
<li><p>随机梯度下降<strong>(Stochastic gradient descent)</strong></p>
<p>随机梯度下降算法每次从训练集中随机选择一个样本来进行学习，即： θ=θ−η⋅∇θJ(θ;xi;yi)</p>
<p>批量梯度下降算法每次都会使用全部训练样本，因此这些计算是冗余的，因为每次都使用完全相同的样本集。而随机梯度下降算法每次只随机选择一个样本来更新模型参数，因此每次的学习是非常快速的，并且可以进行在线更新。</p>
<p>随机梯度下降最大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来优化波动(扰动)</p>
<p><img src="/2019/04/21/神经网络基础/n27.PNG" alt=""></p>
</li>
<li><p>小批量梯度下降<strong>(Mini-batch gradient descent)</strong></p>
<p>Mini-batch 梯度下降综合了 batch 梯度下降与 stochastic 梯度下降，在每次更新速度与更新次数中间取得一个平衡，其每次更新从训练集中随机选择 m,m&lt;n 个样本进行学习，mini-batch梯度下降可以保证收敛性，常用于神经网络中。</p>
</li>
</ul>
<p>以下摘抄自网络:<a href="https://blog.csdn.net/qq_29462849/article/details/80626772" target="_blank" rel="noopener">参数初始化策略</a><br><strong>2.AdaGrad</strong></p>
<p>AdaGrad 算法，如下图所示，独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和的平方根 (Duchi et al., 2011)。具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。在凸优化背景中， AdaGrad 算法具有一些令人满意的理论性质。然而，经验上已经发现，对于训练深度神经网络模型而言， 从训练开始时积累梯度平方会导致有效学习率过早和过量的减小。 AdaGrad 在某些深度学习模型上效果不错，但不是全部</p>
<p><img src="/2019/04/21/神经网络基础/nn23.PNG" alt=""></p>
<p><strong>1.RMSProp</strong></p>
<p>RMSProp 算法 (Hinton, 2012) 修改 AdaGrad 以在非凸设定下效果更好，改<br>变梯度积累为指数加权的移动平均。 AdaGrad 旨在应用于凸问题时快速收敛。当应用于非凸函数训练神经网络时，学习轨迹可能穿过了很多不同的结构，最终到达一个局部是凸碗的区域。 AdaGrad 根据平方梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就变得太小了。 RMSProp 使用指数衰减平均以丢弃遥远过去的历史，使其能够在找到凸碗状结构后快速收敛，它就像一个初始化于该碗状结构的 AdaGrad 算法实例。RMSProp 的标准形式如算法 8.5 所示，结合 Nesterov 动量的形式如算法 8.6 所示。相比于 AdaGrad，使用移动平均引入了一个新的超参数ρ，用来控制移动平均的长度范围。经验上， RMSProp 已被证明是一种有效且实用的深度神经网络优化算法。目前它是深度学习从业者经常采用的优化方法之一。<br><img src="/2019/04/21/神经网络基础/n24.PNG" alt=""></p>
<p><img src="/2019/04/21/神经网络基础/n23.PNG" alt=""></p>
<p><strong>4.Adam</strong></p>
<p>Adam (Kingma and Ba, 2014) 是另一种学习率自适应的优化算法，如算法 8.7 所示。 “Adam’’ 这个名字派生自短语 “adaptive moments’’。早期算法背景下，它也许最好被看作结合 RMSProp 和具有一些重要区别的动量的变种。首先，在 Adam 中，动量直接并入了梯度一阶矩（指数加权）的估计。将动量加入 RMSProp 最直观的方法是将动量应用于缩放后的梯度。结合缩放的动量使用没有明确的理论动机。其次， Adam 包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩的估计（算法 8.7 ）。 RMSProp 也采用了（非中心的）二阶矩估计，然而缺失了修正因子。因此，不像 Adam，RMSProp 二阶矩估计可能在训练初期有很高的偏置。Adam 通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。<br><img src="/2019/04/21/神经网络基础/n26.PNG" alt=""></p>
<h5 id="2-batch-normal层"><a href="#2-batch-normal层" class="headerlink" title="2.batch normal层"></a>2.batch normal层</h5><h5 id="3-layer-normal层"><a href="#3-layer-normal层" class="headerlink" title="3.layer normal层"></a>3.layer normal层</h5>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/17/传统机器学习-LDA/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/17/传统机器学习-LDA/" itemprop="url">传统机器学习--LDA</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-17T23:02:46+08:00">
                2019-04-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1-共轭先验分布"><a href="#1-共轭先验分布" class="headerlink" title="1.共轭先验分布"></a>1.共轭先验分布</h3><p>共轭先验分布的提出：某观测数据服从概率分布p(θ），当观测到新的数据时，思考下列问题：</p>
<blockquote>
<p>1.能否根据新观测数据X更新参数θ；<br>2.根据新观测的数据可以在多大的程度上改变参数θ：θ=θ+rθ；<br>3.当重新估计得到θ时，给出的新参数数值θ的新概率分布p(θ|x)；</p>
</blockquote>
<p>分析：根据贝叶斯公式：p(θ|x)=p(x|θ)p(θ)<br>p(x),其中p(x|θ)是在已知θ的情况下估计x的概率分布，又称似然函数；p(θ)是原有的θ的概率分布；要想利用观测到的数据更新参数θ，就要使更新后的p(θ|x)和p(θ)服从相同的分布，所以p(θ)和p(θ|x)形成共轭分布，p(θ)叫做p(θ|x)的共轭先验分布。<br>举个投硬币的例子：使用参数θ的伯努利模型，θ为正面的概率，则结果为x的概率分布为：p(x|θ)=θx(1−θ)1-x</p>
<p>在贝叶斯概率理论中，如果后验概率P(θ|X)和先验概率P(θ)满足同样的分布律(形式相同，参数不同)。那么，先验分布和后验分布被叫做共轭分布，同时，先验分布叫做似然函数的共轭先验分布。</p>
<p><img src="/2019/04/17/传统机器学习-LDA/kk.PNG" alt=""></p>
<h3 id="2-pLSA"><a href="#2-pLSA" class="headerlink" title="2.pLSA"></a>2.pLSA</h3><p><a href="https://baike.baidu.com/item/LSA" target="_blank" rel="noopener"><strong>LSA</strong></a>是处理这类问题的著名技术。其主要思想就是映射高维向量到潜在语义空间，使其降维。LSA的目标就是要寻找到能够很好解决实体间词法和语义关系的数据映射。正是由于这些特性，使得LSA成为相当有价值并被广泛应用的分析工具。PLSA是以统计学的角度来看待<a href="https://baike.baidu.com/item/LSA" target="_blank" rel="noopener">LSA</a>，相比于标准的LSA，他的概率学变种有着更巨大的影响。</p>
<p>概率潜在语义分析（pLSA）  基于双模式和共现的数据分析方法延伸的经典的统计学方法。 </p>
<p>概率潜在语义分析应用于信息检索，过滤，自然语言处理，文本的机器学习或者其他相关领域。概率潜在语义分析与标准潜在语义分析的不同是，标准潜在语义分析是以共现表（就是共现的矩阵）的奇异值分解的形式表现的，而概率潜在语义分析却是基于派生自LCM的混合矩阵分解。考虑到word和doc共现形式，概率潜在语义分析基于多项式分布和条件分布的混合来建模共现的概率。所谓共现其实就是W和D的一个矩阵，所谓双模式就是在W和D上同时进行考虑。</p>
<p>PLSA有时会出现过拟合的现象。所谓过拟合（Overfit），是这样一种现象：一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集上却不能很好的拟合数据。此时我们就叫这个假设出现了overfit的现象。出现这种现象的主要原因是训练数据中存在噪音或者训练数据太少。</p>
<p>解决办法，要避免过拟合的问题，PLSA使用了一种广泛应用的最大似然估计的方法，期望最大化。PLSA中训练参数的值会随着文档的数目线性递增。PLSA可以生成其所在数据集的的文档的模型，但却不能生成新文档的模型。</p>
<p>数学推导待补充（看不懂…..)</p>
<h3 id="3-LDA主题模型原理"><a href="#3-LDA主题模型原理" class="headerlink" title="3.LDA主题模型原理"></a>3.LDA主题模型原理</h3><p>待补充（看不懂…..)</p>
<h3 id="4-LDA参数学习–Gibbs采样训练流程"><a href="#4-LDA参数学习–Gibbs采样训练流程" class="headerlink" title="4.LDA参数学习–Gibbs采样训练流程"></a>4.LDA参数学习–Gibbs采样训练流程</h3><ol>
<li>选择合适的主题数K，选择合适的超参数α、η；</li>
<li>对于语料库中每一篇文档的每一个词，随机的赋予一个主题编号z；</li>
<li>重新扫描语料库，对于每一个词，利用Gibbs采样公式更新它的topic的编号，并更新语料库中该词的编号。</li>
<li>重复第三步中基于坐标轴轮询的Gibbs采样，直到Gibbs采样收敛。</li>
<li>统计语料库中各个文档各个词的主题，得到文档主题分布</li>
</ol>
<h3 id="5-LDA生成主题特征，在之前的特征的基础上加入主题特征实现文本分类"><a href="#5-LDA生成主题特征，在之前的特征的基础上加入主题特征实现文本分类" class="headerlink" title="5.LDA生成主题特征，在之前的特征的基础上加入主题特征实现文本分类"></a>5.LDA生成主题特征，在之前的特征的基础上加入主题特征实现文本分类</h3><h5 id="1-先来了解数据集"><a href="#1-先来了解数据集" class="headerlink" title="1.先来了解数据集"></a>1.先来了解数据集</h5><p>数据集位于lda安装目录的tests文件夹中，包含三个文件：reuters.ldac, reuters.titles, reuters.tokens。<br>reuters.titles包含了395个文档的标题<br>reuters.tokens包含了这395个文档中出现的所有单词，总共是4258个<br>reuters.ldac有395行，第i行代表第i个文档中各个词汇出现的频率。以第0行为例，第0行代表的是第0个文档，从reuters.titles中可查到该文档的标题为“UK: Prince Charles spearheads British royal revolution. LONDON 1996-08-20”。<br>第0行的数据为：<br>159 0:1 2:1 6:1 9:1 12:5 13:2 20:1 21:4 24:2 29:1 ……<br>第一个数字159表示第0个文档里总共出现了159个单词（每个单词出现一或多次），<br>0:1表示第0个单词出现了1次，从reuters.tokens查到第0个单词为church<br>2:1表示第2个单词出现了1次，从reuters.tokens查到第2个单词为years<br>6:1表示第6个单词出现了1次，从reuters.tokens查到第6个单词为told<br>9:1表示第9个单词出现了1次，从reuters.tokens查到第9个单词为year<br>12:5表示第12个单词出现了5次，从reuters.tokens查到第12个单词为charles<br>……<br>这里第1、3、4、5、7、8、10、11……个单词序号和次数没列出来，表示出现的次数为0<br>注意：<br>395个文档的原文是没有的。上述三个文档是根据这395个文档处理之后得到的。</p>
<p><img src="/2019/04/17/传统机器学习-LDA/kk2.PNG" alt=""></p>
<h5 id="2代码实现"><a href="#2代码实现" class="headerlink" title="2代码实现"></a>2代码实现</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#查看文本中词出先的频率</span><br><span class="line">import numpy as np</span><br><span class="line">import lda</span><br><span class="line">import lda.datasets</span><br><span class="line"></span><br><span class="line">X=lda.datasets.load_reuters()</span><br><span class="line">print(&quot;type(X):&#123;&#125;&quot;.format(type(X)))</span><br><span class="line">print(&quot;shape:&#123;&#125;\n&quot;.format(X.shape))</span><br><span class="line"></span><br><span class="line">print(X[:5,:5])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/kk3.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#查看词</span><br><span class="line">vocab=lda.datasets.load_reuters_vocab()</span><br><span class="line">print(&quot;type(vocab):&#123;&#125;&quot;.format(type(vocab)))</span><br><span class="line">print(&quot;len(vocab):&#123;&#125;\n&quot;.format(len(vocab)))</span><br><span class="line">print(vocab[:5])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/kk4.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#查看文档标题</span><br><span class="line">titles=lda.datasets.load_reuters_titles()</span><br><span class="line">print(&quot;type(title):&#123;&#125;&quot;.format(type(titles)))</span><br><span class="line">print(&quot;len(titles):&#123;&#125;&quot;.format(len(titles)))</span><br><span class="line">print(titles[:5])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/kk5.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#查看前五个文档第0个词出现的次数</span><br><span class="line">doc_id=0</span><br><span class="line">word_id=0</span><br><span class="line">while doc_id &lt;5:</span><br><span class="line">    print(&quot;doc id:&#123;&#125; word id:&#123;&#125;&quot;.format(doc_id,word_id))</span><br><span class="line">    print(&quot;--count:&#123;&#125;&quot;.format(X[doc_id,word_id]))#doc_id是文档的标记</span><br><span class="line">    print(&quot;--word:&#123;&#125;&quot;.format(vocab[word_id]))</span><br><span class="line">    print(&quot;--doc :&#123;&#125;\n&quot;.format(titles[doc_id]))</span><br><span class="line">    doc_id+=1</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/kk6.PNG" alt=""></p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#训练模型</span><br><span class="line">model=lda.LDA(n_topics=20,n_iter=500,random_state=1)</span><br><span class="line">model.fit(X)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/kk7.PNG" alt=""></p>
<p>#主题-单词分布</p>
<p>#计算前三个单词在所有的主题中所占的权重<br>topic_word=model.topic_word_<br>print(“type(topic_word):{}”.format(type(topic_word)))<br>print(“shape:{}”.format(topic_word.shape))<br>print(vocab[:3])<br>print(topic_word[:,:3])</p>
<p><img src="/2019/04/17/传统机器学习-LDA/kk8.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#计算所有行的比重之和</span><br><span class="line">for n in range(20):</span><br><span class="line">    sum_pr=sum(topic_word[n,:])</span><br><span class="line">    print(&quot;topic:&#123;&#125; sum:&#123;&#125;&quot;.format(n,sum_pr))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/kk9.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#计算各个主题的topN个词</span><br><span class="line">n=5</span><br><span class="line">for i,topic_dist in enumerate(topic_word):</span><br><span class="line">    topic_words=np.array(vocab)[np.argsort(topic_dist)[:-(n+1):-1]]</span><br><span class="line">    print(&quot;top &#123;&#125;\n- &#123;&#125;&quot;.format(i,&apos; &apos;.join(topic_words)))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/kk10.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#计算前十篇文章最有可能的主题</span><br><span class="line"></span><br><span class="line">doc_topic=model.doc_topic_</span><br><span class="line">print(&quot;ytpr(doc_topic):&#123;&#125;&quot;.format(type(doc_topic)))</span><br><span class="line">print(&quot;shape:&#123;&#125;&quot;.format(doc_topic.shape))</span><br><span class="line">for n in range(10):</span><br><span class="line">    topic_most_pr=doc_topic[n].argmax()</span><br><span class="line">    print(&quot;doc:&#123;&#125; topic :&#123;&#125;&quot;.format(n,topic_most_pr))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/C:/blog\source\_posts\传统机器学习-LDA\kk11.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#可视化分析</span><br><span class="line">#绘制主题0，5，9，14的词的出现次数</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">f,ax=plt.subplots(5,1,figsize=(8,6),sharex=True)</span><br><span class="line">for i,k in enumerate([0,5,9,14,19]):#枚举</span><br><span class="line">    print(i,k)</span><br><span class="line">    ax[i].stem(topic_word[k,:],linefmt=&apos;b-&apos;, markerfmt=&apos;bo&apos;, basefmt=&apos;w-&apos;)</span><br><span class="line">    </span><br><span class="line">    ax[i].set_xlim(-50, 4350)</span><br><span class="line">    ax[i].set_ylim(0, 0.08)</span><br><span class="line">    ax[i].set_ylabel(&quot;Prob&quot;)</span><br><span class="line">    ax[i].set_title(&quot;topic &#123;&#125;&quot;.format(k))</span><br><span class="line">ax[4].set_xlabel(&quot;word&quot;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/C:/blog\source\_posts\传统机器学习-LDA\kk12.PNG" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/16/支持向量机原理及应用/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/16/支持向量机原理及应用/" itemprop="url">支持向量机原理及应用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-16T13:33:59+08:00">
                2019-04-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="支持向量机（SVM）原理及应用"><a href="#支持向量机（SVM）原理及应用" class="headerlink" title="支持向量机（SVM）原理及应用"></a>支持向量机（SVM）原理及应用</h1><h3 id="1-算法原理"><a href="#1-算法原理" class="headerlink" title="1.算法原理"></a>1.算法原理</h3><h5 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h5><p>支持向量机（support vector machines）是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是间隔最大化，最终转化为一个凸二次规划问题来求解。由简至繁的模型包括：</p>
<ul>
<li>当训练样本线性可分时，通过硬间隔最大化，学习一个线性可分支持向量机；</li>
<li>当训练样本近似线性可分时，通过软间隔最大化，学习一个线性支持向量机；</li>
<li>当训练样本线性不可分时，通过核技巧和软间隔最大化，学习一个非线性支持向量机；</li>
</ul>
<p><img src="/2019/04/16/支持向量机原理及应用/oo3.PNG" alt=""></p>
<h5 id="2-线性可分的支持向量机"><a href="#2-线性可分的支持向量机" class="headerlink" title="2. 线性可分的支持向量机"></a>2. 线性可分的支持向量机</h5><p>基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。训练集标签为yi={1|-1}</p>
<p><img src="/2019/04/16/支持向量机原理及应用/oo1.PNG" alt=""></p>
<p>直观看上去，能将训练样本分开的划分超平面有很多，但应该去找位于两类训练样本“正中间”的划分超平面，即图6.1中红色的那条，因为该划分超平面对训练样本局部扰动的“容忍”性最好，例如，由于训练集的局限性或者噪声的因素，训练集外的样本可能比图6.1中的训练样本更接近两个类的分隔界，这将使许多划分超平面出现错误。而红色超平面的影响最小，简言之，这个划分超平面所产生的结果是鲁棒性的。</p>
<p>那什么是线性可分呢？</p>
<p>如果一个线性函数能够将样本分开，称这些数据样本是线性可分的。那么什么是线性函数呢？其实很简单，在二维空间中就是一条直线，在三维空间中就是一个平面，以此类推，如果不考虑空间维数，这样的线性函数统称为超平面。我们看一个简单的二维空间的例子，O代表正类，X代表负类，样本是线性可分的，但是很显然不只有这一条直线可以将样本分开，而是有无数条，我们所说的线性可分支持向量机就对应着能将数据正确划分并且间隔最大的直线。 </p>
<p>在实际应用总，我们所要求解的是能将数据正确划分的并且间隔最大的超平面<br>一个点距离超平面的远近可以表示分类的确信度，，距离超平面更远的点确信度更高，求解最号的SVM时，我们不许考录所有点，只要让超平面距离较近的点的点间隔最大即可</p>
<pre><code>距离超平面最近的几个点成为“支持向量”，两条虚线的距离称为 * 间隔 *
</code></pre><hr>
<p><img src="/2019/04/16/支持向量机原理及应用/oo2.PNG" alt=""></p>
<blockquote>
<p>间隔的计算：两个异类支持向量在实现的法线方向的投影大小</p>
</blockquote>
<h5 id="3-非线性支持向量机和核函数"><a href="#3-非线性支持向量机和核函数" class="headerlink" title="3.非线性支持向量机和核函数"></a>3.非线性支持向量机和核函数</h5><p>待补充</p>
<h5 id="4-线性支持向量机（软间隔支持向量机）与松弛变量"><a href="#4-线性支持向量机（软间隔支持向量机）与松弛变量" class="headerlink" title="4.线性支持向量机（软间隔支持向量机）与松弛变量"></a>4.线性支持向量机（软间隔支持向量机）与松弛变量</h5><p>待补充</p>
<h3 id="2-SVM优缺点"><a href="#2-SVM优缺点" class="headerlink" title="2.SVM优缺点"></a>2.SVM优缺点</h3><blockquote>
<p>优点：<br>1、使用核函数可以向高维空间进行映射<br>2、使用核函数可以解决非线性的分类<br>3、分类思想很简单，就是将样本与决策面的间隔最大化<br>4、分类效果较好<br>缺点：<br>1、对大规模数据训练比较困难<br>2、无法直接支持多分类，但是可以使用间接的方法来做 </p>
</blockquote>
<h3 id="3-SVM应用场景"><a href="#3-SVM应用场景" class="headerlink" title="3.SVM应用场景"></a>3.SVM应用场景</h3><blockquote>
<p>SVM在很多数据集上都有优秀的表现。<br>相对来说，SVM尽量保持与样本间距离的性质导致它抗攻击的能力更强。<br>和随机森林一样，这也是一个拿到数据就可以先尝试一下的算法</p>
</blockquote>
<h3 id="4-SVM-sklearn参数学习"><a href="#4-SVM-sklearn参数学习" class="headerlink" title="4.SVM sklearn参数学习"></a>4.SVM sklearn参数学习</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.svm.SVC(</span><br><span class="line">            C=1.0, </span><br><span class="line">            kernel=&apos;rbf&apos;, </span><br><span class="line">            degree=3, </span><br><span class="line">            gamma=&apos;auto&apos;, </span><br><span class="line">            coef0=0.0, </span><br><span class="line">            shrinking=True, </span><br><span class="line">            probability=False, </span><br><span class="line">            tol=0.001, </span><br><span class="line">            cache_size=200, </span><br><span class="line">            class_weight=None, </span><br><span class="line">            verbose=False, </span><br><span class="line">            max_iter=-1, </span><br><span class="line">            decision_function_shape=&apos;ovr&apos;, </span><br><span class="line">            random_state=None)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>SVC</strong>　<em>class</em> <code>sklearn.svm.`</code>SVC`(<em>C=1.0</em>, <em>kernel=’rbf’</em>, <em>degree=3</em>, <em>gamma=’auto’</em>, <em>coef0=0.0</em>, <em>shrinking=True</em>, <em>probability=False</em>, <em>tol=0.001</em>, <em>cache_size=200</em>, <em>class_weight=None</em>, <em>verbose=False</em>, <em>max_iter=-1</em>, <em>decision_function_shape=’ovr’</em>, <em>random_state=None</em>)　</li>
</ul>
<p>　　<code>**C:**</code>惩罚系数，用来控制损失函数的惩罚系数，类似于LR中的正则化系数。C越大，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样会出现训练集测试时准确率很高，但泛化能力弱，容易导致过拟合。 C值小，对误分类的惩罚减小，容错能力增强，泛化能力较强，但也可能欠拟合。</p>
<p>　　<code>**kernel:**</code>算法中采用的和函数类型，核函数是用来将非线性问题转化为线性问题的一种方法。参数选择有RBF, Linear, Poly, Sigmoid，precomputed或者自定义一个核函数, 默认的是”RBF”，即径向基核，也就是高斯核函数；而Linear指的是线性核函数，Poly指的是多项式核，Sigmoid指的是双曲正切函数tanh核；。</p>
<p>　　<strong>degree:</strong> 当指定kernel为’poly’时，表示选择的多项式的最高次数，默认为三次多项式；若指定kernel不是’poly’，则忽略，即该参数只对’poly’有用。（多项式核函数是将低维的输入空间映射到高维的特征空间）</p>
<p>　　<code>**gamma:**</code>核函数系数，该参数是rbf，poly和sigmoid的内核系数；默认是’auto’，那么将会使用特征位数的倒数，即1 / n_features。（即核函数的带宽，超圆的半径）。gamma越大，σ越小，使得高斯分布又高又瘦，造成模型只能作用于支持向量附近，可能导致过拟合；反之，gamma越小，σ越大，高斯分布会过于平滑，在训练集上分类效果不佳，可能导致欠拟合。 </p>
<p>　　<code>**coef0:**</code>核函数常数值(y=kx+b中的b值)，只有‘poly’和‘sigmoid’核函数有，默认值是0。</p>
<p>　　<strong>shrinking :</strong>  是否进行启发式。如果能预知哪些变量对应着支持向量，则只要在这些样本上训练就够了，其他样本可不予考虑，这不影响训练结果，但降低了问题的规模并有助于迅速求解。进一步，如果能预知哪些变量在边界上(即a=C)，则这些变量可保持不动，只对其他变量进行优化，从而使问题的规模更小，训练时间大大降低。这就是Shrinking技术。 Shrinking技术基于这样一个事实：支持向量只占训练样本的少部分，并且大多数支持向量的拉格朗日乘子等于C。</p>
<p>　　<strong>probability:</strong> 是否使用概率估计，默认是False。必须在 fit( ) 方法前使用，该方法的使用会降低运算速度。</p>
<p>　　<code>**tol:**</code>残差收敛条件，默认是0.0001，即容忍1000分类里出现一个错误，与LR中的一致；误差项达到指定值时则停止训练。</p>
<p>　　<code>**cache_size:**</code>缓冲大小，用来限制计算量大小，默认是200M。</p>
<p>　　<strong>class_weight :</strong>  {dict, ‘balanced’}，字典类型或者’balance’字符串。权重设置，正类和反类的样本数量是不一样的，这里就会出现类别不平衡问题，该参数就是指每个类所占据的权重，默认为1，即默认正类样本数量和反类一样多，也可以用一个字典dict指定每个类的权值，或者选择默认的参数balanced，指按照每个类中样本数量的比例自动分配权值。如果不设置，则默认所有类权重值相同，以字典形式传入。 将类i 的参数C设置为SVC的class_weight[i]<em>C。如果没有给出，所有类的weight 为1。’balanced’模式使用y 值自动调整权重，调整方式是与输入数据中类频率成反比。如n_samples / (n_classes </em> np.bincount(y))。（给每个类别分别设置不同的惩罚参数C，如果没有给，则会给所有类别都给C=1，即前面参数指出的参数C。如果给定参数’balance’，则使用y的值自动调整与输入数据中的类频率成反比的权重。）</p>
<p>　　<strong>verbose :</strong>  是否启用详细输出。在训练数据完成之后，会把训练的详细信息全部输出打印出来，可以看到训练了多少步，训练的目标值是多少；但是在多线程环境下，由于多个线程会导致线程变量通信有困难，因此verbose选项的值就是出错，所以多线程下不要使用该参数。</p>
<p>　　<code>**max_iter:**</code>最大迭代次数，默认是-1，即没有限制。这个是硬限制，它的优先级要高于tol参数，不论训练的标准和精度达到要求没有，都要停止训练。</p>
<p>　　<strong>decision_function_shape ：</strong>  原始的SVM只适用于二分类问题，如果要将其扩展到多类分类，就要采取一定的融合策略，这里提供了三种选择。‘ovo’ 一对一，为one v one，即将类别两两之间进行划分，用二分类的方法模拟多分类的结果，决策所使用的返回的是（样本数，类别数*(类别数-1)/2）； ‘ovr’ 一对多，为one v rest，即一个类别与其他类别进行划分，返回的是(样本数，类别数)，或者None，就是不采用任何融合策略。默认是ovr，因为此种效果要比oro略好一点。</p>
<p>　　<strong>random_state:</strong> 在使用SVM训练数据时，要先将训练数据打乱顺序，用来提高分类精度，这里就用到了伪随机序列。如果该参数给定的是一个整数，则该整数就是伪随机序列的种子值；如果给定的就是一个随机实例，则采用给定的随机实例来进行打乱处理；如果啥都没给，则采用默认的 np.random实例来处理。 </p>
<ul>
<li><p><strong>NuSVC</strong>　　　　　　　　　　　　　　　　　<em>class</em> <code>sklearn.svm.`</code>NuSVC`(<em>nu=0.5</em>, <em>kernel=’rbf’</em>, <em>degree=3</em>, <em>gamma=’auto’</em>, <em>coef0=0.0</em>, <em>shrinking=True</em>, <em>probability=False</em>, <em>tol=0.001</em>, <em>cache_size=200</em>, <em>class_weight=None</em>, <em>verbose=False</em>, <em>max_iter=-1</em>, <em>decision_function_shape=’ovr’</em>, <em>random_state=None</em>) 　</p>
<p>　　<strong>nu：</strong> 训练误差部分的上限和支持向量部分的下限，取值在（0，1）之间，默认是0.5</p>
</li>
<li><p><strong>LinearSVC</strong>　　　　<em>class</em> <code>sklearn.svm.`</code>LinearSVC`(<em>penalty=’l2’</em>, <em>loss=’squared_hinge’</em>, <em>dual=True</em>, <em>tol=0.0001</em>, <em>C=1.0</em>, <em>multi_class=’ovr’</em>, <em>fit_intercept=True</em>, <em>intercept_scaling=1</em>, <em>class_weight=None</em>, <em>verbose=0</em>, <em>random_state=None</em>, <em>max_iter=1000</em>)</p>
</li>
</ul>
<p>　　<code>**penalty:**</code>正则化参数，L1和L2两种参数可选，仅LinearSVC有。</p>
<p>　　<code>**loss:**</code>损失函数，有‘hinge’和‘squared_hinge’两种可选，前者又称L1损失，后者称为L2损失，默认是是’squared_hinge’，其中hinge是SVM的标准损失，squared_hinge是hinge的平方</p>
<p>　　<code>**dual:**</code>是否转化为对偶问题求解，默认是True。</p>
<p>　　<code>**tol:**</code>残差收敛条件，默认是0.0001，与LR中的一致。</p>
<p>　　<code>**C:**</code>惩罚系数，用来控制损失函数的惩罚系数，类似于LR中的正则化系数。</p>
<p>　　<code>**multi_class:**</code>负责多分类问题中分类策略制定，有‘ovr’和‘crammer_singer’ 两种参数值可选，默认值是’ovr’，’ovr’的分类原则是将待分类中的某一类当作正类，其他全部归为负类，通过这样求取得到每个类别作为正类时的正确率，取正确率最高的那个类别为正类；‘crammer_singer’ 是直接针对目标函数设置多个参数值，最后进行优化，得到不同类别的参数值大小</p>
<p>　　<code>**fit_intercept:**</code>是否计算截距，与LR模型中的意思一致。</p>
<p>　　<code>**class_weight:**</code>与其他模型中参数含义一样，也是用来处理不平衡样本数据的，可以直接以字典的形式指定不同类别的权重，也可以使用balanced参数值。</p>
<p>　　<code>**verbose:**</code>是否冗余，默认是False。</p>
<p>　　<code>**random_state:**</code>随机种子。</p>
<p>　　<code>**max_iter:**</code>最大迭代次数，默认是1000。</p>
<h3 id="SVM结合TF-IDF原理进行文本分类"><a href="#SVM结合TF-IDF原理进行文本分类" class="headerlink" title="SVM结合TF-IDF原理进行文本分类"></a>SVM结合TF-IDF原理进行文本分类</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import jieba</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import time</span><br><span class="line">import codecs</span><br><span class="line">import re</span><br><span class="line">import jieba.posseg as pseg</span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">from sklearn.feature_extraction.text import TfidfTransformer</span><br><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line">from sklearn import metrics</span><br><span class="line"></span><br><span class="line">open=codecs.open</span><br><span class="line">path_en=&apos;../After/En&apos;</span><br><span class="line">path_peo=&apos;../After/new_people&apos;</span><br><span class="line">content_train_src=[]      #训练集文本列表</span><br><span class="line">opinion_train_stc=[]      #训练集类别列表</span><br><span class="line">file_name_src=[]          #训练集文本文件名列表</span><br><span class="line">train_src=[]              #训练集总列表</span><br><span class="line">test_src=[]               #测试集总列表</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def readfile(path):</span><br><span class="line">    for filename in os.listdir(path):</span><br><span class="line">        starttime=time.time()</span><br><span class="line">        filepath=path+&quot;/&quot;+filename</span><br><span class="line">        filestr=open(filepath).read()</span><br><span class="line">        opinion=path[9:]</span><br><span class="line">        train_src.append((filename,filestr,opinion))</span><br><span class="line">        endtime=time.time()</span><br><span class="line">        print &apos;类别:%s &gt;&gt;&gt;&gt;文件:%s &gt;&gt;&gt;&gt;导入用时: %.3f&apos; % (opinion, filename, endtime - starttime)</span><br><span class="line">    return train_src</span><br><span class="line">    </span><br><span class="line">    train_src_all=readfile(path_en)</span><br><span class="line">train_src_all=train_src_all+readfile(path_peo)</span><br><span class="line"></span><br><span class="line">def readtrain(train_src_list):</span><br><span class="line">    for (fn,w,s) in train_src_list:</span><br><span class="line">        file_name_src.append(fn)</span><br><span class="line">        content_train_src.append(w)</span><br><span class="line">        opinion_train_stc.append(s)</span><br><span class="line">    train=[content_train_src,opinion_train_stc,file_name_src]</span><br><span class="line">    return train</span><br><span class="line"></span><br><span class="line">def segmentWord(cont):</span><br><span class="line">    listseg=[]</span><br><span class="line">    for i in cont:</span><br><span class="line">        Wordp = Word_pseg(i)</span><br><span class="line">        New_str = &apos;&apos;.join(Wordp)</span><br><span class="line">        Wordlist = Word_cut_list(New_str)</span><br><span class="line">        file_string = &apos;&apos;.join(Wordlist)</span><br><span class="line">        listseg.append(file_string)</span><br><span class="line">    return listseg</span><br><span class="line"></span><br><span class="line">train=readtrain(train_src_all)</span><br><span class="line">content=segmentWord(train[0])</span><br><span class="line">filenamel=train[2]</span><br><span class="line">opinion=train[1]</span><br><span class="line"></span><br><span class="line">train_content=content[:3000]</span><br><span class="line">test_content=content[3000:]</span><br><span class="line">train_opinion=opinion[:3000]</span><br><span class="line">test_opinion=opinion[3000:]</span><br><span class="line">train_filename=filenamel[:3000]</span><br><span class="line">test_filename=filenamel[3000:]</span><br><span class="line"></span><br><span class="line">test_all=[test_content,test_opinion,test_filename]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vectorizer=CountVectorizer()</span><br><span class="line">tfidftransformer=TfidfTransformer()</span><br><span class="line">tfidf = tfidftransformer.fit_transform(vectorizer.fit_transform(train_content))  # 先转换成词频矩阵，再计算TFIDF值</span><br><span class="line"></span><br><span class="line">print tfidf.shape</span><br><span class="line"></span><br><span class="line">word = vectorizer.get_feature_names()</span><br><span class="line">weight = tfidf.toarray()</span><br><span class="line"># 分类器</span><br><span class="line">clf = MultinomialNB().fit(tfidf, opinion)</span><br><span class="line">docs = [&quot;原任第一集团军副军长&quot;, &quot;在9·3抗战胜利日阅兵中担任“雁门关伏击战英雄连”英模方队领队记者林韵诗继黄铭少将后&quot;]</span><br><span class="line">new_tfidf = tfidftransformer.transform(vectorizer.transform(docs))</span><br><span class="line">predicted = clf.predict(new_tfidf)</span><br><span class="line">print predicted</span><br><span class="line"></span><br><span class="line"># 训练和预测一体</span><br><span class="line">text_clf = Pipeline([(&apos;vect&apos;, CountVectorizer()), (&apos;tfidf&apos;, TfidfTransformer()), (&apos;clf&apos;, SVC(C=1, kernel = &apos;linear&apos;))])</span><br><span class="line">text_clf = text_clf.fit(train_content, train_opinion)</span><br><span class="line">predicted = text_clf.predict(test_content)</span><br><span class="line">print &apos;SVC&apos;,np.mean(predicted == test_opinion)</span><br><span class="line">print set(predicted)</span><br><span class="line">print metrics.confusion_matrix(test_opinion,predicted) # 混淆矩阵</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/15/朴素贝叶斯/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/15/朴素贝叶斯/" itemprop="url">朴素贝叶斯</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-15T00:40:20+08:00">
                2019-04-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h1><blockquote>
<p>朴素贝叶斯法（Naive Bayes）是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对给定的输入 <em>x</em>x ，利用贝叶斯定理求出后验概率最大的输出 <em>y</em>y 。</p>
</blockquote>
<p> 可能读完上面这段话仍旧没办法理解朴素贝叶斯法到底是什么，又是怎样进行分类的。下面我尽可能详细且直观地描述朴素贝叶斯法的工作原理。首先我们需要知道的是，<strong>朴素贝叶斯是基于概率论的分类算法</strong>。</p>
<h3 id="1-朴素贝叶斯"><a href="#1-朴素贝叶斯" class="headerlink" title="1. 朴素贝叶斯"></a>1. 朴素贝叶斯</h3><pre><code>朴素贝叶斯被认为是最简单的分类算法之一。首先，我们需要了解一些概率论的基本理论。假设有两个随机变量X和Y，他们分别可以取值为x和y。有这两个随机变量，我们可以定义两种概率：
关键概念：联合概率与条件概率
联合概率：“X取值为x”和“Y取值为y”两个事件同时发生的概率，表示为P(X=x,Y=y)P(X=x,Y=y)      P(X = x, Y = y)P(X=x,Y=y)
条件概率：在X取值为x的前提下，Y取值为y的概率，表示为P(Y=y∣X=x)P(Y=y∣X=x)      P(Y = y | X = x)P(Y=y∣X=x)

在概率论中，我们可以证明，两个事件的联合概率等于这两个事件任意条件概率 * 这个条件事件本身的概率。
P(X=1,Y=1)=P(Y=1∣X=1)∗P(X=1)=P(X=1∣Y=1)∗P(Y=1)P(X=1,Y=1)=P(Y=1∣X=1)∗P(X=1)=P(X=1∣Y=1)∗P(Y=1)      P(X = 1, Y = 1) =P(Y = 1|X = 1)*P(X=1)=P(X = 1|Y=1)*P(Y=1)P(X=1,Y=1)=P(Y=1∣X=1)∗P(X=1)=P(X=1∣Y=1)∗P(Y=1)
简单一些，则可以将上面的式子写成：
P(X,Y)=P(Y∣X)∗P(X)=P(X∣Y)∗P(Y)P(X,Y)=P(Y∣X)∗P(X)=P(X∣Y)∗P(Y)      P(X, Y)=P(Y|X)*P(X)=P(X|Y)*P(Y)P(X,Y)=P(Y∣X)∗P(X)=P(X∣Y)∗P(Y)
由上面的式子，我们可以得到贝叶斯理论等式：
P(Y∣X)=P(X∣Y)∗P(Y)P(X)P(Y∣X)=P(X∣Y)∗P(Y)P(X)      P(Y|X) =  \frac{P(X|Y)*P(Y)}{P(X)}P(Y∣X)=P(X)P(X∣Y)∗P(Y)
而这个式子，就是我们一切贝叶斯算法的根源理论。我们可以把我们的特征XX      XX当成是我们的条件事件，而我们要求解的标签YY      YY当成是我们被满足条件后会被影响的结果，而两者之间的概率关系就是P(Y∣X)P(Y∣X)      P(Y|X)P(Y∣X)，这个概率在机器学习中，被我们称之为是标签的后验概率（posterior probability），即是说我们先知道了条件，再去求解结果。而标签 在没有任何条件限制下取值为某个值的概率，被我们写作P(Y)P(Y)      P(Y)P(Y)，与后验概率相反，这是完全没有任何条件限制的，标签的先验概率（prior probability）。而我们的P(X∣Y)P(X∣Y)      P(X|Y)P(X∣Y) 被称为“类的条件概率”，表示当Y的取值固定的时候，X为某个值的概率。
</code></pre><h3 id="3-朴素贝叶斯公式的推导"><a href="#3-朴素贝叶斯公式的推导" class="headerlink" title="3.朴素贝叶斯公式的推导"></a>3.朴素贝叶斯公式的推导</h3><p><img src="/2019/04/15/朴素贝叶斯/pp2.PNG" alt=""></p>
<h3 id="2-贝叶斯估计"><a href="#2-贝叶斯估计" class="headerlink" title="2.贝叶斯估计"></a>2.贝叶斯估计</h3><p><img src="/2019/04/15/朴素贝叶斯/pp3.PNG" alt=""></p>
<h3 id="3-朴素贝叶斯算法的过程"><a href="#3-朴素贝叶斯算法的过程" class="headerlink" title="3.朴素贝叶斯算法的过程"></a>3.朴素贝叶斯算法的过程</h3><p><img src="/2019/04/15/朴素贝叶斯/pp4.PNG" alt=""></p>
<h2 id="使用朴素贝叶斯进行文本分类示例"><a href="#使用朴素贝叶斯进行文本分类示例" class="headerlink" title="使用朴素贝叶斯进行文本分类示例"></a>使用朴素贝叶斯进行文本分类示例</h2><p>本次使用的数据集来自于业内著名的20 Newsgroups 数据集，包含20类标注好的样本，数据量共计约2万条记录。该数据集每篇文档均不长，即使同时使用多个类目数据合并起来进行建模，在单机上也可快速完成，因此具有很好的学习训练价值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from time import time</span><br><span class="line">from sklearn.datasets import load_files</span><br><span class="line"></span><br><span class="line">#加载数据</span><br><span class="line">print(&quot;loading train dataset.....&quot;)</span><br><span class="line">t=time()</span><br><span class="line">news_train=load_files(&apos;D:/20news/20news-bydate-train/&apos;)</span><br><span class="line">print(&quot;summary:&#123;0&#125; documents in &#123;1&#125; categories.&quot;.format(len(news_train.data),len(news_train.target_names)))</span><br><span class="line">print(&quot;done in &#123;0&#125; seconds&quot;.format(time()-t))</span><br></pre></td></tr></table></figure>
<p>结果输出:</p>
<p><img src="/2019/04/15/朴素贝叶斯/结果1.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#将文档转化为TF-IDF形式</span><br><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line"></span><br><span class="line">print(&quot;vaectoring train dataset......&quot;)</span><br><span class="line">t=time()</span><br><span class="line">vectorsize=TfidfVectorizer(encoding=&quot;latin-1&quot;)</span><br><span class="line">X_train=vectorsize.fit_transform((d for d in news_train.data))#转换为矩阵</span><br><span class="line">print(&quot;n_sample:%d,n_features:%d&quot;% X_train.shape)</span><br><span class="line">print(&quot;number of non-zero features in sample [&#123;0&#125;]:&#123;1&#125;&quot;.format(news_train.filenames[0],X_train[0].getnnz()))#打印</span><br><span class="line">#第一篇文章名称和文章中的词语数</span><br><span class="line">print(&quot;done in &#123;0&#125; seconds&quot;.format(time()-t))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/15/朴素贝叶斯/结果2.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#使用模块MultinomialNB训练数据集</span><br><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line"></span><br><span class="line">print(&quot;training models....&quot;.format(time()-t))</span><br><span class="line">t=time()</span><br><span class="line">y_train=news_train.target#元素大小为0到20的一维矩阵</span><br><span class="line">clf=MultinomialNB(alpha=0.0001)#拼平滑指数，过小容易造成过拟合，过大容易过拟合</span><br><span class="line">clf.fit(X_train,y_train)</span><br><span class="line">train_score=clf.score(X_train,y_train)</span><br><span class="line">print(&quot;train_socre :&#123;0&#125;&quot;.format(train_score))</span><br><span class="line">print(&quot;done in &#123;0&#125; seconds&quot;.format(time()-t))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/15/朴素贝叶斯/结果3.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#加载测试集合</span><br><span class="line">print(&quot;loading test dataset.....&quot;)</span><br><span class="line">t=time()</span><br><span class="line">news_test=load_files(&quot;D:/20news/20news-bydate-test/&quot;)</span><br><span class="line">print(&quot;summary :&#123;0&#125; Documents in &#123;1&#125; catelogies.&quot;.format(len(news_test.data),len(news_test.target_names)))</span><br><span class="line">print(&quot;done in &#123;0&#125; seconds &quot;.format(time()-t))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">      </span><br><span class="line">      #将测试集向量化</span><br><span class="line">print(&quot;向量化测试机集&quot;)</span><br><span class="line">y_test=news_test.target</span><br><span class="line">X_test=vectorsize.transform((b for b in news_test.data))</span><br><span class="line">print(&quot;n_samples: %d. n_fetures : %d &quot;% X_test.shape)</span><br><span class="line">print(&quot;&#123;0&#125; numers of None-zero in file[&#123;1&#125;]&quot;.format(X_test[0].getnnz(),news_test.filenames[0]))</span><br><span class="line">print(&quot;done in %fs&quot;%(time()-t))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/15/朴素贝叶斯/结果4.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#取第一篇预测一下</span><br><span class="line">pred=clf.predict(X_test[0])</span><br><span class="line"></span><br><span class="line">#print(X_test[0])</span><br><span class="line">print(&quot;the first page is &#123;0&#125;,the predict is &#123;1&#125;&quot;.format(news_test.filenames[0],news_test.target_names[news_test.target[0]]))</span><br><span class="line">#模型评价</span><br><span class="line">#首先对测试机进行预测</span><br><span class="line">print(&quot;predicting the test.......&quot;)</span><br><span class="line">t0=time()</span><br><span class="line">pred=clf.predict(X_test)</span><br><span class="line">print(&quot;done in &#123;0&#125; seconds&quot;.format(time()-t0))</span><br><span class="line">#使用classification_report查看每个类别预测的准确性</span><br><span class="line">from sklearn.metrics import classification_report</span><br><span class="line"></span><br><span class="line">print(&quot;classification_report on test set for classifier:&quot;)</span><br><span class="line">print(clf)</span><br><span class="line">print(classification_report(y_test,pred,target_names=news_test.target_names))#精确度，召回率，F1值</span><br><span class="line"></span><br><span class="line">print(&quot;**********************************************&quot;)</span><br><span class="line">#使用confusion_matrix生成混淆矩阵,</span><br><span class="line">from sklearn.metrics import confusion_matrix</span><br><span class="line">cm=confusion_matrix(y_test,pred)</span><br><span class="line">print(&quot;confusion_matrix:&quot;)</span><br><span class="line">print(cm)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/15/朴素贝叶斯/结果5.PNG" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/14/自然语言处理4-文本分析/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/14/自然语言处理4-文本分析/" itemprop="url">自然语言处理4:文本分析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-14T14:13:39+08:00">
                2019-04-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="自然语言处理-4-文本表示"><a href="#自然语言处理-4-文本表示" class="headerlink" title="自然语言处理(4):文本表示"></a>自然语言处理(4):文本表示</h1><h3 id="1-TF-IDF原理"><a href="#1-TF-IDF原理" class="headerlink" title="1.TF-IDF原理"></a>1.TF-IDF原理</h3><ol>
<li><strong> 词频TF（item frequency）</strong>：某一给定词语在该文本中出现次数。该数字通常会被归一化（分子一般小于分母），以防止它偏向长的文件，因为不管该词语重要与否，它在长文件中出现的次数很可能比在段文件中出现的次数更大。       需要注意的是有一些通用词对文章主题没有太大作用，如“的”“是”等，而有一些频率出现少的词如一些专业词更能表现文章主题，所以为词语设置权重，权重的设计满足：一个词预测主题的能力越强，权重越大，反之，权重越小。也就是说，一些词只在很少几篇文章中出现，那么这样的词对文章主题的判断能力很大，这些词的权重应该设计的较大。IDF完成这样的工作。       </li>
<li><strong>逆向文件频率IDF（inverse document frequency）</strong>：一个词语普遍重要性的度量。主要思想是：如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TF=(某个词在文章中出现的次数)/(文章的总词数)</span><br><span class="line">IDF=log((语料库的文档总数)/(包含该词的文档数+1))</span><br></pre></td></tr></table></figure>
<h3 id="2-使用不同的方法来计算TF-IDF的值"><a href="#2-使用不同的方法来计算TF-IDF的值" class="headerlink" title="2.使用不同的方法来计算TF-IDF的值"></a>2.使用不同的方法来计算TF-IDF的值</h3><ol>
<li>词袋模型<br>Bag of words，也叫做“词袋”，在信息检索中，Bag of words model假定对于一个文本，忽略其词序和语法，句法，将其仅仅看做是一个词集合，或者说是词的一个组合，文本中每个词的出现都是独立的，不依赖于其他词是否出现，或者说当这篇文章的作者在任意一个位置选择一个词汇都不受前面句子的影响而独立选择的。</li>
</ol>
<h5 id="1-使用gensim模块"><a href="#1-使用gensim模块" class="headerlink" title="1.使用gensim模块"></a>1.使用gensim模块</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">corpus = [</span><br><span class="line">    &apos;this is the first document&apos;,</span><br><span class="line">    &apos;this is the second second document&apos;,</span><br><span class="line">    &apos;and the third one&apos;,</span><br><span class="line">    &apos;is this the first document&apos;</span><br><span class="line">]#语料库</span><br><span class="line"></span><br><span class="line">#先做分词处理</span><br><span class="line">word_list=[]</span><br><span class="line">for i in range(len(corpus)):</span><br><span class="line">    word_list.append(corpus[i].split(&apos; &apos;))</span><br><span class="line">print(word_list)</span><br><span class="line"></span><br><span class="line">#得到每个词的id值和词频</span><br><span class="line"></span><br><span class="line">from gensim import corpora</span><br><span class="line">dic=corpora.Dictionary(word_list)#赋给语料库中每个不重复的词一个id</span><br><span class="line">new_corpus=[dic.doc2bow(text) for text in word_list]#寻找整篇语料的词典、所有词，corpora.Dictionary</span><br><span class="line">print(new_corpus)</span><br><span class="line"></span><br><span class="line">#训练gensim模型</span><br><span class="line">from gensim import models</span><br><span class="line">tfidf=models.TfidfModel(new_corpus)</span><br><span class="line">tfidf.save(&quot;my_model.tfidf&quot;)</span><br><span class="line"></span><br><span class="line">#载入模型</span><br><span class="line">tfidf=models.TfidfModel.load(&quot;my_model.tfidf&quot;)</span><br><span class="line">#使用模型得到单词的tfidf值</span><br><span class="line">tfidf_vec=[]</span><br><span class="line">for i in range(len(corpus)):</span><br><span class="line">    string=corpus[i]</span><br><span class="line">    string_bow=dic.doc2bow(string.lower().split())</span><br><span class="line">    stringtfidf=tfidf[string_bow]</span><br><span class="line">    tfidf_vec.append(stringtfidf)</span><br><span class="line">print(tfidf_vec)</span><br></pre></td></tr></table></figure>
<ol>
<li>结论</li>
</ol>
<ul>
<li>gensim训练出来的tf-idf值左边是词的id，右边是词的tfidf值</li>
<li>gensim有自动去除停用词的功能，比如the</li>
<li>gensim会自动去除单个字母，比如i</li>
<li>gensim会去除没有被训练到的词，比如name<br>所以通过gensim并不能计算每个单词的tfidf值</li>
</ul>
<h5 id="2-使用sklearn提取文本tfidf特征"><a href="#2-使用sklearn提取文本tfidf特征" class="headerlink" title="2.使用sklearn提取文本tfidf特征"></a>2.使用sklearn提取文本tfidf特征</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">tfidf_vec=TfidfVectorizer()#模型构建</span><br><span class="line">tfidf_matrix=tfidf_vec.fit_transform(corpus)#传入句子组成的list</span><br><span class="line"></span><br><span class="line">print(tfidf_vec.get_feature_names())#得到所有不重复的词</span><br><span class="line">print(tfidf_vec.vocabulary_)#得到对应的id值</span><br><span class="line">print(tfidf_matrix.toarray())</span><br></pre></td></tr></table></figure>
<h6 id="3-python提取文本特征tfidf值"><a href="#3-python提取文本特征tfidf值" class="headerlink" title="3.python提取文本特征tfidf值"></a>3.python提取文本特征tfidf值</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#统计词频</span><br><span class="line">from collections import Counter</span><br><span class="line">countlist=[]</span><br><span class="line">for i in range(len(word_list)):</span><br><span class="line">    count=Counter(word_list[i])#每个count是一个句子</span><br><span class="line">    countlist.append(count)</span><br><span class="line"># word可以通过count得到，count可以通过countlist得到</span><br><span class="line"># count[word]可以得到每个单词的词频， sum(count.values())得到整个句子的单词总数</span><br><span class="line">def tf(word,count):</span><br><span class="line">    return count[word]/sum(count.values())</span><br><span class="line">#统计含有该单词的句子数</span><br><span class="line">def n_containing(word,countlist):</span><br><span class="line">    return sum(1 for count in countlist if word in count)</span><br><span class="line"># len(count_list)是指句子的总数，n_containing(word, count_list)是指含有该单词的句子的总数，加1是为了防止分母为0</span><br><span class="line">def idf(word, count_list):</span><br><span class="line">    return math.log(len(count_list) / (1 + n_containing(word, count_list)))</span><br><span class="line"></span><br><span class="line"># 将tf和idf相乘</span><br><span class="line">def tfidf(word, count, count_list):</span><br><span class="line">    return tf(word, count) * idf(word, count_list)</span><br><span class="line"></span><br><span class="line">import math</span><br><span class="line">for i,count in enumerate(countlist):</span><br><span class="line">    print(&quot;Top words in docment&#123;&#125;&quot;.format(i+1))</span><br><span class="line">    scores=&#123;word:tfidf(word,count,countlist) for word in count&#125;</span><br><span class="line">    sorted_words=sorted(scores.items(),key=lambda x:x[1],reverse=True)</span><br><span class="line">    for word,score in sorted_words[:]:</span><br><span class="line">        print(&quot;\tWord: &#123;&#125;, TF-IDF: &#123;&#125;&quot;.format(word, round(score, 5)))</span><br></pre></td></tr></table></figure>
<h3 id="3-sklearn：点互信息和互信息"><a href="#3-sklearn：点互信息和互信息" class="headerlink" title="3.sklearn：点互信息和互信息"></a>3.sklearn：点互信息和互信息</h3><ol>
<li>点互信息PMI<br>机器学习相关文献里面，经常会用到点互信息PMI(Pointwise Mutual Information)这个指标来衡量两个事物之间的相关性（比如两个词）。<br>  在概率论中，我们知道，如果x跟y不相关，则p(x,y)=p(x)p(y)。二者相关性越大，则p(x, y)就相比于p(x)p(y)越大。用后面的式子可能更好理解，在y出现的情况下x出现的条件概率p(x|y)除以x本身出现的概率p(x)，自然就表示x跟y的相关程度。举个自然语言处理中的例子来说，我们想衡量like这个词的极性（正向情感还是负向情感）。我们可以预先挑选一些正向情感的词，比如good。然后我们算like跟good的PMI。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PMI(x;y)=log(p(x,y)/(p(x)*p(y)))=log(p(y|x)/p(y))</span><br></pre></td></tr></table></figure>
<ol>
<li>互信息MI<br>衡量的是两个随机变量之间的相关性，即一个随机变量中包含的关于另一个随机变量的信息量。所谓的随机变量，即随机试验结果的量的表示，可以简单理解为按照一个概率分布进行取值的变量，比如随机抽查的一个人的身高就是一个随机变量。可以看出，互信息其实就是对X和Y的所有可能的取值情况的点互信息PMI的加权和。因此，点互信息这个名字还是很形象的。</li>
</ol>
<h3 id="4-如何进行特征选择（理论篇）机器学习你会遇到的“坑”-来源于网络"><a href="#4-如何进行特征选择（理论篇）机器学习你会遇到的“坑”-来源于网络" class="headerlink" title="4.如何进行特征选择（理论篇）机器学习你会遇到的“坑”(来源于网络)"></a>4.如何进行特征选择（理论篇）机器学习你会遇到的“坑”(来源于网络)</h3><p>一个典型的机器学习任务，是通过样本的特征来预测样本所对应的值。如果样本的特征少了，我们会考虑增加特征，比如Polynomial Regression就是典型的增加特征的算法。在前一周的课程中，相信大家已经体会到，模型特征越多，模型的复杂度也就越高，越容易导致过拟合。事实上，如果我们的样本数少于特征数，那么过拟合就不可避免。</p>
<p>而现实中的情况，往往是特征太多了，需要减少一些特征。</p>
<p>首先是“无关特征”（irrelevant feature）。比如，通过空气的湿度，环境的温度，风力和当地人的男女比例来预测明天会不会下雨，其中男女比例就是典型的无关特征。</p>
<p>其次，要减少的另一类特征叫做“多余特征”（redundant feature），比如，通过房屋的面积，卧室的面积，车库的面积，所在城市的消费水平，所在城市的税收水平等特征来预测房价，那么消费水平（或税收水平）就是多余特征。证据表明，消费水平和税收水平存在相关性，我们只需要其中一个特征就够了，因为另一个能从其中一个推演出来。（如果是线性相关，那么我们在用线性模型做回归的时候，会出现严重的多重共线性问题，将会导致过拟合。）</p>
<p>减少特征有非常重要的现实意义，甚至有人说，这是工业界最重要的问题。因为除了降低过拟合，特征选择还可以使模型获得更好的解释性，加快模型的训练速度，一般的，还会获得更好的性能。</p>
<p>问题在于，在面对未知领域的时候，很难有足够的知识去判断特征与我们的目标是不是相关，特征与特征之间是不是相关。这时候，就需要一些数学和工程上的办法来帮助我们尽可能地把恰好需要的特征选择出来。</p>
<p>常见的方法包括过滤法（Filter）、包裹法（Warpper），嵌入法</p>
<h5 id="1-过滤法"><a href="#1-过滤法" class="headerlink" title="1.过滤法"></a>1.过滤法</h5><p>过滤法只用于检验特征向量和目标（响应变量）的相关度，不需要任何的机器学习的算法，不依赖于任何模型，只是应用统计量做筛选：我们根据统计量的大小，设置合适的阈值，将低于阈值的特征剔除.</p>
<h5 id="2-包裹法"><a href="#2-包裹法" class="headerlink" title="2.包裹法"></a>2.包裹法</h5><p>与过滤法不同的是，包裹法采用的是特征搜索的办法。它的基本思路是，从初始特征集合中不断的选择子集合，根据学习器的性能来对子集进行评价，直到选择出最佳的子集。在搜索过程中，我们会对每个子集做建模和训练。</p>
<p>图为包裹法的流程图，其中Estimated Accuracy是机器学习分类问题的典型的性能指标。</p>
<p>基于此，包裹法很大程度上变成了一个计算机问题：在特征子集的搜索问题（subset search）。我们有多种思路，最容易想到的办法是穷举（Brute-force search），遍历所有可能的子集，但这样的方法适用于特征数较少的情形，特征一旦增多，就会遇到组合爆炸，在计算上并不可行。（N个特征，则子集会有种可能）</p>
<p>另一个思路是随机化搜索，比如拉斯维加斯算法（Las Vegas algorithm），但这样的算法在特征数大的时候，计算开销仍然很大，而且有给不出任何解的风险。所以，我们常使用的是贪心算法：</p>
<ul>
<li>前向搜索（Forward search）</li>
</ul>
<p>在开始时，按照特征数来划分子集，每个子集只有一个特征，对每个子集进行评价。然后在最优的子集上逐步增加特征，使模型性能提升最大，直到增加特征并不能使模型性能提升为止。</p>
<ul>
<li>后向搜索（Backward search）</li>
</ul>
<p>在开始时，将特征集合分别减去一个特征作为子集，每个子集有N—1个特征，对每个子集进行评价。然后在最优的子集上逐步减少特征，使得模型性能提升最大，直到减少特征并不能使模型性能提升为止。</p>
<ul>
<li>双向搜索（Bidirectional search）</li>
</ul>
<p>将Forward search 和Backward search结合起来。</p>
<ul>
<li>递归剔除（Recursive elimination ）</li>
</ul>
<p>反复的训练模型，并剔除每次的最优或者最差的特征，将剔除完毕的特征集进入下一轮训练，直到所有的特征被剔除，被剔除的顺序度量了特征的重要程度。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/14/自然语言处理3-文本处理技能基础/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/14/自然语言处理3-文本处理技能基础/" itemprop="url">自然语言处理3:文本处理技能基础</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-14T14:13:14+08:00">
                2019-04-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="NPL之基本文本处理技能"><a href="#NPL之基本文本处理技能" class="headerlink" title="NPL之基本文本处理技能"></a>NPL之基本文本处理技能</h1><h2 id="1-分词的概念"><a href="#1-分词的概念" class="headerlink" title="1.分词的概念"></a>1.分词的概念</h2><p>一个良好的分词系统应由词典和统计两套系统组成。后者为前者构造可持续更新的词典，识别新词，同时对消岐部分进行匹配。在分词过程中，好的词典很重要，其次算法要跟着需求走，不同需求选择不同算法，比如有些要求速度快，与兴趣相关，此时算法是次要的，而有些需求注重的是精度。</p>
<p>现有方法：基于词典的匹配：前向最大匹配，后向最大匹配；基于字的标注：最大熵模型，条件随机场模型，感知器模型；其他方法：与词性标注集合，与句法分析结合。</p>
<h2 id="2-分词最大向量化，逆向最大，双向最大匹配法"><a href="#2-分词最大向量化，逆向最大，双向最大匹配法" class="headerlink" title="2.分词最大向量化，逆向最大，双向最大匹配法"></a>2.分词最大向量化，逆向最大，双向最大匹配法</h2><h5 id="1-分词最大正向匹配法（示例来自网络）"><a href="#1-分词最大正向匹配法（示例来自网络）" class="headerlink" title="1.分词最大正向匹配法（示例来自网络）"></a>1.分词最大正向匹配法（示例来自网络）</h5><p>1、正向最大匹配法：<br>正向即从前往后取词，从7-&gt;1，每次减一个字，直到词典命中或剩下1个单字。<br>第1次：“我们在野生动物”，扫描7字词典，无<br>第2次：“我们在野生动”，扫描6字词典，无<br>。。。。<br>第6次：“我们”，扫描2字词典，有<br>扫描中止，输出第1个词为“我们”，去除第1个词后开始第2轮扫描，即：<br>第2轮扫描：<br>第1次：“在野生动物园玩”，扫描7字词典，无<br>第2次：“在野生动物园”，扫描6字词典，无<br>。。。。<br>第6次：“在野”，扫描2字词典，有<br>扫描中止，输出第2个词为“在野”，去除第2个词后开始第3轮扫描，即：<br>第3轮扫描：<br>第1次：“生动物园玩”，扫描5字词典，无<br>第2次：“生动物园”，扫描4字词典，无<br>第3次：“生动物”，扫描3字词典，无<br>第4次：“生动”，扫描2字词典，有<br>扫描中止，输出第3个词为“生动”，第4轮扫描，即：<br>第4轮扫描：<br>第1次：“物园玩”，扫描3字词典，无<br>第2次：“物园”，扫描2字词典，无<br>第3次：“物”，扫描1字词典，无<br>扫描中止，输出第4个词为“物”，非字典词数加1，开始第5轮扫描，即：<br>第5轮扫描：<br>第1次：“园玩”，扫描2字词典，无<br>第2次：“园”，扫描1字词典，有<br>扫描中止，输出第5个词为“园”，单字字典词数加1，开始第6轮扫描，即：<br>第6轮扫描：<br>第1次：“玩”，扫描1字字典词，有<br>扫描中止，输出第6个词为“玩”，单字字典词数加1，整体扫描结束。<br>正向最大匹配法，最终切分结果为：“我们/在野/生动/物/园/玩”，其中，单字字典词为2，非词典词为1。</p>
<h5 id="2-逆向最大匹配法"><a href="#2-逆向最大匹配法" class="headerlink" title="2.逆向最大匹配法"></a>2.逆向最大匹配法</h5><p>逆向即从后往前取词，其他逻辑和正向相同。即：<br>第1轮扫描：“在野生动物园玩”<br>第1次：“在野生动物园玩”，扫描7字词典，无<br>第2次：“野生动物园玩”，扫描6字词典，无<br>。。。。<br>第7次：“玩”，扫描1字词典，有<br>扫描中止，输出“玩”，单字字典词加1，开始第2轮扫描<br>第2轮扫描：“们在野生动物园”<br>第1次：“们在野生动物园”，扫描7字词典，无<br>第2次：“在野生动物园”，扫描6字词典，无<br>第3次：“野生动物园”，扫描5字词典，有<br>扫描中止，输出“野生动物园”，开始第3轮扫描<br>第3轮扫描：“我们在”<br>第1次：“我们在”，扫描3字词典，无<br>第2次：“们在”，扫描2字词典，无<br>第3次：“在”，扫描1字词典，有<br>扫描中止，输出“在”，单字字典词加1，开始第4轮扫描<br>第4轮扫描：“我们”<br>第1次：“我们”，扫描2字词典，有<br>扫描中止，输出“我们”，整体扫描结束。<br>逆向最大匹配法，最终切分结果为：“我们/在/野生动物园/玩”，其中，单字字典词为2，非词典词为0。</p>
<h5 id="3-双向最大匹配法"><a href="#3-双向最大匹配法" class="headerlink" title="3.双向最大匹配法"></a>3.双向最大匹配法</h5><p>正向最大匹配法和逆向最大匹配法，都有其局限性(如：长春药店，逆向切分为“长/春药店”），因此有人又提出了双向最大匹配法，双向最大匹配法。即，两种算法都切一遍，然后根据大颗粒度词越多越好，非词典词和单字词越少越好的原则，选取其中一种分词结果输出。<br>如：“我们在野生动物园玩”<br>正向最大匹配法，最终切分结果为：“我们/在野/生动/物/园/玩”，其中，两字词3个，单字字典词为2，非词典词为1。<br>逆向最大匹配法，最终切分结果为：“我们/在/野生动物园/玩”，其中，五字词1个，两字词1个，单字字典词为2，非词典词为0。<br>非字典词：正向(1)&gt;逆向(0)（越少越好）<br>单字字典词：正向(2)=逆向(2)（越少越好）<br>总词数：正向(6)&gt;逆向(4)（越少越好）<br>因此最终输出为逆向结果。</p>
<h2 id="2-Python标准库——collections模块的Counter类"><a href="#2-Python标准库——collections模块的Counter类" class="headerlink" title="2.Python标准库——collections模块的Counter类"></a>2.Python标准库——collections模块的Counter类</h2><p>Counter类的目的是用来跟踪值出现的次数。它是一个无序的容器类型，以字典的键值对形式存储，其中元素作为key，其计数作为value。计数值可以是任意的Interger（包括0和负数）</p>
<h6 id="1-创建"><a href="#1-创建" class="headerlink" title="1.创建"></a>1.创建</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; c = Counter()  # 创建一个空的Counter类</span><br><span class="line">&gt;&gt;&gt; c = Counter(&apos;gallahad&apos;)  # 从一个可iterable对象（list、tuple、dict、字符串等）创建</span><br><span class="line">&gt;&gt;&gt; c = Counter(&#123;&apos;a&apos;: 4, &apos;b&apos;: 2&#125;)  # 从一个字典对象创建</span><br><span class="line">&gt;&gt;&gt; c = Counter(a=4, b=2)  # 从一组键值对创建</span><br></pre></td></tr></table></figure>
<h6 id="2-2-计数值的访问与缺失的键"><a href="#2-2-计数值的访问与缺失的键" class="headerlink" title="2.2 计数值的访问与缺失的键"></a>2.2 计数值的访问与缺失的键</h6><p>当所访问的键不存在时，返回0，而不是KeyError；否则返回它的计数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; c = Counter(&quot;abcdefgab&quot;)</span><br><span class="line">&gt;&gt;&gt; c[&quot;a&quot;]</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; c[&quot;c&quot;]</span><br><span class="line">1</span><br><span class="line">&gt;&gt;&gt; c[&quot;h&quot;]</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<h5 id="3-计数器的更新（update和subtract）"><a href="#3-计数器的更新（update和subtract）" class="headerlink" title="3. 计数器的更新（update和subtract）"></a>3. 计数器的更新（update和subtract）</h5><p>可以使用一个iterable对象或者另一个Counter对象来更新键值。<br>计数器的更新包括增加和减少两种。其中，增加使用update()方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; c = Counter(&apos;which&apos;)</span><br><span class="line">&gt;&gt;&gt; c.update(&apos;witch&apos;)  # 使用另一个iterable对象更新</span><br><span class="line">&gt;&gt;&gt; c[&apos;h&apos;]</span><br><span class="line">3</span><br><span class="line">&gt;&gt;&gt; d = Counter(&apos;watch&apos;)</span><br><span class="line">&gt;&gt;&gt; c.update(d)  # 使用另一个Counter对象更新</span><br><span class="line">&gt;&gt;&gt; c[&apos;h&apos;]</span><br><span class="line">4</span><br></pre></td></tr></table></figure>
<h5 id="4-元素的删除"><a href="#4-元素的删除" class="headerlink" title="4.元素的删除"></a>4.元素的删除</h5><p>删除元素使用del</p>
<h5 id="5-elements"><a href="#5-elements" class="headerlink" title="5 elements()"></a>5 elements()</h5><p>返回一个迭代器。元素被重复了多少次，在该迭代器中就包含多少个该元素。元素排列无确定顺序，个数小于1的元素不被包含。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; c = Counter(a=4, b=2, c=0, d=-2)</span><br><span class="line">&gt;&gt;&gt; list(c.elements())</span><br><span class="line">[&apos;a&apos;, &apos;a&apos;, &apos;a&apos;, &apos;a&apos;, &apos;b&apos;, &apos;b&apos;]</span><br></pre></td></tr></table></figure>
<h5 id="6-most-common-n"><a href="#6-most-common-n" class="headerlink" title="6.most_common([n])"></a>6.most_common([n])</h5><p>返回一个TopN列表。如果n没有被指定，则返回所有元素。当多个元素计数值相同时，排列是无确定顺序的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; c = Counter(&apos;abracadabra&apos;)</span><br><span class="line">&gt;&gt;&gt; c.most_common()</span><br><span class="line">[(&apos;a&apos;, 5), (&apos;r&apos;, 2), (&apos;b&apos;, 2), (&apos;c&apos;, 1), (&apos;d&apos;, 1)]</span><br><span class="line">&gt;&gt;&gt; c.most_common(3)</span><br><span class="line">[(&apos;a&apos;, 5), (&apos;r&apos;, 2), (&apos;b&apos;, 2)]</span><br></pre></td></tr></table></figure>
<h5 id="7-常规操作"><a href="#7-常规操作" class="headerlink" title="7.常规操作"></a>7.常规操作</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sum(c.values())  # 所有计数的总数</span><br><span class="line">c.clear()  # 重置Counter对象，注意不是删除</span><br><span class="line">list(c)  # 将c中的键转为列表</span><br><span class="line">set(c)  # 将c中的键转为set</span><br><span class="line">dict(c)  # 将c中的键值对转为字典</span><br><span class="line">c.items()  # 转为(elem, cnt)格式的列表</span><br><span class="line">Counter(dict(list_of_pairs))  # 从(elem, cnt)格式的列表转换为Counter类对象</span><br><span class="line">c.most_common()[:-n:-1]  # 取出计数最少的n-1个元素</span><br><span class="line">c += Counter()  # 移除0和负值</span><br></pre></td></tr></table></figure>
<h2 id="3-语言模型中unigram、bigram、trigram的概念"><a href="#3-语言模型中unigram、bigram、trigram的概念" class="headerlink" title="3.语言模型中unigram、bigram、trigram的概念"></a>3.语言模型中unigram、bigram、trigram的概念</h2><ol>
<li>unigram 一元分词，把句子分成一个一个的汉字</li>
<li>bigram 二元分词，把句子从头到尾每两个字组成一个词语</li>
<li>trigram 三元分词，把句子从头到尾每三个字组成一个词语.</li>
</ol>
<h5 id="n-gram模型的概念"><a href="#n-gram模型的概念" class="headerlink" title="n-gram模型的概念"></a>n-gram模型的概念</h5><p>n-gram模型也称为n-1阶马尔科夫模型，它有一个有限历史假设：当前词的出现概率仅仅与前面n-1个词相关。因此(1)式可以近似为：</p>
<p>当n取1、2、3时，n-gram模型分别称为unigram、bigram和trigram语言模型。n-gram模型的参数就是条件概率</p>
<p>假设词表的大小为100,000，那么n-gram模型的参数数量为</p>
<p>n越大，模型越准确，也越复杂，需要的计算量越大。最常用的是bigram，其次是unigram和trigram，n取≥4的情况较少。</p>
<h2 id="4-文本矩阵化-要求采用结巴分词来进行分词操作"><a href="#4-文本矩阵化-要求采用结巴分词来进行分词操作" class="headerlink" title="4.文本矩阵化(要求采用结巴分词来进行分词操作)"></a>4.文本矩阵化(要求采用结巴分词来进行分词操作)</h2><p>分词（可采用结巴分词来进行分词操作，其他库也可以）；去停用词；构造词表。<br>每篇文档的向量化。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import jieba</span><br><span class="line">import re</span><br><span class="line">stopwords = &#123;&#125;</span><br><span class="line">fstop = open(&apos;stop_words.txt&apos;, &apos;r&apos;,encoding=&apos;utf-8&apos;,errors=&apos;ingnore&apos;)</span><br><span class="line">for eachWord in fstop:</span><br><span class="line">    stopwords[eachWord.strip()] = eachWord.strip()  #停用词典</span><br><span class="line">fstop.close()</span><br><span class="line">f1=open(&apos;tt1.txt&apos;,&apos;r&apos;,encoding=&apos;utf-8&apos;,errors=&apos;ignore&apos;)</span><br><span class="line">f2=open(&apos;fenci.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">line=f1.readline()</span><br><span class="line">while line:</span><br><span class="line">    line = line.strip()  #去前后的空格</span><br><span class="line">    line = re.sub(r&quot;[0-9\s+\.\!\/_,$%^*()?;；:-【】+\&quot;\&apos;]+|[+——！，;:。？、~@#￥%……&amp;*（）]+&quot;, &quot; &quot;, line) #去标点符号</span><br><span class="line">    seg_list=jieba.cut(line,cut_all=False)  #结巴分词</span><br><span class="line">    outStr=&quot;&quot;</span><br><span class="line">    for word in seg_list:</span><br><span class="line">        if word not in stopwords:</span><br><span class="line">            outStr+=word</span><br><span class="line">            outStr+=&quot; &quot;</span><br><span class="line">    f2.write(outStr)</span><br><span class="line">    line=f1.readline()</span><br><span class="line">f1.close()</span><br><span class="line">f2.close()</span><br></pre></td></tr></table></figure>
<p>##### </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/14/KNN算法原理及代码实现/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/14/KNN算法原理及代码实现/" itemprop="url">KNN算法原理及代码实现</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-14T00:51:31+08:00">
                2019-04-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="k邻近算法的原理和使用"><a href="#k邻近算法的原理和使用" class="headerlink" title="k邻近算法的原理和使用"></a>k邻近算法的原理和使用</h1><h3 id="1-算法原理"><a href="#1-算法原理" class="headerlink" title="1.算法原理"></a>1.算法原理</h3><h6 id="1-KNN算法三要素"><a href="#1-KNN算法三要素" class="headerlink" title="1. KNN算法三要素"></a>1. KNN算法三要素</h6><blockquote>
<p>KNN算法我们主要要考虑三个重要的要素，对于固定的训练集，只要这三点确定了，算法的预测方式也就决定了。这三个最终的要素是k值的选取，距离度量的方式和分类决策规则。</p>
</blockquote>
<ol>
<li>对于分类决策规则，一般都是使用前面提到的多数表决法。所以我们重点是关注与k值的选择和距离的度量方式。</li>
</ol>
<ol start="2">
<li>对于k值的选择，没有一个固定的经验，一般根据样本的分布，选择一个较小的值，可以通过交叉验证选择一个合适的k值。</li>
</ol>
<ul>
<li>选择较小的k值，就相当于用较小的领域中的训练实例进行预测，训练误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是泛化误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；</li>
<li>选择较大的k值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少泛化误差，但缺点是训练误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。</li>
<li>一个极端是k等于样本数m，则完全没有分类，此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单。</li>
</ul>
<ol start="3">
<li>对于距离的度量，我们有很多的距离度量方式，但是最常用的是欧式距离。</li>
</ol>
<h5 id="2-核心思想"><a href="#2-核心思想" class="headerlink" title="2.核心思想"></a>2.核心思想</h5><blockquote>
<p>计算待标记的数据样本和数据集中的每个样本的距离，取距离最近的K个样本，待标记的数据样本就由这k个距离最近的样本投票产生</p>
</blockquote>
<p>算法原理的伪代码</p>
<ul>
<li>遍历x_train中的所有样本，计算每个样本与X_test的距离，并吧距离保存在Distance数组中 </li>
<li>对Distance数组进行排序，取距离最近的k个点，即为X_knn</li>
<li>对X-knn中统计每个类别的个数，即在class0在X_knn中有几个样本，class1在X_knn中有几个样本…..</li>
<li>待标记的样本类别，就是在X_knn中样本个数最多的那个类别。</li>
</ul>
<h5 id="3-算法优缺点"><a href="#3-算法优缺点" class="headerlink" title="3.算法优缺点"></a>3.算法优缺点</h5><ul>
<li><em>优点</em>:准确性高，对异常值和噪声有较高的容忍度</li>
<li><em>缺点</em>:计算量大，对内存需求大<h3 id="2-简单运用-使用k邻近算法进行分类"><a href="#2-简单运用-使用k邻近算法进行分类" class="headerlink" title="2.简单运用(使用k邻近算法进行分类)"></a>2.简单运用(使用k邻近算法进行分类)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets.samples_generator import make_blobs</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">#生成数据</span><br><span class="line"></span><br><span class="line">centers=[[-2,2],[2,2],[0,4]]</span><br><span class="line">X,y=make_blobs(n_samples=600,centers=centers,random_state=0,cluster_std=0.6)</span><br><span class="line"></span><br><span class="line">#画出数据</span><br><span class="line">plt.figure(figsize=(16,10),dpi=144)</span><br><span class="line">c=np.array(centers)</span><br><span class="line">plt.scatter(X[:,0],X[:,1],cmap=&apos;cool&apos;,c=y,s=100)</span><br><span class="line">plt.scatter(c[:,0],c[:,1],s=100,marker=&apos;^&apos;,c=&apos;orange&apos;)</span><br><span class="line"></span><br><span class="line">#调用训练器来进行分类</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">k=5</span><br><span class="line">#训练模型</span><br><span class="line">clf=KNeighborsClassifier()</span><br><span class="line">clf.fit(X,y)</span><br><span class="line"></span><br><span class="line">#预测</span><br><span class="line">X_sample=[[0,2]]</span><br><span class="line">y_sample=clf.predict(X_sample)</span><br><span class="line">neighbors=clf.kneighbors(X_sample,return_distance=False)</span><br><span class="line"></span><br><span class="line">#画出该点和临近点</span><br><span class="line">print(X_sample)</span><br><span class="line">print(neighbors)</span><br><span class="line">print(X_sample[0][0])</span><br><span class="line">plt.scatter(X_sample[0][0],X_sample[0][1],c=&quot;red&quot;,s=100)</span><br><span class="line"></span><br><span class="line">for i in neighbors[0]:</span><br><span class="line">    #print(X[i][0])</span><br><span class="line">    plt.plot([X[i][0],X_sample[0][0]],[X[i][1],X_sample[0][1]],&apos;k--&apos;,linewidth=0.8);</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="/2019/04/14/KNN算法原理及代码实现/捕获.PNG" alt=""></p>
<h3 id="3-示例运用-使用k邻近算法进行回归拟合"><a href="#3-示例运用-使用k邻近算法进行回归拟合" class="headerlink" title="3.示例运用(使用k邻近算法进行回归拟合)"></a>3.示例运用(使用k邻近算法进行回归拟合)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#生成数据集</span><br><span class="line">import numpy as np</span><br><span class="line">n_dots=400</span><br><span class="line">X=5*np.random.rand(n_dots,1)</span><br><span class="line">y=np.cos(X).ravel()#将多为数组将为一维</span><br><span class="line">#print(X)</span><br><span class="line">print(&quot;******************&quot;)</span><br><span class="line">#print(y)</span><br><span class="line"></span><br><span class="line">#添加一些噪声</span><br><span class="line">y=y+0.2*np.random.rand(n_dots)-0.1</span><br><span class="line"></span><br><span class="line">#训练模型</span><br><span class="line">from sklearn.neighbors import KNeighborsRegressor</span><br><span class="line">k=5</span><br><span class="line">knn=KNeighborsRegressor(k)</span><br><span class="line">knn.fit(X,y)</span><br><span class="line"></span><br><span class="line">#将所有的预测点用吸纳画起来，形成回归曲线</span><br><span class="line">T=np.linspace(0,5,500)[:,np.newaxis]#np.newaxis 在使用和功能上等价于 None，其实就是 None 的一个别名. </span><br><span class="line">print(T.shape)</span><br><span class="line">y_pred=knn.predict(T)</span><br><span class="line">print(&quot;得分为&#123;0&#125;&quot;.format(knn.score(X,y)))</span><br><span class="line"></span><br><span class="line">#画出拟合曲线</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">plt.figure(figsize=(16,10),dpi=144)</span><br><span class="line">plt.scatter(X,y,c=&apos;g&apos;,label=&apos;data&apos;,s=100)#画出训练样本</span><br><span class="line">plt.plot(T,y_pred,c=&apos;k&apos;,label=&apos;prediction&apos;,lw=4)#画出拟合曲线（T为横坐标，y_pred为纵坐标</span><br><span class="line">plt.axis(&apos;tight&apos;)</span><br><span class="line">plt.title(&quot;KNeightborsRessor&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/14/KNN算法原理及代码实现/mm.PNG" alt=""></p>
<h3 id="4-实际运用"><a href="#4-实际运用" class="headerlink" title="4.实际运用"></a>4.实际运用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">![gg](C:\Users\wuxun\Desktop\gg.PNG)from sklearn import datasets</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">text=datasets.load_iris()</span><br><span class="line">#print(text)</span><br><span class="line">target=text.target</span><br><span class="line">data=text.data</span><br><span class="line">#print(target)</span><br><span class="line">print(&quot;***********&quot;)</span><br><span class="line">#print(data)</span><br><span class="line"></span><br><span class="line">data.shape</span><br><span class="line">#将样本分为训练集和测试集</span><br><span class="line">X_train,X_test,Y_train,Y_test=train_test_split(data,target,test_size=0.2)</span><br><span class="line"></span><br><span class="line">#通过使用普通的k-均值算法，带权重的k-均值算法，指定半斤的k-均值算法对数据进行拟合</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier,RadiusNeighborsClassifier</span><br><span class="line"></span><br><span class="line">#构造三个模型</span><br><span class="line"></span><br><span class="line">models=[]</span><br><span class="line">models.append((&quot;KNN&quot;,KNeighborsClassifier(n_neighbors=2)))</span><br><span class="line">models.append((&quot;KNN with weight&quot;,KNeighborsClassifier(n_neighbors2,weightsh=&quot;distance&quot;)))</span><br><span class="line">models.append((&quot;Radius Neighbors&quot;,RadiusNeighborsClassifier(n_neighbors=2,radius=500.0)))</span><br><span class="line"></span><br><span class="line">#分别训练三个模型</span><br><span class="line">results=[]</span><br><span class="line">for name,model in models:</span><br><span class="line">    model.fit(X_train,Y_train)</span><br><span class="line">    results.append((name,model.score(X_test,Y_test)))</span><br><span class="line">for i in range(len(results)):</span><br><span class="line">    print(&quot;name: &#123;&#125;: score:&#123;&#125;&quot;.format(results[i][0],results[i][1]))</span><br><span class="line"></span><br><span class="line">#由于随机测验数据存在的差异，所以我们用KFold计算一般的算法性质</span><br><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">results=[]</span><br><span class="line">for name,model in models:</span><br><span class="line">    kfold=KFold(n_splits=10)#将数据集分成十份，一份作为验证集</span><br><span class="line">    cv_result=cross_val_score(model,data,target,cv=kfold)#函数计算出十次数据集组合而得的模型得分</span><br><span class="line">for i in range(len(results)):</span><br><span class="line">    print(&quot;name &#123;&#125; :scor &#123;&#125;&quot;.format(results[i][0],results[i][1].mean()))</span><br><span class="line">    </span><br><span class="line">#模型训练及分析</span><br><span class="line">#普通的knn训练模型可能更优，我们使用普通的knn模型训练数据</span><br><span class="line"></span><br><span class="line">knn=KNeighborsClassifier(n_neighbors=2)</span><br><span class="line">knn.fit(X_train,Y_train)</span><br><span class="line">train_score=knn.score(X_train,Y_train)</span><br><span class="line">test_score=knn.score(X_test,Y_test)</span><br><span class="line">print(&quot;tsin score:&#123;&#125;,test score:&#123;&#125;&quot;.format(train_score,test_score))</span><br><span class="line"></span><br><span class="line">from sklearn.model_selection import ShuffleSplit</span><br><span class="line">#from common.utils import plot_learning_curve</span><br><span class="line"></span><br><span class="line">#knn.KNeighborsClassifier(n_neighbors=2)</span><br><span class="line">#cv=ShuffleSplit(n_splits=10,test_size=0.2,random_state=0)</span><br><span class="line"></span><br><span class="line">from sklearn.feature_selection import SelectKBest#选择相关性最大的两个特征</span><br><span class="line"></span><br><span class="line">selector=SelectKBest(k=2)</span><br><span class="line">X_new=selector.fit_transform(data,target)</span><br><span class="line">X_new[0:5]</span><br><span class="line"></span><br><span class="line">#选择最好的两个特征在对三个模型进行检测</span><br><span class="line">results=[]</span><br><span class="line">for name,model in models:</span><br><span class="line">    kfold=KFold(n_splits=10)</span><br><span class="line">    cv_result=cross_val_score(model,X_new,target,cv=kfold)</span><br><span class="line">    results.append((name,cv_result))</span><br><span class="line">for i in range(len(results)):</span><br><span class="line">    print(&quot;name:&#123;&#125;; cross val score:&#123;&#125;&quot;.format(results[i][0],results[i][1].mean()))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">#画出二维数据</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">plt.figure(figsize=(10,6),dpi=200)</span><br><span class="line">plt.ylabel(&quot;BMI&quot;)</span><br><span class="line">plt.xlabel(&quot;Glucose&quot;)</span><br><span class="line">plt.scatter(X_new[:,0],X_new[:,1],c=target)</span><br><span class="line">#plt.scatter(X_new,target,c=&apos;r&apos;,s=20,marker=&apos;o&apos;)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/14/KNN算法原理及代码实现/gg.PNG" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/09/tensorflow-及机器学习基础知识/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/09/tensorflow-及机器学习基础知识/" itemprop="url">tensorflow 及机器学习基础知识</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-09T14:03:45+08:00">
                2019-04-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-召回率和查全率"><a href="#1-召回率和查全率" class="headerlink" title="1.召回率和查全率"></a>1.召回率和查全率</h2><p>召回率(Recall Rate,也叫查全率，是检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率，精度是检索出的相关文档数与检索出的文档总数的比率，衡量的是检索系统查准率。</p>
<p>召回率(Recall)和精度(Precise)是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量。</p>
<h2 id="1-ROC曲线"><a href="#1-ROC曲线" class="headerlink" title="1.ROC曲线"></a>1.ROC曲线</h2><p>1、roc曲线：接收者操作特征(receiveroperating characteristic),roc曲线上每个点反映着对同一信号刺激的感受性。</p>
<p><strong>横轴</strong>：负正类率(false postive rate FPR)特异度，划分实例中所有负例占所有负例的比例；(1-Specificity)</p>
<p><strong>纵轴</strong>：真正类率(true postive rate TPR)灵敏度，Sensitivity(正类覆盖率)</p>
<p>机器学习中常用的指标量</p>
<p><strong>TP</strong>:正确的肯定数目</p>
<p><strong>FN</strong>:漏报，没有找到正确匹配的数目</p>
<p><strong>FP</strong>:误报，没有的匹配不正确</p>
<p><strong>TN</strong>:正确拒绝的非匹配数目</p>
<p>由上表可得出横，纵轴的计算公式：</p>
<p>(1)真正类率(True Postive Rate)TPR: <strong>TP/(TP+FN)</strong>,代表分类器预测的<strong>正类中</strong>实际正实例占所有正实例的比例。Sensitivity</p>
<p>(2)负正类率(False Postive Rate)FPR: <strong>FP/(FP+TN)</strong>，代表分类器预测的<strong>正类中</strong>实际负实例占所有负实例的比例。1-Specificity</p>
<p>(3)真负类率(True Negative Rate)TNR: <strong>TN/(FP+TN)</strong>,代表分类器预测的<strong>负类中</strong>实际负实例占所有负实例的比例，<strong>TNR=1-FPR</strong>。Specificity</p>
<p>AUC(Area under Curve)：Roc曲线下的面积，介于0.1和1之间。Auc作为数值可以直观的评价分类器的好坏，值越大越好。</p>
<p><strong>首先AUC值是一个概率值，当你随机挑选一个正样本以及负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值，AUC值越大，当前分类算法越有可能将正样本排在负样本前面，从而能够更好地分类。</strong></p>
<h2 id="3-为什么使用Roc-和Auc评价分类器"><a href="#3-为什么使用Roc-和Auc评价分类器" class="headerlink" title="3. 为什么使用Roc 和Auc评价分类器"></a>3. 为什么使用Roc 和Auc评价分类器</h2><p>既然已经这么多标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现样本类不平衡，即正负样本比例差距较大，而且测试数据中的正负样本也可能随着时间变化。</p>
<h2 id="4-PR曲线"><a href="#4-PR曲线" class="headerlink" title="4.PR曲线"></a>4.PR曲线</h2><p>P-R曲线刻画查准率和查全率之间的关系，查准率指的是在所有预测为正例的数据中，真正例所占的比例，查全率是指预测为真正例的数据占所有正例数据的比例。<br> 即：查准率P=TP／(TP + FP) 查全率=TP／（TP+FN）<br> 查准率和查全率是一对矛盾的度量，一般来说，查准率高时，查全率往往偏低，查全率高时，查准率往往偏低，例如，若希望将好瓜尽可能多选出来，则可通过增加选瓜的数量来实现，如果希望将所有的西瓜都选上，那么所有的好瓜必然都被选上了，但这样查准率就会较低；若希望选出的瓜中好瓜比例尽可能高，则可只挑选最有把握的瓜，但这样就难免会漏掉不少好瓜，使得查全率较低。</p>
<p>在很多情况下，我们可以根据学习器的预测结果对样例进行排序，排在前面的是学习器认为最可能是正例的样本，排在后面的是学习器认为最不可能是正例的样本，按此顺序逐个把样本作为正例进行预测，则每次可计算当前的查全率和查准率，以查准率为y轴，以查全率为x轴，可以画出下面的P-R曲线。</p>
<p><img src="/2019/04/09/tensorflow-及机器学习基础知识/pr曲线.png" alt="PR曲线"></p>
<h2 id="5-IMDB影评数据集"><a href="#5-IMDB影评数据集" class="headerlink" title="5.IMDB影评数据集"></a>5.IMDB影评数据集</h2><p>该数据下载后包含train和test两个文件夹和三个文件，其中test文件夹中的两个文件夹pos和neg分别为1.25W个代表积极和消极态度的训练样本。而train中的三个文件夹pos、neg、unsup分别为1.25W代表积极和消极态度的训练样本以及5W个未标记的样本。未标记样本可以用来作无监督学习时使用。</p>
<h2 id="THUCnews数据集"><a href="#THUCnews数据集" class="headerlink" title="THUCnews数据集"></a>THUCnews数据集</h2><p>THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。我们在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/07/用skikit-learn实现决策树/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/07/用skikit-learn实现决策树/" itemprop="url">用skikit-learn实现决策树</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-07T21:42:41+08:00">
                2019-04-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="用scikit-learn实现决策树算法"><a href="#用scikit-learn实现决策树算法" class="headerlink" title="用scikit-learn实现决策树算法"></a>用scikit-learn实现决策树算法</h1><h2 id="1、scikit-learn决策树算法类库介绍"><a href="#1、scikit-learn决策树算法类库介绍" class="headerlink" title="1、scikit-learn决策树算法类库介绍"></a>1、scikit-learn决策树算法类库介绍</h2><p>​         scikit-learn决策树算法类库内部实现是使用了调优过的CART树算法，既可以做分类，又可以做回归。分类决策树的类对应的是DecisionTreeClassifier，而回归决策树的类对应的是DecisionTreeRegressor。两者的参数定义几乎完全相同，但是意义不全相同。下面就对DecisionTreeClassifier和DecisionTreeRegressor的重要参数做一个总结，重点比较两者参数使用的不同点和调参的注意点。 </p>
<h2 id="2、DecisionTreeClassifier和DecisionTreeClassifier-重要参数调参注意点"><a href="#2、DecisionTreeClassifier和DecisionTreeClassifier-重要参数调参注意点" class="headerlink" title="2、DecisionTreeClassifier和DecisionTreeClassifier 重要参数调参注意点"></a>2、DecisionTreeClassifier和DecisionTreeClassifier 重要参数调参注意点</h2><h3 id="2-1-特征选择标准criterion"><a href="#2-1-特征选择标准criterion" class="headerlink" title="2.1 特征选择标准criterion"></a>2.1 特征选择标准criterion</h3><p>​        DecisionTreeClassifier：可以使用”gini”或者”entropy”，前者代表基尼系数，后者代表信息增益。一般说使用默认的基尼系数”gini”就可以了，即CART算法。除非你更喜欢类似ID3, C4.5的最优特征选择方法。 </p>
<p>​        DecisionTreeRegressor：可以使用”mse”或者”mae”，前者是均方差，后者是和均值之差的绝对值之和。推荐使用默认的”mse”。一般来说”mse”比”mae”更加精确。除非你想比较二个参数的效果的不同之处。</p>
<h3 id="2-2-特征划分点选择标准splitter"><a href="#2-2-特征划分点选择标准splitter" class="headerlink" title="2.2 特征划分点选择标准splitter"></a>2.2 特征划分点选择标准splitter</h3><p>​        可以使用”best”或者”random”。前者在特征的所有划分点中找出最优的划分点。后者是随机的在部分划分点中找局部最优的划分点。<br>​         默认的”best”适合样本量不大的时候，而如果样本数据量非常大，此时决策树构建推荐”random” </p>
<h3 id="2-3-划分时考虑的最大特征数max-features"><a href="#2-3-划分时考虑的最大特征数max-features" class="headerlink" title="2.3 划分时考虑的最大特征数max_features"></a>2.3 划分时考虑的最大特征数max_features</h3><p>​        可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数；如果是”log2”意味着划分时最多考虑log2N个特征；如果是”sqrt”或者”auto”意味着划分时最多考虑sqrt(N)个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。<br>​         一般来说，如果样本特征数不多，比如小于50，我们用默认的”None”就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。</p>
<h3 id="2-4-决策树最大深max-depth"><a href="#2-4-决策树最大深max-depth" class="headerlink" title="2.4 决策树最大深max_depth"></a>2.4 决策树最大深max_depth</h3><p>​        决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。</p>
<h3 id="2-5-内部节点再划分所需最小样本数min-samples-split"><a href="#2-5-内部节点再划分所需最小样本数min-samples-split" class="headerlink" title="2.5 内部节点再划分所需最小样本数min_samples_split"></a>2.5 内部节点再划分所需最小样本数min_samples_split</h3><p>​        这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。我之前的一个项目例子，有大概10万样本，建立决策树时，我选择了min_samples_split=10。可以作为参考。</p>
<h3 id="2-6-叶子节点最少样本数min-samples-leaf"><a href="#2-6-叶子节点最少样本数min-samples-leaf" class="headerlink" title="2.6 叶子节点最少样本数min_samples_leaf"></a>2.6 叶子节点最少样本数min_samples_leaf</h3><p>​        这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1，可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。</p>
<h3 id="2-7-叶子节点最小的样本权重和min-weight-fraction-leaf"><a href="#2-7-叶子节点最小的样本权重和min-weight-fraction-leaf" class="headerlink" title="2.7 叶子节点最小的样本权重和min_weight_fraction_leaf"></a>2.7 叶子节点最小的样本权重和min_weight_fraction_leaf</h3><p>​        这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。</p>
<h3 id="2-8-最大叶子节点数max-leaf-nodes"><a href="#2-8-最大叶子节点数max-leaf-nodes" class="headerlink" title="2.8 最大叶子节点数max_leaf_nodes"></a>2.8 最大叶子节点数max_leaf_nodes</h3><p>​        通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。</p>
<h3 id="2-9-类别权重class-weight"><a href="#2-9-类别权重class-weight" class="headerlink" title="2.9 类别权重class_weight"></a>2.9 类别权重class_weight</h3><p>​        不适用于回归树。指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重，或者用“balanced”，如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。当然，如果你的样本类别分布没有明显的偏倚，则可以不管这个参数，选择默认的”None”</p>
<h3 id="2-10-节点划分最小不纯度min-impurity-split"><a href="#2-10-节点划分最小不纯度min-impurity-split" class="headerlink" title="2.10 节点划分最小不纯度min_impurity_split"></a>2.10 节点划分最小不纯度min_impurity_split</h3><p>​        这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。</p>
<h3 id="2-11-数据是否预排序presort"><a href="#2-11-数据是否预排序presort" class="headerlink" title="2.11 数据是否预排序presort"></a>2.11 数据是否预排序presort</h3><p>​        这个值是布尔值，默认是False不排序。一般来说，如果样本量少或者限制了一个深度很小的决策树，设置为true可以让划分点选择更加快，决策树建立的更加快。如果样本量太大的话，反而没有什么好处。问题是样本量少的时候，我速度本来就不慢。所以这个值一般懒得理它就可以了。</p>
<p>​        除了这些参数要注意以外，其他在调参时的注意点有：<br> 　　1）当样本少数量但是样本特征非常多的时候，决策树很容易过拟合，一般来说，样本数比特征数多一些会比较容易建立健壮的模型<br> 　　2）如果样本数量少但是样本特征非常多，在拟合决策树模型前，推荐先做维度规约，比如主成分分析（PCA），特征选择（Losso）或者独立成分分析（ICA）。这样特征的维度会大大减小。再来拟合决策树模型效果会好。<br>​     　3）推荐多用决策树的可视化（下节会讲），同时先限制决策树的深度（比如最多3层），这样可以先观察下生成的决策树里数据的初步拟合情况，然后再决定是否要增加深度。<br> 　　4）在训练模型前，注意观察样本的类别情况（主要指分类树），如果类别分布非常不均匀，就要考虑用class_weight来限制模型过于偏向样本多的类别。<br> 　　5）决策树的数组使用的是numpy的float32类型，如果训练数据不是这样的格式，算法会先做copy再运行。<br> 　　6）如果输入的样本矩阵是稀疏的，推荐在拟合前调用csc_matrix稀疏化，在预测前调用csr_matrix稀疏化</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Yu-shui</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">43</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Yu-shui" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yu-shui</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count"></span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
