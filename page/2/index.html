<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="Yu-shui">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Yu-shui">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Yu-shui">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/">





  <title>Yu-shui</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
    <a href="https://github.com/Yu-shui" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yu-shui</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>
            
            日程表
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/12/数据分析任务1/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/12/数据分析任务1/" itemprop="url">数据分析任务1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-12T19:38:47+08:00">
                2019-05-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="数据分析任务-1"><a href="#数据分析任务-1" class="headerlink" title="数据分析任务(1)"></a>数据分析任务(1)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.preprocessing import Imputer</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">#导入数据</span><br><span class="line">data = pd.read_csv(&apos;D:\statistical\data.csv&apos;, encoding=&apos;gbk&apos;)</span><br><span class="line"></span><br><span class="line">#查看数据概况</span><br><span class="line">data.info()</span><br><span class="line"></span><br><span class="line">#查看数据前5行</span><br><span class="line">data.head()</span><br><span class="line"></span><br><span class="line">#统计每列的缺失值个数</span><br><span class="line">data.isnull().sum().sort_values(ascending=False)</span><br><span class="line"></span><br><span class="line">#&apos;student_feature&apos; 这列考虑删除, 或者将NA以0填充</span><br><span class="line">data[[&apos;student_feature&apos;, &apos;status&apos;]].groupby(data[&apos;status&apos;]).count()</span><br><span class="line"></span><br><span class="line">cat_vars = []</span><br><span class="line">def fea_categorical_check(df):</span><br><span class="line">    print(&apos;描述变量有:\n&apos;)</span><br><span class="line">    for col in df.columns:</span><br><span class="line">        if df[col].dtype == &apos;object&apos;:</span><br><span class="line">            print(col)</span><br><span class="line">            cat_vars.append(col)</span><br><span class="line">    return cat_vars</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">#测试集30%，训练集70%，随机种子设置为2018</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">x_train,x_test,y_train,y_test=train_test_split(data,y_data,train_size=0.7,random_state=2018</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/06/逻辑回归/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/06/逻辑回归/" itemprop="url">逻辑回归</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-06T23:44:12+08:00">
                2019-05-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="逻辑回归基础"><a href="#逻辑回归基础" class="headerlink" title="逻辑回归基础"></a>逻辑回归基础</h1><p><strong>所以逻辑回归，不是回归，而是分类器，二分类，多分类。</strong></p>
<h3 id="1-使用sigmoid函数，对y的取值进行压缩至-0-1"><a href="#1-使用sigmoid函数，对y的取值进行压缩至-0-1" class="headerlink" title="1.使用sigmoid函数，对y的取值进行压缩至[0,1]"></a>1.使用sigmoid函数，对y的取值进行压缩至[0,1]</h3><p><img src="/2019/05/06/逻辑回归/nn1.PNG" alt=""></p>
<p>图像如下：</p>
<p><img src="/2019/05/06/逻辑回归/nn2.PNG" alt=""></p>
<h3 id="2-损失函数"><a href="#2-损失函数" class="headerlink" title="2.损失函数"></a>2.损失函数</h3><p>真实值与预测值之间的误差的函数，我们希望这个函数越小越好。在这里，最小损失是0。</p>
<p><img src="/2019/05/06/逻辑回归/nn3.PNG" alt=""></p>
<h3 id="3-在sklearm中的应用"><a href="#3-在sklearm中的应用" class="headerlink" title="3.在sklearm中的应用"></a>3.在sklearm中的应用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">sklearn.linear_model.LogisticRegression(penalty=l2, # 惩罚项，可选l1,l2，对参数约束，减少过拟合风险</span><br><span class="line"></span><br><span class="line">                                        dual=False, # 对偶方法（原始问题和对偶问题），用于求解线性多核（liblinear)的L2的惩罚项上。样本数大于特征数时设置False</span><br><span class="line"></span><br><span class="line">                                        tol=0.0001, # 迭代停止的条件，小于等于这个值停止迭代，损失迭代到的最小值。</span><br><span class="line"></span><br><span class="line">                                        C=1.0, # 正则化系数λ的倒数，越小表示越强的正则化。</span><br><span class="line"></span><br><span class="line">                                        fit_intercept=True, # 是否存在截距值，即b</span><br><span class="line"></span><br><span class="line">                                        intercept_scaling=1, #</span><br><span class="line"></span><br><span class="line">                                        class_weight=None, # 类别的权重，样本类别不平衡时使用，设置balanced会自动调整权重。为了平横样本类别比例，类别样本多的，权重低，类别样本少的，权重高。</span><br><span class="line"></span><br><span class="line">                                        random_state=None, # 随机种子</span><br><span class="line"></span><br><span class="line">                                        solver=’liblinear’, # 优化算法的参数，包括newton-cg,lbfgs,liblinear,sag,saga,对损失的优化的方法</span><br><span class="line"></span><br><span class="line">                                        max_iter=100,# 最大迭代次数，</span><br><span class="line"></span><br><span class="line">                                        multi_class=’ovr’,# 多分类方式，有‘ovr&apos;,&apos;mvm&apos;</span><br><span class="line"></span><br><span class="line">                                        verbose=0, # 输出日志，设置为1，会输出训练过程的一些结果</span><br><span class="line"></span><br><span class="line">                                        warm_start=False, # 热启动参数，如果设置为True,则下一次训练是以追加树的形式进行（重新使用上一次的调用作为初始化）</span><br><span class="line"></span><br><span class="line">                                        n_jobs=1 # 并行数，设置为1，用1个cpu运行，设置-1，用你电脑的所有cpu运行程序</span><br><span class="line"></span><br><span class="line">                                            )</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/02/干货/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/02/干货/" itemprop="url">干货</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-02T01:54:48+08:00">
                2019-05-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/27/神经网络基础-1/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/27/神经网络基础-1/" itemprop="url">神经网络基础</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-27T16:38:17+08:00">
                2019-04-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h1><h3 id="1-RNN结构"><a href="#1-RNN结构" class="headerlink" title="1.RNN结构"></a>1.RNN结构</h3><h5 id="1-背景"><a href="#1-背景" class="headerlink" title="(1).背景"></a>(1).背景</h5><p>DNN以及CNN在对样本提取特征的时候，样本与样本之间是独立的，而有些情况是无法把每个输入的样本都看作是独立的，比如NLP中的此行标注问题，ASR中每个音素都和前一个音素是相关的，这类问题可以看做一种带有时序序列的问题，无法将样本看做是相互独立的，因此单纯的DNN和CNN解决这类问题就比较棘手。此时RNN就是一种解决这类问题很好的模型。</p>
<p><img src="/2019/04/27/神经网络基础-1/nn1.PNG" alt=""></p>
<p>特点：</p>
<ul>
<li><p>权重W,U,V是共享的，以此减少参数量</p>
</li>
<li><p>第t时刻的输出与第t-1时刻的输出有关</p>
<p><img src="/2019/04/27/神经网络基础-1/nn2.gif" alt=""></p>
</li>
</ul>
<p><img src="/2019/04/27/神经网络基础-1/nn3.gif" alt=""></p>
<p><img src="/2019/04/27/神经网络基础-1/nn4.PNG" alt=""></p>
<h5 id="2-优缺点，存在的问题"><a href="#2-优缺点，存在的问题" class="headerlink" title="(2).优缺点，存在的问题"></a>(2).优缺点，存在的问题</h5><p>DNN无法对<strong>时间序列</strong>上有变化的情况进行处理。然而，样本出现的时间顺序对于自然语言处理、语音识别、手写体识别等应用非常重要。因此出现了——循环神经网络RNN。<br>在普通的全连接网络或CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被成为前向神经网络(Feed-forward Neural Networks)。而在RNN中，<strong>神经元的输出可以在下一个时间段直接作用到自身</strong>，即第i层神经元在m时刻的输入，除了(i-1)层神经元在该时刻的输出外，还包括其自身在(m-1)时刻的输出！</p>
<p>但是出现了一个问题——<strong>“梯度消失”现象</strong>又要出现了，只不过这次发生在时间轴上。<br>所以RNN存在无法解决<strong>长时依赖</strong>的问题。为解决上述问题，提出了LSTM（长短时记忆单元），通过cell门开关实现时间上的记忆功能，并防止梯度消失.</p>
<h5 id="3-RNN如何实现参数更新-基于时间得反向传播算法-BPTT"><a href="#3-RNN如何实现参数更新-基于时间得反向传播算法-BPTT" class="headerlink" title="(3).RNN如何实现参数更新(基于时间得反向传播算法 BPTT)"></a>(3).RNN如何实现参数更新(基于时间得反向传播算法 BPTT)</h5><p>下图展示了在训练过程中的损失的产生：</p>
<p><img src="/2019/04/27/神经网络基础-1/nn5.gif" alt=""></p>
<p>详细的数学推导过程：</p>
<p>首先列出在推导过程中的一般计算公式：</p>
<p><img src="/2019/04/27/神经网络基础-1/nn6.gif" alt=""></p>
<p>将输出对应的损失求导：</p>
<p><img src="/2019/04/27/神经网络基础-1/nn7.gif" alt=""></p>
<p><img src="/2019/04/27/神经网络基础-1/nn8.gif" alt=""></p>
<p>接下来是对参数W,U,b进行更新，虽然三者在过程中是共享的，但是他们不止在t时刻做出了贡献，在t+1时刻也对隐藏层St+1做出了贡献，所以求导时，要从前往后一步步推导。</p>
<p><img src="/2019/04/27/神经网络基础-1/nn9.PNG" alt=""></p>
<p>三者有一个共同项，所以先求这个共同项：</p>
<p><img src="/2019/04/27/神经网络基础-1/nn10.PNG" alt=""></p>
<p>在求解激活函数导数时，是将已知的部分求导之后，然后将它和激活函数导数部分进行哈达马乘积。激活函数的导数一般是和前面的进行哈达马乘积，这里的激活函数是双曲正切，用矩阵中对角线元素表示向量中各个值的导数，可以去掉哈达马乘积，转化为矩阵乘法。</p>
<p><img src="/2019/04/27/神经网络基础-1/n11.PNG" alt=""></p>
<p>在求得了st以后，将其带回最初得公式对参数进行求导：</p>
<p><img src="/2019/04/27/神经网络基础-1/nn12.PNG" alt=""></p>
<p>在有了各个导数以后，对参数进行更新：</p>
<p><img src="/2019/04/27/神经网络基础-1/nn13.PNG" alt=""></p>
<h3 id="2-双向RNN"><a href="#2-双向RNN" class="headerlink" title="2.双向RNN"></a>2.双向RNN</h3><p>Bidirectional RNN(双向RNN)假设当前t的输出不仅仅和之前的序列有关，并且 还与之后的序列有关，例如：预测一个语句中缺失的词语那么需要根据上下文进 行预测；Bidirectional RNN是一个相对简单的RNNs，由两个RNNs上下叠加在 一起组成。输出由这两个RNNs的隐藏层的状态决定。</p>
<p><img src="/2019/04/27/神经网络基础-1/nn14.png" alt=""></p>
<h3 id="3-LSTM"><a href="#3-LSTM" class="headerlink" title="3.LSTM"></a>3.LSTM</h3><p>LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，</p>
<p><img src="/2019/04/27/神经网络基础-1/nn15.PNG" alt=""></p>
<h3 id="4-Text-RNN原理"><a href="#4-Text-RNN原理" class="headerlink" title="4.Text-RNN原理"></a>4.Text-RNN原理</h3><p><strong>TextCNN</strong>擅长捕获更短的序列信息，但是TextRNN擅长捕获更长的序列信息。</p>
<h5 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">tf.set_random_seed(1)</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(&apos;MNIST_data&apos;, one_hot=True)</span><br><span class="line"></span><br><span class="line"># hyperparameters</span><br><span class="line">lr = 0.001</span><br><span class="line">training_iters = 100000</span><br><span class="line">batch_size = 128</span><br><span class="line"></span><br><span class="line">n_inputs = 28  # shape 28*28</span><br><span class="line">n_steps = 28  # time steps</span><br><span class="line">n_hidden_unis = 128  # neurons in hidden layer</span><br><span class="line">n_classes = 10  # classes 0-9</span><br><span class="line"></span><br><span class="line"># tf Graph input</span><br><span class="line">x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])</span><br><span class="line">y = tf.placeholder(tf.float32, [None, n_classes])</span><br><span class="line"></span><br><span class="line"># Define weights</span><br><span class="line">weights = &#123;</span><br><span class="line">    # (28,128)</span><br><span class="line">    &apos;in&apos;: tf.Variable(tf.random_normal([n_inputs, n_hidden_unis])),</span><br><span class="line">    # (128,10)</span><br><span class="line">    &apos;out&apos;: tf.Variable(tf.random_normal([n_hidden_unis, n_classes]))</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    # (128,)</span><br><span class="line">    &apos;in&apos;: tf.Variable(tf.constant(0.1, shape=[n_hidden_unis, ])),</span><br><span class="line">    # (10,)</span><br><span class="line">    &apos;out&apos;: tf.Variable(tf.constant(0.1, shape=[n_classes, ]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def RNN(X, weights, biases):</span><br><span class="line"></span><br><span class="line">    # hidden layer for input to cell</span><br><span class="line">    # X(128 batch, 28 steps, 28 inputs) =&gt; (128*28, 28)</span><br><span class="line">    X = tf.reshape(X, [-1, n_inputs])</span><br><span class="line">    # ==&gt;(128 batch * 28 steps, 28 hidden)</span><br><span class="line">    X_in = tf.matmul(X, weights[&apos;in&apos;])+biases[&apos;in&apos;]</span><br><span class="line">    # ==&gt;(128 batch , 28 steps, 28 hidden)</span><br><span class="line">    X_in = tf.reshape(X_in,[-1, n_steps, n_hidden_unis])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # cell</span><br><span class="line">    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_unis, forget_bias=1.0, state_is_tuple=True)</span><br><span class="line">    # lstm cell is divided into two parts(c_state, m_state)</span><br><span class="line">    _init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">    outputs, states = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=_init_state, time_major=False)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # hidden layer for output as the final results</span><br><span class="line">    results = tf.matmul(states[1], weights[&apos;out&apos;]) + biases[&apos;out&apos;]  # states[1]-&gt;m_state states[1]=output[-1]</span><br><span class="line">    # outputs = tf.unstack(tf.transpose(outputs,[1,0,2]))</span><br><span class="line">    # results = tf.matmul(outputs[-1], weights[&apos;out&apos;]) + biases[&apos;out&apos;]</span><br><span class="line">    return results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pred = RNN(x, weights, biases)</span><br><span class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))</span><br><span class="line">train_op = tf.train.AdamOptimizer(lr).minimize(cost)</span><br><span class="line"></span><br><span class="line">correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    step = 0</span><br><span class="line">    while step * batch_size &lt; training_iters:</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">        batch_xs = batch_xs.reshape([batch_size, n_steps, n_inputs])</span><br><span class="line">        sess.run([train_op], feed_dict=&#123;</span><br><span class="line">            x: batch_xs,</span><br><span class="line">            y: batch_ys</span><br><span class="line">        &#125;)</span><br><span class="line">        if step % 20 ==0:</span><br><span class="line">            print (sess.run(accuracy, feed_dict=&#123;</span><br><span class="line">            x: batch_xs,</span><br><span class="line">            y: batch_ys</span><br><span class="line">        &#125;))</span><br><span class="line">        step += 1</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/23/卷积神经网络基础/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/23/卷积神经网络基础/" itemprop="url">卷积神经网络基础</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-23T22:27:11+08:00">
                2019-04-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="卷积神经网络基础"><a href="#卷积神经网络基础" class="headerlink" title="卷积神经网络基础"></a>卷积神经网络基础</h1><h2 id="1-卷积运算"><a href="#1-卷积运算" class="headerlink" title="1.卷积运算"></a>1.卷积运算</h2><h3 id="1-定义"><a href="#1-定义" class="headerlink" title="1.定义"></a>1.定义</h3><p><img src="/2019/04/23/卷积神经网络基础/nn1.PNG" alt=""></p>
<h3 id="2-动机"><a href="#2-动机" class="headerlink" title="2.动机"></a>2.动机</h3><p>卷积运算改进机器学习系统的三个重要思想：</p>
<p><strong>稀疏交互(saprse interactions)</strong><br>又称稀疏权重(sparse weights)，这是因为核大小远小于输入大小。eg：当输入图像包含成千上万像素点，只采用几十个到上百个像素点的和来检测小而有意义的特征。<br>减少了存取需求，提高统计效率。</p>
<p><strong>参数共享(parameter sharing)</strong><br>是指，在一个模型中的多个函数中使用相同参数。卷积运算共享一个卷积核，保证我们只需学习一个参数集，而非在每一个位置学习同一个参数集。</p>
<p><strong>等变表示(equivariant represetations)</strong><br>如果一个函数满足输入改变，而输出也以同样方式改变则该函数是等变的。这在处理时间序列中事件延后，图像序列中对象的移动是非常有利的。<br>而一些其他的变换在卷积中并非必然。比如图像放缩和旋转变换，则需要其他机制处理。</p>
<p><img src="/2019/04/23/卷积神经网络基础/nn2.gif" alt=""></p>
<h3 id="3-一维矩阵运算"><a href="#3-一维矩阵运算" class="headerlink" title="3.一维矩阵运算"></a>3.一维矩阵运算</h3><p>在数学里我们知道f(-x)的图像是f(x)对y轴的反转</p>
<p>​     g(-m)就是把g(m)的序列反转，g(n-m)的意义是把g(-m)平移的n点：</p>
<p><img src="/2019/04/23/卷积神经网络基础/nn2.PNG" alt=""></p>
<p>卷积运算时可以交换顺序.</p>
<ul>
<li><p>代码实现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">x=np.array([1,2,3])</span><br><span class="line">h=np.array([2,3,1])</span><br><span class="line">import scipy.signal</span><br><span class="line">scipy.signal.convolve(x,h)</span><br><span class="line">print(scipy.signal.convolve(x,h))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="4-二维矩阵运算"><a href="#4-二维矩阵运算" class="headerlink" title="4.二维矩阵运算"></a>4.二维矩阵运算</h3><p>其中，矩阵A和B的尺寸分别为ma<em>na即mb</em>nb</p>
<p>① 对矩阵A补零，</p>
<p>第一行之前和最后一行之后都补mb-1行，</p>
<p>第一列之前和最后一列之后都补nb-1列</p>
<p>（注意conv2不支持其他的边界补充选项，函数内部对输入总是补零）；</p>
<p>之所以都是-1是因为卷积核要在图像A上面移动,移动的时候需要满足两者至少有一列或者一行是重叠的.</p>
<p><img src="/2019/04/23/卷积神经网络基础/nn3.PNG" alt=""></p>
<p>再将卷积核旋转180度，之后有三种计算方式，分别是按照shape=full,shape=same,shape=valid进行计算(重叠部分进行矢量积运算)</p>
<p><img src="/2019/04/23/卷积神经网络基础/nn4.png" alt=""></p>
<p><strong>卷积层输入特征和输出特征尺寸和卷积核的关系</strong></p>
<p><img src="/2019/04/23/卷积神经网络基础/nn5.PNG" alt=""></p>
<h1 id="2-反卷积"><a href="#2-反卷积" class="headerlink" title="2.反卷积"></a>2.反卷积</h1><p>我们把4×4的输入特征展成[16,1]的矩阵X ,那么Y = CX则是一个[4,1]的输出特征矩阵，把它重新排列2×2的输出特征就得到最终的结果，从上述分析可以看出卷积层的计算其实是可以转化成矩阵相乘的。值得注意的是，在一些深度学习网络的开源框架中并不是通过这种这个转换方法来计算卷积的，因为这个转换会存在很多无用的0乘操作，<strong>Caffe</strong>中具体实现卷积计算的方法可参考<strong>Implementing convolution as a matrix multiplication</strong></p>
<p>​    通过上述的分析，我们已经知道卷积层的前向操作可以表示为和矩阵C相乘，那么 我们很容易得到卷积层的反向传播就是和C的转置相乘。</p>
<p><img src="/2019/04/23/卷积神经网络基础/nn7.gif" alt=""></p>
<p>这里提到的反卷积跟1维信号处理的反卷积计算是很不一样的，FCN作者称为backwards convolution，有人称Deconvolution layer is a very unfortunate name and should rather be called a transposed convolutional layer. 我们可以知道，在CNN中有con layer与pool layer，con layer进行对图像卷积提取特征，pool layer对图像缩小一半筛选重要特征，对于经典的图像识别CNN网络，如IMAGENET，最后输出结果是1X1X1000，1000是类别种类，1x1得到的是。FCN作者，或者后来对end to end研究的人员，就是对最终1x1的结果使用反卷积（事实上FCN作者最后的输出不是1X1，是图片大小的32分之一，但不影响反卷积的使用）。</p>
<p> 这里图像的反卷积与图6的full卷积原理是一样的，使用了这一种反卷积手段使得图像可以变大，FCN作者使用的方法是这里所说反卷积的一种变体，这样就可以获得相应的像素值，图像可以实现end to end。</p>
<p>这里说另外一种反卷积做法，假设原图是3X3，首先使用上采样让图像变成7X7，可以看到图像多了很多空白的像素点。使用一个3X3的卷积核对图像进行滑动步长为1的valid卷积，得到一个5X5的图像，我们知道的是使用上采样扩大图片，使用反卷积填充图像内容，使得图像内容变得丰富，这也是CNN输出end to end结果的一种方法。韩国作者Hyeonwoo Noh使用VGG16层CNN网络后面加上对称的16层反卷积与上采样网络实现end to end 输出，其不同层上采样与反卷积变化效果如下:</p>
<p><img src="/2019/04/23/卷积神经网络基础/nn6.PNG" alt=""></p>
<h3 id="3-图像卷积的再解释"><a href="#3-图像卷积的再解释" class="headerlink" title="3.图像卷积的再解释"></a>3.图像卷积的再解释</h3><p><strong>基本方法</strong>（2种）：</p>
<p>方法1：full卷积,完整的卷积使得原来的定义变大</p>
<p>方法2：记录pooling index,然后扩大空间</p>
<p>图像的反卷积过程如下：</p>
<p><img src="/2019/04/23/卷积神经网络基础/nn7.PNG" alt=""></p>
<ol>
<li>输入图片每个像素进行一次full卷积，根据full卷积大小计算可以知道每个像素的卷积后大小为 1+4-1=4， 即4x4大小的特征图，输入有4个像素所以4个4x4的特征图</li>
<li>将4个特征图进行步长为3的fusion（即相加）； 例如红色的特征图仍然是在原来输入位置（左上角），绿色还是在原来的位置（右上角），步长为3是指每隔3个像素进行fusion，重叠部分进行相加，即输出的第1行第4列是由红色特阵图的第一行第四列与绿色特征图的第一行第一列相加得到，其他如此类推。</li>
</ol>
<h2 id="2-池化运算"><a href="#2-池化运算" class="headerlink" title="2.池化运算"></a>2.池化运算</h2><p>池化层往往在卷积层后面，通过池化来降低卷积层输出的特征向量，同时改善结果（不易出现过拟合）。图像具有一种“静态性”的属性，这也就意味着在一个图像区域有用的特征极有可能在另一个区域同样适用。因此，为了描述大的图像，一个很自然的想法就是对不同位置的特征进行聚合统计，例如，人们可以计算图像一个区域上的某个特定特征的平均值 (或最大值)来代表这个区域的特征</p>
<h4 id="1-Max-pooling池化操作"><a href="#1-Max-pooling池化操作" class="headerlink" title="1.Max pooling池化操作"></a>1.Max pooling池化操作</h4><p>利用CNN卷积神经网络进行训练时，进行完卷积运算，还需要接着进行Max pooling池化操作，目的是在尽量不丢失图像特征前期下，对图像进行downsampling。</p>
<p>整个图片被不重叠的分割成若干个同样大小的小块（pooling size）。每个小块内只取最大的数字，再舍弃其他节点后，保持原有的平面结构得出 output。</p>
<p><img src="/2019/04/23/卷积神经网络基础/nn8.PNG" alt=""></p>
<p>是利用<strong>平移不变性</strong></p>
<p>如下面，先进行卷积操作，再进行池化操作：</p>
<p><img src="/2019/04/23/卷积神经网络基础/nn9.PNG" alt=""></p>
<h4 id="2-一般池化"><a href="#2-一般池化" class="headerlink" title="2.一般池化"></a>2.一般池化</h4><p>池化窗口和窗口移动的距离大小是相等的</p>
<p><img src="/2019/04/23/卷积神经网络基础/nn10.PNG" alt=""></p>
<h4 id="3-重叠池化"><a href="#3-重叠池化" class="headerlink" title="3.重叠池化"></a>3.重叠池化</h4><p>重叠池化正如其名字所说的，相邻池化窗口之间会有重叠区域</p>
<h4 id="4-空金字塔池化"><a href="#4-空金字塔池化" class="headerlink" title="4.空金字塔池化"></a>4.空金字塔池化</h4><p>先让图像进行卷积操作，然后转化成维度相同的特征输入到全连接层，这个可以把CNN扩展到任意大小的图像。</p>
<p><img src="/2019/04/23/卷积神经网络基础/nn11.png" alt=""></p>
<p>空间金字塔池化的思想来自于Spatial Pyramid Model，它一个pooling变成了多个scale的pooling。用不同大小池化窗口作用于卷积特征，我们可以得到1X1,2X2,4X4的池化结果，由于conv5中共有256个过滤器，所以得到1个256维的特征，4个256个特征，以及16个256维的特征，然后把这21个256维特征链接起来输入全连接层，通过这种方式把不同大小的图像转化成相同维度的特征。</p>
<p><img src="/2019/04/23/卷积神经网络基础/nn12.png" alt=""></p>
<h2 id="3-Text-CNN原理及文本分类模型"><a href="#3-Text-CNN原理及文本分类模型" class="headerlink" title="3.Text-CNN原理及文本分类模型"></a>3.Text-CNN原理及文本分类模型</h2><h3 id="1-原理"><a href="#1-原理" class="headerlink" title="1.原理"></a>1.原理</h3><p>具体过程如下</p>
<p><img src="/2019/04/23/卷积神经网络基础/nn13.PNG" alt=""></p>
<ul>
<li><strong>Embedding</strong>：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点。</li>
<li><strong>Convolution</strong>：然后经过 kernel_sizes=(2,3,4) 的一维卷积层，每个kernel_size 有两个输出 channel。</li>
<li><strong>MaxPolling</strong>：第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示。</li>
<li><p><strong>FullConnection and Softmax</strong>：最后接一层全连接的 softmax 层，输出每个类别的概率。</p>
<p><strong>文本是一维数据，因此在TextCNN卷积用的是一维卷积</strong>（在<strong>word-level</strong>上是一维卷积；虽然文本经过词向量表达后是二维数据，但是在embedding-level上的二维卷积没有意义）。一维卷积带来的问题是需要<strong>通过设计不同 kernel_size 的 filter 获取不同宽度的视野</strong>。</p>
</li>
</ul>
<p><strong>CNN的卷积和池化的过程就是一个抽取特征的过程</strong>（摘自<a href="https://blog.csdn.net/vivian_ll/article/details/80831509" target="_blank" rel="noopener">CNN原理文本分类</a>)</p>
<p>2004年Yoon Kim 在 “Convolutional Neural Networks for Sentence Classification” 一文中提出（虽然第一个用的并不是他，但是在这篇文章中提出了4种Model Variations，并有详细的调参）：</p>
<p>论文使用的模型主要包括五层，第一层是embedding layer,第二层是convolutional layer,第三层是max-pooling layer,第四层是fully connected layer，最后一层是softmax layer. </p>
<ul>
<li><p>为了使其可以进行卷积，首先需要将其转化为二维矩阵表示，通常使用word2vec、glove等word embedding实现。d=5表示每个词转化为5维的向量，矩阵的形状是[sentence_matrix × 5]，即[7 × 5]。</p>
</li>
<li><p>在处理图像数据时，CNN使用的卷积核的宽度和高度的一样的，但是在text-CNN中，卷积核的宽度是与词向量的维度一致。这是因为我们输入的每一行向量代表一个词，在抽取特征的过程中，词做为文本的最小粒度，如果我们使用卷积核的宽度小于词向量的维度就已经不是以词作为最小粒度了。而高度和CNN一样，可以自行设置（通常取值2,3,4,5）。由于我们的输入是一个句子，句子中相邻的词之间关联性很高，因此，当我们用卷积核进行卷积时，不仅考虑了词义而且考虑了词序及其上下文。（类似于skip-gram和CBOW模型的思想）。<br>详细讲解卷积的过程：卷积层输入的是一个表示句子的矩阵，维度为n*d，即每句话共有n个词，每个词有一个d维的词向量表示。假设Xi:i+j表示Xi到Xi+j个词，使用一个宽度为d，高度为h的卷积核W与Xi:i+h-1(h个词)进行卷积操作后再使用激活函数激活得到相应的特征ci，则卷积操作可以表示为：（使用点乘来表示卷积操作） </p>
</li>
</ul>
<p><img src="/2019/04/23/卷积神经网络基础/nn14.PNG" alt=""></p>
<ul>
<li><p>在文本分类中常用的步长是1,更大的补偿会使得向神经网络靠近</p>
</li>
<li><p>窄卷积 vs 宽卷积</p>
<p>在矩阵的中部使用卷积核滤波没有问题，在矩阵的边缘，如对于左侧和顶部没有相邻元素的元素，该如何滤波呢？解决的办法是采用补零法（zero-padding）。所有落在矩阵范围之外的元素值都默认为0。这样就可以对输入矩阵的每一个元素做滤波了，输出一个同样大小或是更大的矩阵。补零法又被称为是宽卷积，不使用补零的方法则被称为窄卷积。</p>
<p><img src="/2019/04/23/卷积神经网络基础/nn15.png" alt=""></p>
</li>
<li><p><strong>池化</strong>:Max-Pooling之后，<strong>还需要将每个值给拼接起来</strong>。得到池化层最终的特征向量。在池化层到全连接层之前可以加上dropout防止过拟合。 </p>
</li>
</ul>
<p>​       文本中pooling方法的优缺点:</p>
<ul>
<li><p>Max Pooling over time</p>
<p>这个操作可以保证特征的位置与旋转不变性，对于图像处理来说这种位置与旋转不变性是很好的特性，但是对于NLP来说，这个特性其实并不一定是好事，因为在很多NLP的应用场合，特征的出现位置信息是很重要的，比如主语出现位置一般在句子头，宾语一般出现在句子尾等等，这些位置信息其实有时候对于分类任务来说还是很重要的，但是Max Pooling 基本把这些信息抛掉了。其次，MaxPooling能减少模型参数数量，有利于减少模型过拟合问题。再者，对于NLP任务来说，Max Pooling可以把变长的输入X整理成固定长度的输入。<br>但是，CNN模型采取MaxPooling Over Time也有一些值得注意的缺点：首先就如上所述，特征的位置信息在这一步骤完全丢失。在卷积层其实是保留了特征的位置信息的，但是通过取唯一的最大值，现在在Pooling层只知道这个最大值是多少，但是其出现位置信息并没有保留；另外一个明显的缺点是：有时候有些强特征会出现多次，比如我们常见的TF.IDF公式，TF就是指某个特征出现的次数，出现次数越多说明这个特征越强，但是因为Max Pooling只保留一个最大值，所以即使某个特征出现多次，现在也只能看到一次，就是说同一特征的强度信息丢失了。这是Max Pooling Over Time典型的两个缺点。</p>
</li>
</ul>
<ul>
<li><p>K-Max pooling</p>
<p>取所有特征值中得分在Top –K的值，并保留这些特征值原始的先后顺序（下图是2-max Pooling的示意图），就是说通过多保留一些特征信息供后续阶段使用</p>
</li>
<li><p>Chunk-Max pooling</p>
</li>
<li><p>Chunk-MaxPooling的思想是：把某个Filter对应的Convolution层的所有特征向量进行分段，切割成若干段后，在每个分段里面各自取得一个最大特征值，比如将某个Filter的特征向量切成3个Chunk，那么就在每个Chunk里面取一个最大值，于是获得3个特征值。（如下图所示，不同颜色代表不同分段） </p>
</li>
</ul>
<p><img src="/2019/04/23/卷积神经网络基础/nn14.jpg" alt=""></p>
<ul>
<li><p>全连接层跟其他模型一样，假设有两层全连接层，第一层可以加上relu作为激活函数，对于多分类来说，第二层则使用softmax激活函数得到属于每个类的概率，并选择categorical_crossentropy 作为激活函数。如果处理的数据集为二分类问题，如情感分析的正负面时，第二层也可以使用sigmoid作为激活函数，然后损失函数使用对数损失函数binary_crossentropy。 </p>
<p>*将不同的词向量表征看成是不同的通道：</p>
<ul>
<li>CNN-rand: 随机初始化每个单词的词向量通过后续的训练去调整。 </li>
<li>CNN-static: 使用预先训练好的词向量，如word2vec训练出来的词向量，在训练过程中不再调整该词向量。 </li>
<li>CNN-non-static: 使用预先训练好的词向量，并在训练过程进一步进行调整。 </li>
<li>CNN-multichannel: 将static与non-static作为两通道的词向量。</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/23/tensorflow-实现神经网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/23/tensorflow-实现神经网络/" itemprop="url">tensorflow 实现神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-23T15:13:56+08:00">
                2019-04-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">#生产数据集</span><br><span class="line">D=2#X的特征个数</span><br><span class="line">K=300</span><br><span class="line">C=2#输出Y的几类</span><br><span class="line">X=(np.random((K,D))*10)-5</span><br><span class="line">Y_c=[int(x0*x0+x1*x1&lt;9) for (x0,x1) in X]</span><br><span class="line">Y=pd.get_dummies(Y_c).values#one-hot</span><br><span class="line"></span><br><span class="line">#构建网络模型</span><br><span class="line">def get_weight(shape,reg):</span><br><span class="line">    w=tf.Variable(tf.random_normal(shape),dtype=tf.float32)</span><br><span class="line">    #如果进行正则化将损失值加入到loss中</span><br><span class="line">    tf.add_to_collection(&apos;losses&apos;,tf.contrib.layers.l2_regularizer(reg)(w))</span><br><span class="line">    return w</span><br><span class="line"></span><br><span class="line">def get_bais(shape):</span><br><span class="line">    return tf.Variable(tf.constant(0.0,shape=shape,dtypeype=tf.float32))</span><br><span class="line">#输入输出占位</span><br><span class="line">x=tf.placeholder(dtype=tf.float32,shape=(None,D))</span><br><span class="line">y=tf.placeholder(dtype=float32,shape=(None,C))#输入数据的量不确定，使用None占位</span><br><span class="line">h=100</span><br><span class="line">#构建网络</span><br><span class="line">w1=get_weight([D,h],1e-3)</span><br><span class="line">b1=get_bais([1,h])</span><br><span class="line">#隐藏层使用激活函数relu</span><br><span class="line">y1=tf.nn.relu(tf.matmul(x,w1)+b1)</span><br><span class="line"></span><br><span class="line">w2=get_weight([h,C],1e-3)</span><br><span class="line">b2=get_bias([1,C])</span><br><span class="line">y_=tf.matmul(y1,w2)+b2</span><br><span class="line"> </span><br><span class="line">#参数设置</span><br><span class="line">STEPS=2000</span><br><span class="line">BATCH_SIZE=256</span><br><span class="line">#学习率</span><br><span class="line">learning_rate=1e-0</span><br><span class="line"></span><br><span class="line">#数据损失</span><br><span class="line">data_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y,logits =y_))#交叉熵</span><br><span class="line">#正则损失</span><br><span class="line">reg_loss=tf.add_n(tf.get_collection(&quot;losses&quot;))</span><br><span class="line">total_loss=reg_loss+data_loss</span><br><span class="line"></span><br><span class="line">#定义训练方式-梯度下降</span><br><span class="line">train_function=tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)</span><br><span class="line"></span><br><span class="line">#开始训练</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op=tf.global_variables_initializervar()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    fpr i in range(STEPS):</span><br><span class="line">        start=(i*BATCH_SIZE)%k</span><br><span class="line">        end=start+BATCH_SIZE</span><br><span class="line">        sess.run(train_function,feed_dict=&#123;x:X[start:end],y:Y[start:end]&#125;)</span><br><span class="line">        if i%100==0:</span><br><span class="line">#             # 计算当前数据损失值</span><br><span class="line">            loss_v = sess.run(data_loss,feed_dict=&#123;x:X,y:Y&#125;)</span><br><span class="line">            print(&quot;this is %dth step,the loss is %f&quot;%(i,loss_v))</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/22/简单神经网络-2/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/22/简单神经网络-2/" itemprop="url">简单神经网络(2)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-22T21:31:36+08:00">
                2019-04-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简单神经网络（2）"><a href="#简单神经网络（2）" class="headerlink" title="简单神经网络（2）"></a>简单神经网络（2）</h1><h3 id="1-词袋模型-bow-离散、高维、稀疏"><a href="#1-词袋模型-bow-离散、高维、稀疏" class="headerlink" title="1.词袋模型(bow : 离散、高维、稀疏)"></a>1.词袋模型(bow : 离散、高维、稀疏)</h3><p>在自然语言处理和文本分析的问题中，词袋（Bag of Words, BOW）和词向量（Word Embedding）是两种最常用的模型。更准确地说，词向量只能表征单个词，如果要表示文本，需要做一些额外的处理。所谓BOW，就是将文本/Query看作是一系列词的集合。</p>
<ul>
<li><strong>离散</strong>：无法衡量词向量之间的关系, 比如酒店、宾馆、旅社 三者都只在某一个固定的位置为 1 ，所以找不到三者的关系，各种度量(与或非、距离)都不合适，即太稀疏，很难捕捉到文本的含义。</li>
<li><p><strong>高维</strong>：词表维度随着语料库增长膨胀，n-gram 序列随语料库膨胀更快。</p>
</li>
<li><p><strong>数据稀疏</strong>： 数据都没有特征多，数据有 100 条，特征有 1000 个</p>
</li>
</ul>
<p>举个例子：<br>                   <strong>文本1：苏宁易购/是/国内/著名/的/B2C/电商/之一</strong><br>这是一个短文本。“/”作为词与词之间的分割。从中我们可以看到这个文本包含“苏宁易购”，“B2C”，“电商”等词。换句话说，该文本的的词袋由“苏宁易购”，“电商”等词构成。就像这样：</p>
<p><img src="/2019/04/22/简单神经网络-2/nn1.PNG" alt=""></p>
<p>但计算机不认识字，只认识数字，那在计算机中怎么表示词袋模型呢？其实很简单，给每个词一个位置/索引就可以了。例如，我们令“苏宁易购”的索引为0，“电商”的索引为1，其他以此类推。则该文本的词袋就变成了：</p>
<p><img src="/2019/04/22/简单神经网络-2/nn2.PNG" alt=""></p>
<p>词袋是在词集的基础上增加了频率的维度，词集只关注有和没有，词袋还要关注有几个。 </p>
<p>假设我们要对一篇文章进行特征化，最常见的方式就是词袋。</p>
<p>假如现在有1000篇新闻文档，把这些文档拆成一个个的字，去重后得到3000个字，然后把这3000个字作为字典，进行文本表示的模型，叫做词袋模型。这种模型的特点是字典中的字没有特定的顺序，句子的总体结构也被舍弃了.</p>
<ul>
<li>2.分布式表示(连续，稠密，低维)</li>
</ul>
<p>分布式（distributed）描述的是把<br>信息分布式地存储在向量的各个维度中，与之相对的是局部表示（local<br>representation），如词的独热表示（one-hot representation），在高维向量中<br>只有一个维度描述了词的语义。一般来说，通过矩阵降维或神经网络降维<br>可以将语义分散存储到向量的各个维度中，因此，这类方法得到的低维向 量一般都可以称作分布式表示。</p>
<h3 id="2-word2vec代码实现"><a href="#2-word2vec代码实现" class="headerlink" title="2.word2vec代码实现"></a>2.word2vec代码实现</h3><p><img src="/2019/04/22/简单神经网络-2/nn3.PNG" alt=""></p>
<h3 id="3-bow2vec-TFIDF模型"><a href="#3-bow2vec-TFIDF模型" class="headerlink" title="3.bow2vec + TFIDF模型"></a>3.bow2vec + TFIDF模型</h3><p>主要内容为：<br>拆分句子为单词颗粒，记号化；<br>生成词典；<br>生成稀疏文档矩阵</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">documents = [&quot;Human machine interface for lab abc computer applications&quot;,</span><br><span class="line">             &quot;A survey of user opinion of computer system response time&quot;,</span><br><span class="line">             &quot;The EPS user interface management system&quot;,</span><br><span class="line">             &quot;System and human system engineering testing of EPS&quot;,              </span><br><span class="line">             &quot;Relation of user perceived response time to error measurement&quot;,</span><br><span class="line">             &quot;The generation of random binary unordered trees&quot;,</span><br><span class="line">             &quot;The intersection graph of paths in trees&quot;,</span><br><span class="line">             &quot;Graph minors IV Widths of trees and well quasi ordering&quot;,</span><br><span class="line">             &quot;Graph minors A survey&quot;]</span><br><span class="line"></span><br><span class="line"># 分词并根据词频剔除</span><br><span class="line"># remove common words and tokenize</span><br><span class="line">stoplist = set(&apos;for a of the and to in&apos;.split())</span><br><span class="line">texts = [[word for word in document.lower().split() if word not in stoplist]</span><br><span class="line">         for document in documents]</span><br><span class="line">         </span><br><span class="line">         #分词并根据词频剔除</span><br><span class="line">#将句子划分为词</span><br><span class="line">stoplist=set(&apos;for a of the end to in&apos;.split())</span><br><span class="line">texts=[[word for word in document.lower().split() if word not in stoplist] for document in documents]</span><br><span class="line">print(texts)</span><br><span class="line"></span><br><span class="line">#生成词典</span><br><span class="line">from gensim import corpora</span><br><span class="line">import os</span><br><span class="line">dictionary=corpora.Dictionary(texts)</span><br><span class="line">dictionary.save(os.path.join(&apos;C:/Users/wuxun/Desktop/&apos;,&apos;deerwester.dict&apos;))#存储词典#路径拼接函数</span><br><span class="line">print(dictionary)</span><br><span class="line">print(dictionary.token2id)</span><br></pre></td></tr></table></figure>
<h5 id="tfidf"><a href="#tfidf" class="headerlink" title="tfidf"></a>tfidf</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">#生成词袋</span><br><span class="line">new_doc=&quot;Human computer interaction Human&quot;</span><br><span class="line">new_vec=dictionary.doc2bow(new_doc.lower().split())</span><br><span class="line">print(new_vec)</span><br><span class="line"># the word &quot;interaction&quot; does not appear in the dictionary and is ignored</span><br><span class="line">    # [(0, 1), (1, 1)] ，词典（dictionary）中第0个词，出现的频数为1（当前句子），</span><br><span class="line">    # 第1个词，出现的频数为1</span><br><span class="line">#多句bow生成</span><br><span class="line">[dictionary.doc2bow(text) for text in texts]</span><br><span class="line">#print(texts)</span><br><span class="line"></span><br><span class="line">from gensim import corpora,models,similarities</span><br><span class="line">corpus = [[(0, 1.0), (1, 1.0), (2, 1.0)],</span><br><span class="line">          [(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (8, 1.0)],</span><br><span class="line">          [(1, 1.0), (3, 1.0), (4, 1.0), (7, 1.0)],</span><br><span class="line">          [(0, 1.0), (4, 2.0), (7, 1.0)],</span><br><span class="line">          [(3, 1.0), (5, 1.0), (6, 1.0)],</span><br><span class="line">          [(9, 1.0)],</span><br><span class="line">          [(9, 1.0), (10, 1.0)],</span><br><span class="line">          [(9, 1.0), (10, 1.0), (11, 1.0)],</span><br><span class="line">          [(8, 1.0), (10, 1.0), (11, 1.0)]]</span><br><span class="line">tfidf=models.TfidfModel(corpus)</span><br><span class="line"></span><br><span class="line">#词袋模型，实践</span><br><span class="line">vec=[(0,1),(4,1),(9,1)]</span><br><span class="line">print(tfidf[vec])</span><br><span class="line">#查找vec中，0,4,9号三个词的TFIDF值。同时进行转化，把之前的文档矩阵中的词频变成了TFIDF值。</span><br><span class="line">#利用tfidf求相似：</span><br><span class="line"></span><br><span class="line">index=similarities.SparseMatrixSimilarity(tfidf[corpus],num_features=12)</span><br><span class="line">vec=[(0,1),(4,1),(9,1)]</span><br><span class="line">sims=index[tfidf[vec]]</span><br><span class="line">#对corpus中的九个文档进行文档级别索引，vec是新文档的词袋,sim就是该vec向量对corpus中九个文档的相似性</span><br><span class="line">print(list(enumerate(sims)))</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/22/简单神经网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/22/简单神经网络/" itemprop="url">简单神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-22T14:29:45+08:00">
                2019-04-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简单神经网络"><a href="#简单神经网络" class="headerlink" title="简单神经网络"></a>简单神经网络</h1><h3 id="1-文本表示"><a href="#1-文本表示" class="headerlink" title="1.文本表示"></a>1.文本表示</h3><h5 id="1-one-hot"><a href="#1-one-hot" class="headerlink" title="(1).one-hot"></a>(1).one-hot</h5><p>one-hot即独立热词，词语被表示成一个维度为词表大小的向量，这个向量中<strong>只有一个维度是1其他位置都是0</strong>.假如词表中只有四个个词“奥巴马”、“特朗普”、“宣誓”、“就职”，那么他们将被表示为：</p>
<p><img src="/2019/04/22/简单神经网络/nn1.PNG" alt=""></p>
<p>one-hot表示的优点是简单易用，但是有着致命的缺点：</p>
<ul>
<li>如果一个词表有十万个单词，那么就需要十万维的向量来表示每一个单词，而每个向量只有一位是1，在储存上造成大量浪费，在深度学习场景中容易受到维度灾难的困扰。</li>
<li>奥巴马和特朗普都是美国总统，而one-hot表示出的两个向量是正交向量，也就是他们之间毫无关系，这显然是丢失了相关语义信息。<br> 因此我们需要更好的表示方法。</li>
</ul>
<p><strong>实例例举</strong>：</p>
<p><img src="/2019/04/22/简单神经网络/nn2.PNG" alt=""></p>
<h5 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h5><p><strong>word2vec</strong>就是将词表征为实数值向量的一种高效的算法模型，其利用深度学习的思想，可以通过训练，把对文本内容的处理简化为 K 维向量空间中的向量运算，而向量空间上的相似度可以用来表示文本语义上的相似。当我们在说word2vec算法或模型的时候，其实指的是其背后用于计算word vector的CBoW模型和Skip-gram模型。很多人以为word2vec指的是一个算法或模型，这也是一种谬误。</p>
<ul>
<li><p><strong>引子</strong>：<strong>Distributed representation词向量表示</strong></p>
<p>低维实数向量。向量的距离可以用最传统的欧氏距离来衡量，也可以用 cos 夹角来衡量。它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。</p>
</li>
<li><p><strong>Word2vec</strong> 使用的词向量不是我们上述提到的One-hot Representation那种词向量，而是 Distributed representation 的词向量表示方式。其基本思想是 <strong>通过训练将每个词映射成 K 维实数向量</strong>（K 一般为模型中的超参数），通过词之间的距离（比如 cosine 相似度、欧氏距离等）来判断它们之间的语义相似度.其采用一个 <strong>三层的神经网络</strong> ，输入层-隐层-输出层。有个核心的技术是 <strong>根据词频用Huffman编码</strong> ，使得所有词频相似的词隐藏层激活的内容基本一致，出现频率越高的词语，他们激活的隐藏层数目越少，这样有效的降低了计算的复杂度。而Word2vec大受欢迎的一个原因正是其高效性。这个三层神经网络本身是 <strong>对语言模型进行建模</strong> ，但也同时 <strong>获得一种单词在向量空间上的表示</strong> ，而这个副作用才是Word2vec的真正目标。</p>
</li>
<li><p>输入是one-hot,，Hidden Layer没有激活函数，也就是线性的单元。Output Layer维度跟Input Layer的维度一样，用的是Softmax回归。当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵。</p>
</li>
</ul>
<p><strong>Word2Vec</strong>实际上是两种不同的方法：Continuous Bag of Words (CBOW) 和 Skip-gram。CBOW的目标是根据上下文来预测当前词语的概率。Skip-gram刚好相反：根据当前词语来预测上下文的概率（如下图所示）。这两种方法都利用人工神经网络作为它们的分类算法。起初，每个单词都是一个随机 N 维向量。经过训练之后，该算法利用 CBOW 或者 Skip-gram 的方法获得了每个单词的最优向量。</p>
<p><img src="/2019/04/22/简单神经网络/nn3.PNG" alt=""></p>
<p>　取一个适当大小的窗口当做语境，输入层读入窗口内的词，将它们的向量（K维，初始随机）加和在一起，形成隐藏层K个节点。输出层是一个巨大的二叉 树，叶节点代表语料里所有的词（语料含有V个独立的词，则二叉树有|V|个叶节点）。而这整颗二叉树构建的算法就是Huffman树。这样，对于叶节点的 每一个词，就会有一个全局唯一的编码，形如”010011”，不妨记左子树为1，右子树为0。接下来，隐层的每一个节点都会跟二叉树的内节点有连边，于是 对于二叉树的每一个内节点都会有K条连边，每条边上也会有权值。</p>
<p><img src="/2019/04/22/简单神经网络/nn4.PNG" alt=""></p>
<p>对于语料库中的某个词w_t，对应着二叉树的某个叶子节点，因此它必然有一个二进制编码，如”010011”。在训练阶段，当给定上下文，要预测后 面的词w_t的时候，我们就从二叉树的根节点开始遍历，这里的目标就是预测这个词的二进制编号的每一位。即对于给定的上下文，我们的目标是使得预测词的二 进制编码概率最大。形象地说，我们希望在根节点，词向量和与根节点相连经过 logistic 计算得到 bit=1 的概率尽量接近 0，在第二层，希望其 bit=1 的概率尽量接近1，这么一直下去，我们把一路上计算得到的概率相乘，即得到目标词w_t在当前网络下的概率P(w_t)，那么对于当前这个 sample的残差就是1-P(w_t)，于是就可以使用梯度下降法训练这个网络得到所有的参数值了。显而易见，按照目标词的二进制编码计算到最后的概率 值就是归一化的。</p>
<p>　　最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大,word2vec使用霍夫曼树来代替传统的DNN算法（因为DNN算法处理过程非常耗时）。 而内部节点则起到隐藏层神经元的作用。其实借助了分类问题中，使用一连串二分类近似多分类的思想。例如我们是把所有的词都作为输出，那么“桔 子”、“汽车”都是混在一起。给定w_t的上下文，先让模型判断w_t是不是名词，再判断是不是食物名，再判断是不是水果，再判断是不是“桔子”。</p>
<p>　　但是在训练过程中，模型会赋予这些抽象的中间结点一个合适的向量，这个向量代表了它对应的所有子结点。因为真正的单词公用了这些抽象结点的向量，所 以Hierarchical Softmax方法和原始问题并不是等价的，但是这种近似并不会显著带来性能上的损失同时又使得模型的求解规模显著上升。</p>
<p>　　没有使用这种二叉树，而是直接从隐层直接计算每一个输出的概率——即传统的Softmax，就需要对|V|中的每一个词都算一遍，这个过程时间复杂 度是O(|V|)的。而使用了二叉树（如Word2vec中的Huffman树），其时间复杂度就降到了O(log2(|V|))，速度大大地加快了。</p>
<p>　　现在这些词向量已经捕捉到上下文的信息。我们可以利用基本代数公式来发现单词之间的关系（比如，“国王”-“男人”+“女人”=“王后”）。这些词向量可 以代替词袋用来预测未知数据的情感状况。该模型的优点在于不仅考虑了语境信息还压缩了数据规模（通常情况下，词汇量规模大约在300个单词左右而不是之前 模型的100000个单词）。因为神经网络可以替我们提取出这些特征的信息，所以我们仅需要做很少的手动工作。但是由于文本的长度各异，我们可能需要利用 所有词向量的平均值作为分类算法的输入值，从而对整个文本文档进行分类处理。</p>
<p><strong>注：以下内容转载自<a href="https://www.cnblogs.com/pinard/p/7243513.html" target="_blank" rel="noopener">网络</a></strong></p>
<h3 id="基于Hierarchical-Softmax的模型概述"><a href="#基于Hierarchical-Softmax的模型概述" class="headerlink" title=". 基于Hierarchical Softmax的模型概述"></a>. 基于Hierarchical Softmax的模型概述</h3><p>　　　　我们先回顾下传统的神经网络词向量语言模型，里面一般有三层，输入层（词向量），隐藏层和输出层（softmax层）。里面最大的问题在于从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值。这个模型如下图所示。其中<em>V</em>V  是词汇表的大小，</p>
<p><img src="/2019/04/22/简单神经网络/nn5.PNG" alt=""></p>
<p>　　　　word2vec对这个模型做了改进，首先，对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。比如输入的是三个4维词向量：(1,2,3,4),(9,6,11,8),(5,10,7,12)(1,2,3,4),(9,6,11,8),(5,10,7,12)  ,那么我们word2vec映射后的词向量就是(5,6,7,8)(5,6,7,8)  。由于这里是从多个词向量变成了一个词向量。</p>
<p>　　　　第二个改进就是从隐藏层到输出的softmax层这里的计算量个改进。为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。我们在上一节已经介绍了霍夫曼树的原理。如何映射呢？这里就是理解word2vec的关键所在了。</p>
<p>　　　　由于我们把之前所有都要计算的从输出softmax层的概率计算变成了一颗二叉霍夫曼树，那么我们的softmax概率计算只需要沿着树形结构进行就可以了。如下图所示，我们可以沿着霍夫曼树从根节点一直走到我们的叶子节点的词<em>w</em>2w2  。</p>
<p><img src="/2019/04/22/简单神经网络/nn6.PNG" alt=""></p>
<p>　　　　和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种softmax取名为”Hierarchical Softmax”。</p>
<p>　　　　如何“沿着霍夫曼树一步步完成”呢？在word2vec中，我们采用了二元逻辑回归的方法，即规定沿着左子树走，那么就是负类(霍夫曼树编码1)，沿着右子树走，那么就是正类(霍夫曼树编码0)。判别正类和负类的方法是使用sigmoid函数，即：</p>
<p><em>P</em>(+)=<em>σ</em>(<em>x<strong>T</strong>w**θ</em>)=11+<em>e</em>−<em>x<strong>T</strong>w**θ</em>P(+)=σ(xwTθ)=11+e−xwTθ</p>
<p>　　　　其中<em>x**w</em>xw  是当前内部节点的词向量，而<em>θ</em>θ  则是我们需要从训练样本求出的逻辑回归的模型参数。</p>
<p>　　　　使用霍夫曼树有什么好处呢？首先，由于是二叉树，之前计算量为<em>V</em>V  ,现在变成了<em>l<strong>o</strong>g</em>2<em>V</em>log2V  。第二，由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到，这符合我们的贪心优化思想。</p>
<p>　　　　容易理解，被划分为左子树而成为负类的概率为<em>P</em>(−)=1−<em>P</em>(+)P(−)=1−P(+)  。在某一个内部节点，要判断是沿左子树还是右子树走的标准就是看<em>P</em>(−),<em>P</em>(+)P(−),P(+)  谁的概率值大。而控制<em>P</em>(−),<em>P</em>(+)P(−),P(+)  谁的概率值大的因素一个是当前节点的词向量，另一个是当前节点的模型参数<em>θ</em>θ  。</p>
<p>　　　　对于上图中的<em>w</em>2w2  ，如果它是一个训练样本的输出，那么我们期望对于里面的隐藏节点<em>n</em>(<em>w</em>2,1)n(w2,1)  的<em>P</em>(−)P(−)  概率大，<em>n</em>(<em>w</em>2,2)n(w2,2)  的<em>P</em>(−)P(−)  概率大，<em>n</em>(<em>w</em>2,3)n(w2,3)  的<em>P</em>(+)P(+)  概率大。</p>
<p>　　　　回到基于Hierarchical Softmax的word2vec本身，我们的目标就是找到合适的所有节点的词向量和所有内部节点<em>θ</em>θ  , 使训练样本达到最大似然。那么如何达到最大似然呢？</p>
<h3 id="2-基于Hierarchical-Softmax的模型梯度计算"><a href="#2-基于Hierarchical-Softmax的模型梯度计算" class="headerlink" title="2. 基于Hierarchical Softmax的模型梯度计算"></a>2. 基于Hierarchical Softmax的模型梯度计算</h3><p>　　　我们使用最大似然法来寻找所有节点的词向量和所有内部节点<em>θ</em>θ  。先拿上面的<em>w</em>2w2  例子来看，我们期望最大化下面的似然函数：</p>
<p>∏<em>i</em>=13<em>P</em>(<em>n</em>(<em>w**i</em>),<em>i</em>)=(1−11+<em>e</em>−<em>x<strong>T</strong>w**θ</em>1)(1−11+<em>e</em>−<em>x<strong>T</strong>w**θ</em>2)11+<em>e</em>−<em>x<strong>T</strong>w**θ</em>3∏i=13P(n(wi),i)=(1−11+e−xwTθ1)(1−11+e−xwTθ2)11+e−xwTθ3</p>
<p>　　　　对于所有的训练样本，我们期望最大化所有样本的似然函数乘积。</p>
<p>　　　　为了便于我们后面一般化的描述，我们定义输入的词为<em>w</em>w  ,其从输入层词向量求和平均后的霍夫曼树根节点词向量为<em>x**w</em>xw  , 从根节点到<em>w</em>w  所在的叶子节点，包含的节点总数为<em>l**w</em>lw  , <em>w</em>w  在霍夫曼树中从根节点开始，经过的第<em>i</em>i  个节点表示为<em>p<strong>w</strong>i</em>piw  ,对应的霍夫曼编码为<em>d<strong>w</strong>i</em>∈{0,1}diw∈{0,1}  ,其中<em>i</em>=2,3,…<em>l**w</em>i=2,3,…lw  。而该节点对应的模型参数表示为<em>θ<strong>w</strong>i</em>θiw  , 其中<em>i</em>=1,2,…<em>l**w</em>−1i=1,2,…lw−1  ，没有<em>i</em>=<em>l**w</em>i=lw  是因为模型参数仅仅针对于霍夫曼树的内部节点。</p>
<p>　　　　定义<em>w</em>w  经过的霍夫曼树某一个节点j的逻辑回归概率为<em>P</em>(<em>d<strong>w</strong>j</em>|<em>x**w</em>,<em>θ<strong>w</strong>j</em>−1)P(djw|xw,θj−1w)  ，其表达式为：</p>
<p><em>P</em>(<em>d<strong>w</strong>j</em>|<em>x**w</em>,<em>θ<strong>w</strong>j</em>−1)={<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)1−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)<em>d<strong>w</strong>j</em>=0<em>d<strong>w</strong>j</em>=1P(djw|xw,θj−1w)={σ(xwTθj−1w)djw=01−σ(xwTθj−1w)djw=1</p>
<p>　　　　那么对于某一个目标输出词<em>w</em>w  ,其最大似然为：</p>
<p>∏<em>j</em>=2<em>l<strong>w</strong>P</em>(<em>d<strong>w</strong>j</em>|<em>x**w</em>,<em>θ<strong>w</strong>j</em>−1)=∏<em>j</em>=2<em>l**w</em>[<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)]1−<em>d<strong>w</strong>j</em>[1−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)]<em>d<strong>w</strong>j</em>∏j=2lwP(djw|xw,θj−1w)=∏j=2lw[σ(xwTθj−1w)]1−djw[1−σ(xwTθj−1w)]djw</p>
<p>　　　　在word2vec中，由于使用的是随机梯度上升法，所以并没有把所有样本的似然乘起来得到真正的训练集最大似然，仅仅每次只用一个样本更新梯度，这样做的目的是减少梯度计算量。这样我们可以得到<em>w</em>w  的对数似然函数<em>L</em>L  如下：</p>
<p><em>L</em>=<em>l<strong>o</strong>g</em>∏<em>j</em>=2<em>l<strong>w</strong>P</em>(<em>d<strong>w</strong>j</em>|<em>x**w</em>,<em>θ<strong>w</strong>j</em>−1)=∑<em>j</em>=2<em>l**w</em>((1−<em>d<strong>w</strong>j</em>)<em>l<strong>o</strong>g</em>[<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)]+<em>d<strong>w</strong>j<strong>l</strong>o**g</em>[1−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)])L=log∏j=2lwP(djw|xw,θj−1w)=∑j=2lw((1−djw)log[σ(xwTθj−1w)]+djwlog[1−σ(xwTθj−1w)])</p>
<p>　　　　要得到模型中<em>w</em>w  词向量和内部节点的模型参数<em>θ</em>θ  , 我们使用梯度上升法即可。首先我们求模型参数<em>θ<strong>w</strong>j</em>−1θj−1w  的梯度：</p>
<p>∂<em>L</em>∂<em>θ<strong>w</strong>j</em>−1=(1−<em>d<strong>w</strong>j</em>)(<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)(1−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)<em>x**w</em>−<em>d<strong>w</strong>j</em>(<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)(1−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)1−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)<em>x**w</em>=(1−<em>d<strong>w</strong>j</em>)(1−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1))<em>x**w</em>−<em>d<strong>w</strong>j**σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)<em>x**w</em>=(1−<em>d<strong>w</strong>j</em>−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1))<em>x**w</em>(1)(2)(3)(1)∂L∂θj−1w=(1−djw)(σ(xwTθj−1w)(1−σ(xwTθj−1w)σ(xwTθj−1w)xw−djw(σ(xwTθj−1w)(1−σ(xwTθj−1w)1−σ(xwTθj−1w)xw(2)=(1−djw)(1−σ(xwTθj−1w))xw−djwσ(xwTθj−1w)xw(3)=(1−djw−σ(xwTθj−1w))xw</p>
<p>　　　　如果大家看过之前写的<a href="http://www.cnblogs.com/pinard/p/6029432.html" target="_blank" rel="noopener">逻辑回归原理小结</a>，会发现这里的梯度推导过程基本类似。</p>
<p>　　　　同样的方法，可以求出<em>x**w</em>xw  的梯度表达式如下：</p>
<p>∂<em>L</em>∂<em>x**w</em>=∑<em>j</em>=2<em>l**w</em>(1−<em>d<strong>w</strong>j</em>−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1))<em>θ<strong>w</strong>j</em>−1∂L∂xw=∑j=2lw(1−djw−σ(xwTθj−1w))θj−1w</p>
<p>　　　　有了梯度表达式，我们就可以用梯度上升法进行迭代来一步步的求解我们需要的所有的<em>θ<strong>w</strong>j</em>−1θj−1w  和<em>x**w</em>xw  。</p>
<h3 id="3-基于Hierarchical-Softmax的CBOW模型"><a href="#3-基于Hierarchical-Softmax的CBOW模型" class="headerlink" title="3. 基于Hierarchical Softmax的CBOW模型"></a>3. 基于Hierarchical Softmax的CBOW模型</h3><p>　　　　由于word2vec有两种模型：CBOW和Skip-Gram,我们先看看基于CBOW模型时， Hierarchical Softmax如何使用。</p>
<p>　　　　首先我们要定义词向量的维度大小<em>M</em>M  ，以及CBOW的上下文大小2<em>c</em>2c  ,这样我们对于训练样本中的每一个词，其前面的<em>c</em>c  个词和后面的<em>c</em>c  个词作为了CBOW模型的输入,该词本身作为样本的输出，期望softmax概率最大。</p>
<p>　　　　在做CBOW模型前，我们需要先将词汇表建立成一颗霍夫曼树。</p>
<p>　　　　对于从输入层到隐藏层（投影层），这一步比较简单，就是对<em>w</em>w  周围的2<em>c</em>2c  个词向量求和取平均即可，即：</p>
<p><em>x**w</em>=12<em>c</em>∑<em>i</em>=12<em>c<strong>x</strong>i</em>xw=12c∑i=12cxi</p>
<p>　　　　第二步，通过梯度上升法来更新我们的<em>θ<strong>w</strong>j</em>−1θj−1w  和<em>x**w</em>xw  ，注意这里的<em>x**w</em>xw  是由2<em>c</em>2c  个词向量相加而成，我们做梯度更新完毕后会用梯度项直接更新原始的各个<em>x**i</em>(<em>i</em>=1,2,,,,2<em>c</em>)xi(i=1,2,,,,2c)  ，即：</p>
<p><em>θ<strong>w</strong>j</em>−1=<em>θ<strong>w</strong>j</em>−1+<em>η</em>(1−<em>d<strong>w</strong>j</em>−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1))<em>x**w</em>θj−1w=θj−1w+η(1−djw−σ(xwTθj−1w))xw</p>
<p><em>x**i</em>=<em>x**i</em>+<em>η</em>∑<em>j</em>=2<em>l**w</em>(1−<em>d<strong>w</strong>j</em>−<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1))<em>θ<strong>w</strong>j</em>−1(<em>i</em>=1,2..,2<em>c</em>)xi=xi+η∑j=2lw(1−djw−σ(xwTθj−1w))θj−1w(i=1,2..,2c)</p>
<p>　　　　其中<em>η</em>η  为梯度上升法的步长。</p>
<p>　　　　这里总结下基于Hierarchical Softmax的CBOW模型算法流程，梯度迭代使用了随机梯度上升法：</p>
<p>　　　　输入：基于CBOW的语料训练样本，词向量的维度大小<em>M</em>M  ，CBOW的上下文大小2<em>c</em>2c  ,步长<em>η</em>η  </p>
<p>　　　　输出：霍夫曼树的内部节点模型参数<em>θ</em>θ  ，所有的词向量<em>w</em>w  </p>
<p>　　　　1. 基于语料训练样本建立霍夫曼树。</p>
<p>　　　　2. 随机初始化所有的模型参数<em>θ</em>θ  ，所有的词向量<em>w</em>w  </p>
<p>　　　　3. 进行梯度上升迭代过程，对于训练集中的每一个样本(<em>c<strong>o</strong>n<strong>t</strong>e<strong>x</strong>t</em>(<em>w</em>),<em>w</em>)(context(w),w)  做如下处理：</p>
<p>　　　　　　a)  e=0， 计算<em>x**w</em>=12<em>c</em>∑<em>i</em>=12<em>c<strong>x</strong>i</em>xw=12c∑i=12cxi  </p>
<p>　　　　　　b)  for j = 2 to <em>l**w</em>lw  , 计算：</p>
<p><em>f</em>=<em>σ</em>(<em>x<strong>T</strong>w<strong>θ</strong>w**j</em>−1)f=σ(xwTθj−1w)</p>
<p><em>g</em>=(1−<em>d<strong>w</strong>j</em>−<em>f</em>)<em>η</em>g=(1−djw−f)η</p>
<p><em>e</em>=<em>e</em>+<em>g<strong>θ</strong>w**j</em>−1e=e+gθj−1w</p>
<p><em>θ<strong>w</strong>j</em>−1=<em>θ<strong>w</strong>j</em>−1+<em>g<strong>x</strong>w</em>θj−1w=θj−1w+gxw</p>
<p>　　　           c) 对于<em>c<strong>o</strong>n<strong>t</strong>e<strong>x</strong>t</em>(<em>w</em>)context(w)  中的每一个词向量<em>x**i</em>xi  (共2c个)进行更新：</p>
<p><em>x**i</em>=<em>x**i</em>+<em>e</em>xi=xi+e</p>
<p>　　　　　　d) 如果梯度收敛，则结束梯度迭代，否则回到步骤3继续迭代。</p>
<h3 id="4-基于Hierarchical-Softmax的Skip-Gram模型"><a href="#4-基于Hierarchical-Softmax的Skip-Gram模型" class="headerlink" title="4. 基于Hierarchical Softmax的Skip-Gram模型"></a>4. 基于Hierarchical Softmax的Skip-Gram模型</h3><p>　　　　现在我们先看看基于Skip-Gram模型时， Hierarchical Softmax如何使用。此时输入的只有一个词<em>w</em>w  ,输出的为2<em>c</em>2c  个词向量<em>c<strong>o</strong>n<strong>t</strong>e<strong>x</strong>t</em>(<em>w</em>)context(w)  。</p>
<p>　　　　我们对于训练样本中的每一个词，该词本身作为样本的输入， 其前面的<em>c</em>c  个词和后面的<em>c</em>c  个词作为了Skip-Gram模型的输出,，期望这些词的softmax概率比其他的词大。</p>
<p>　　　　Skip-Gram模型和CBOW模型其实是反过来的，在上一篇已经讲过。</p>
<p>　　　　在做CBOW模型前，我们需要先将词汇表建立成一颗霍夫曼树。</p>
<p>　　　　对于从输入层到隐藏层（投影层），这一步比CBOW简单，由于只有一个词，所以，即<em>x**w</em>xw  就是词<em>w</em>w  对应的词向量。</p>
<p>　　　　第二步，通过梯度上升法来更新我们的<em>θ<strong>w</strong>j</em>−1θj−1w  和<em>x**w</em>xw  ，注意这里的<em>x**w</em>xw  周围有2<em>c</em>2c  个词向量，此时如果我们期望<em>P</em>(<em>x**i</em>|<em>x**w</em>),<em>i</em>=1,2…2<em>c</em>P(xi|xw),i=1,2…2c  最大。此时我们注意到由于上下文是相互的，在期望<em>P</em>(<em>x**i</em>|<em>x**w</em>),<em>i</em>=1,2…2<em>c</em>P(xi|xw),i=1,2…2c  最大化的同时，反过来我们也期望<em>P</em>(<em>x**w</em>|<em>x**i</em>),<em>i</em>=1,2…2<em>c</em>P(xw|xi),i=1,2…2c  最大。那么是使用<em>P</em>(<em>x**i</em>|<em>x**w</em>)P(xi|xw)  好还是<em>P</em>(<em>x**w</em>|<em>x**i</em>)P(xw|xi)  好呢，word2vec使用了后者，这样做的好处就是在一个迭代窗口内，我们不是只更新<em>x**w</em>xw  一个词，而是<em>x**i</em>,<em>i</em>=1,2…2<em>c</em>xi,i=1,2…2c  共2<em>c</em>2c  个词。这样整体的迭代会更加的均衡。因为这个原因，Skip-Gram模型并没有和CBOW模型一样对输入进行迭代更新，而是对2<em>c</em>2c  个输出进行迭代更新。</p>
<p>　　　　这里总结下基于Hierarchical Softmax的Skip-Gram模型算法流程，梯度迭代使用了随机梯度上升法：</p>
<p>　　　　输入：基于Skip-Gram的语料训练样本，词向量的维度大小<em>M</em>M  ，Skip-Gram的上下文大小2<em>c</em>2c  ,步长<em>η</em>η  </p>
<p>　　　　输出：霍夫曼树的内部节点模型参数<em>θ</em>θ  ，所有的词向量<em>w</em>w  </p>
<p>　　　　1. 基于语料训练样本建立霍夫曼树。</p>
<p>　　　　2. 随机初始化所有的模型参数<em>θ</em>θ  ，所有的词向量<em>w</em>w  ,</p>
<p>　　　　3. 进行梯度上升迭代过程，对于训练集中的每一个样本(<em>w</em>,<em>c<strong>o</strong>n<strong>t</strong>e<strong>x</strong>t</em>(<em>w</em>))(w,context(w))  做如下处理：</p>
<p>　　　　　　a)  for i =1 to 2c:</p>
<p>　　　　　　　　i) e=0</p>
<p>　　　　　　　　ii)for j = 2 to <em>l**w</em>lw  , 计算：</p>
<p><em>f</em>=<em>σ</em>(<em>x<strong>T</strong>i<strong>θ</strong>w**j</em>−1)f=σ(xiTθj−1w)</p>
<p><em>g</em>=(1−<em>d<strong>w</strong>j</em>−<em>f</em>)<em>η</em>g=(1−djw−f)η</p>
<p><em>e</em>=<em>e</em>+<em>g<strong>θ</strong>w**j</em>−1e=e+gθj−1w</p>
<p><em>θ<strong>w</strong>j</em>−1=<em>θ<strong>w</strong>j</em>−1+<em>g<strong>x</strong>i</em>θj−1w=θj−1w+gxi   iii) </p>
<p><em>x**i</em>=<em>x**i</em>+<em>e</em>xi=xi+e</p>
<p>　　　　　　b)如果梯度收敛，则结束梯度迭代，算法结束，否则回到步骤a继续迭代。</p>
<h3 id="5-Hierarchical-Softmax的模型源码和算法的对应"><a href="#5-Hierarchical-Softmax的模型源码和算法的对应" class="headerlink" title="5. Hierarchical Softmax的模型源码和算法的对应　　　　"></a>5. Hierarchical Softmax的模型源码和算法的对应　　　　</h3><p>　　　　这里给出上面算法和<a href="https://github.com/tmikolov/word2vec/blob/master/word2vec.c" target="_blank" rel="noopener">word2vec源码</a>中的变量对应关系。</p>
<p>　　　　在源代码中，基于Hierarchical Softmax的CBOW模型算法在435-463行，基于Hierarchical Softmax的Skip-Gram的模型算法在495-519行。大家可以对着源代码再深入研究下算法。</p>
<p>　　　　在源代码中，neule对应我们上面的<em>e</em>e  , syn0对应我们的<em>x**w</em>xw  , syn1对应我们的<em>θ<strong>i</strong>j</em>−1θj−1i  , layer1_size对应词向量的维度，window对应我们的<em>c</em>c  。</p>
<p>　　　　另外，vocab[word].code[d]指的是，当前单词word的，第d个编码，编码不含Root结点。vocab[word].point[d]指的是，当前单词word，第d个编码下，前置的结点。</p>
<p>　　</p>
<p>　</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/21/神经网络基础/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/21/神经网络基础/" itemprop="url">神经网络基础</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-21T11:20:09+08:00">
                2019-04-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h1><h3 id="1-神经网络模型"><a href="#1-神经网络模型" class="headerlink" title="1.神经网络模型"></a>1.神经网络模型</h3><p>所谓神经网络就是将许多个单一“神经元”联结在一起，这样，一个“神经元”的输出就可以是另一个“神经元”的输入。例如，下图就是一个简单的神经网络： </p>
<p><img src="/2019/04/21/神经网络基础/nn1.PNG" alt=""></p>
<p>我们使用圆圈来表示神经网络的输入，标上“+1”的圆圈被称为”’偏置节点”’，也就是截距项。神经网络最左边的一层叫做 <strong>”‘输入层”’</strong>，最右的一层叫做<strong>”‘输出层’”</strong>（本例中，输出层只有一个节点）。中间所有节点组成的一层叫做<strong>”’隐藏层”’</strong>，因为我们不能在训练样本集中观测到它们的值。同时可以看到，以上神经网络的例子中有3个”’输入单元”’（偏置单元不计在内），3个”’隐藏单元”’及一个”’输出单元”’。</p>
<p>我们用nl 来表示网络的层数，本例中nl=3，我们将第l 层记为Ll ，于是L1是输入层，输出层Lni 。本例神经网络有参数(W,b)=(W(1),b(1),W(2),b(2)) ，其中W(l)ij（下面的式子中用到）是第l层第j 单元与第l+1 层第i 单元之间的联接参数（其实就是连接线上的权重，注意标号顺序），bi(l) 是第l+1 层第i 单元的偏置项。因此在本例中，W(1)∈ℜ3×3 ，W(2)∈ℜ1×3 。注意，没有其他单元连向偏置单元(即偏置单元没有输入)，因为它们总是输出+1。同时，我们用sl表示第ll 层的节点数（偏置单元不计在内）。</p>
<p>本例神经网络的计算步骤如下:</p>
<p><img src="/2019/04/21/神经网络基础/nn2.PNG" alt=""></p>
<p>我们用<em>z</em>(<em>l</em>)<em>i</em> 表示第<em>l</em> 层第<em>i</em>单元输入加权和（包括偏置单元）:</p>
<p><img src="/2019/04/21/神经网络基础/nn3.PNG" alt=""></p>
<p>再将激活函数进行广播,及可以得到前向传播过程：</p>
<p><img src="/2019/04/21/神经网络基础/nn4.PNG" alt=""></p>
<p>当然神经网络也可以具有多个隐藏层，其计算原理同之前类似。</p>
<p><img src="/2019/04/21/神经网络基础/nn5.PNG" alt=""></p>
<p>如果你想预测的输出是多个的，那这种神经网络很适用。（比如，在医疗诊断应用中，患者的体征指标就可以作为向量的输入值，而不同的输出值<em>y**i</em>yi 可以表示不同的疾病存在与否。）</p>
<h3 id="2-前馈神经网络-FF"><a href="#2-前馈神经网络-FF" class="headerlink" title="2.前馈神经网络(FF)"></a>2.前馈神经网络(FF)</h3><p><img src="/2019/04/21/神经网络基础/nn6.PNG" alt=""></p>
<p><strong>前馈神经网络（FF）</strong>，这是一个很古老的方法——这种方法起源于50年代。它的工作原理通常遵循以下规则：</p>
<p>1.所有节点都完全连接</p>
<p>2.激活从输入层流向输出，无回环</p>
<p>3.输入和输出之间有一层（隐含层）</p>
<p>在大多数情况下，这种类型的网络使用<strong>反向传播方法</strong>进行训练。</p>
<h3 id="3-感知机"><a href="#3-感知机" class="headerlink" title="3.感知机"></a>3.感知机</h3><h5 id="1-算法原理"><a href="#1-算法原理" class="headerlink" title="1.算法原理"></a>1.算法原理</h5><p>想法来自生物学的神经元的一些工作方式，多个生物信号 (input singals) 到达树突 (dentrites)并进入细胞核 (cell nucleus)，如果这些信号的效果累加达到一个阈值，那么通过轴突 (axon) 产生一个输出信号 (output signals)。在有监督学习与分类的背景下，这样的算法可被用来预测一个样本是否属于某个类别。</p>
<p><img src="/2019/04/21/神经网络基础/nn7.PNG" alt=""></p>
<p>正式地，我们可以把这个问题表述为一个二分类任务，并且为了简单起见，将这两个类别分别定义为 1 (正类) 与 -1（负类）。接着定义一个激活函数 (activation function) ,, 它输入的是 xx 与其对应的权重向量 ww 的一个线性组合.</p>
<p>对于一个指定样本 x(i)x(i), 如果 ϕ(z)ϕ(z) 的输出值大于预先定义的一个阈值 Θ, 那么就预测其类别 1. 否则，预测为类别 -1. 在感知器算法中，激活函数 是一个简单的单位阶跃函数 (unit step function)</p>
<p>需要注意一点的是只有当两个类别是<strong>线性可分</strong>时，感知器算法才能保证收敛。如果两个类别不是线性可分，那么我们可以在训练集上设置一个最大的迭代次数, 或是设置一个可接受的错误分类的阈值。</p>
<h5 id="2-感知机权重的学习过程"><a href="#2-感知机权重的学习过程" class="headerlink" title="2.感知机权重的学习过程"></a>2.感知机权重的学习过程</h5><p>采用随机梯度下降法：<br><img src="/2019/04/21/神经网络基础/nn8.PNG" alt=""></p>
<h3 id="推荐一个神经网络可视化的网站-tensorflow-游乐场"><a href="#推荐一个神经网络可视化的网站-tensorflow-游乐场" class="headerlink" title="推荐一个神经网络可视化的网站 :tensorflow 游乐场"></a>推荐一个神经网络可视化的网站 :<a href="https://www.jianshu.com/p/191d1e21f7ed" target="_blank" rel="noopener">tensorflow 游乐场</a></h3><h3 id="3-使用tensoeflow定义几层简单的神经网络"><a href="#3-使用tensoeflow定义几层简单的神经网络" class="headerlink" title="3.使用tensoeflow定义几层简单的神经网络"></a>3.使用tensoeflow定义几层简单的神经网络</h3><p>激活函数使用sigmoid,递归使用链式法则来实现反向传播:</p>
<ol>
<li><p>再次运用的时简单类定义进行神经网络的计算。</p>
</li>
<li><p>tensorflow版本待更</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import math</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">def rand(a,b):</span><br><span class="line">    return (b-a)*random.random()+a</span><br><span class="line"></span><br><span class="line">def make_matrix(m,n,fill=0.0):</span><br><span class="line">    mat=[]</span><br><span class="line">    for i in range(m):</span><br><span class="line">        mat.append([fill]*n)</span><br><span class="line">    return mat</span><br><span class="line"></span><br><span class="line">def sigmoid(x):</span><br><span class="line">    return 1.0/(1.0+math.exp(-x));</span><br><span class="line"></span><br><span class="line">def sigmod_derivate(x):</span><br><span class="line">    return x*(1-x)</span><br><span class="line"></span><br><span class="line">class BPNeuralNetWork:</span><br><span class="line">    </span><br><span class="line">    def _init_(self):</span><br><span class="line">        self.input_n=0</span><br><span class="line">        self.hidden_n=0</span><br><span class="line">        self.output_n=0</span><br><span class="line">        self.input_cells=[]</span><br><span class="line">        self.hidden_cells=[]</span><br><span class="line">        self.output_cells=[]</span><br><span class="line">        self.input_weights=[]</span><br><span class="line">        self.output_weights=[]</span><br><span class="line">        </span><br><span class="line">    def setup(self,ni,nh,no):</span><br><span class="line">        self.input_n=ni+1</span><br><span class="line">        self.hidden_n=nh</span><br><span class="line">        self.output_n=no</span><br><span class="line">        </span><br><span class="line">        self.input_cells=[1.0]*self.input_n</span><br><span class="line">        self.hidden_cells=[1.0]*self.hidden_n</span><br><span class="line">        self.output_cells=[1.0]*self.output_n</span><br><span class="line">        </span><br><span class="line">        self.input_weights=make_matrix(self.input_n,self.hidden_n)</span><br><span class="line">        self.output_weights=make_matrix(self.hidden_n,self.output_n)</span><br><span class="line">        for i in range(self.input_n):</span><br><span class="line">            for h in range(self.hidden_n):</span><br><span class="line">                self.input_weights[i][h]=rand(-0.2,0.2)</span><br><span class="line">        for h in range(self.hidden_n):</span><br><span class="line">            for j in range(self.output_n):</span><br><span class="line">                self.output_weights[h][j]=rand(-0.2,0.2)</span><br><span class="line"></span><br><span class="line">    def predict(self,inputs):</span><br><span class="line">        for i in range(self.input_n-1.0):</span><br><span class="line">            self.input_cells[i]=inputs[i]</span><br><span class="line">            </span><br><span class="line">        for j in range(self.hidden_n):</span><br><span class="line">            total+=self.input_cells[i]*self.input_weights[i][j]</span><br><span class="line">        self.hidden_cells[i]=sigmoid(total)#不要忘记去线性化</span><br><span class="line">        </span><br><span class="line">        for k in range(self.output_n):</span><br><span class="line">            total=0.0</span><br><span class="line">            for j in range(self.output_n):</span><br><span class="line">                total+=self.hidden_cells[j]*self.output_weights[j][k]</span><br><span class="line">                self.output_cells[k]=sigmoid(total)</span><br><span class="line">        return self.output_cells[:]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    def back_propagate(self,case,label,learn):</span><br><span class="line">        </span><br><span class="line">        self.predict(case)</span><br><span class="line">        </span><br><span class="line">        output_deltas=[0.0]*self.output_n</span><br><span class="line">        for k in range(self.output_n):</span><br><span class="line">            error=label[k]-self.output_cells[k]</span><br><span class="line">            output_deltas[k]=sigmod_derivate(self.output_cells[k])*error</span><br><span class="line">            </span><br><span class="line">        hidden_deltas=[0.0]*self.hidden_n</span><br><span class="line">        for j in range(self.input_n):</span><br><span class="line">            error=0.0</span><br><span class="line">            for k in range(self.output_n):</span><br><span class="line">                error+=output_deltas[k]*self.output_weights[j][k]</span><br><span class="line">            hidden_deltas[j]=sigmod_derivate(self.hidden_cells[j])*error</span><br><span class="line">            </span><br><span class="line">        for j in range(self.hidden_n):</span><br><span class="line">            for k in range(self.output_n):</span><br><span class="line">                self.output_weights[j][k]+=learn*output_deltas[k]*self.hidden_cells[j]</span><br><span class="line">        </span><br><span class="line">        for i in range(self.input_n):</span><br><span class="line">            for j in range(self.hidden_n):</span><br><span class="line">                self.input_weights[i][j]=learn*hidden_deltas[j]*self.input_cells[i]</span><br><span class="line">                </span><br><span class="line">                error=0</span><br><span class="line">        </span><br><span class="line">        for o in range(len(label)):</span><br><span class="line">            #L1实现</span><br><span class="line">            error+=0.5*(label[o]-self.output_cells[o])**2</span><br><span class="line">        return error</span><br><span class="line">    </span><br><span class="line">    def train(self,cases,labels,limit=100,learn=0.05):</span><br><span class="line">        for i in range(limit):</span><br><span class="line">            error=0</span><br><span class="line">            for i in range(len(cases)):</span><br><span class="line">                label=labels[i]</span><br><span class="line">                case=cases[i]</span><br><span class="line">                error+=self.back_propagate(case,label,learn)</span><br><span class="line">            pass</span><br><span class="line">        </span><br><span class="line">    def test(self):</span><br><span class="line">        cases=[</span><br><span class="line">            [0,0],</span><br><span class="line">            [0,1],</span><br><span class="line">            [1,0],</span><br><span class="line">            [1,1],</span><br><span class="line">        ]</span><br><span class="line">        labels=[[0],[1],[1],[0]]</span><br><span class="line">        dself.setup(2,5,1)</span><br><span class="line">        self.train(cases,labels,10000,0.05)</span><br><span class="line">        for case in cases:</span><br><span class="line">            print(self.predict(case))</span><br><span class="line">            </span><br><span class="line">if __name__==&apos;__mian__&apos;:</span><br><span class="line">    nn=BPNeuralNetWork()</span><br><span class="line">    nn.test()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="4-激活函数的种类以及各自的背景，优缺点"><a href="#4-激活函数的种类以及各自的背景，优缺点" class="headerlink" title="4.激活函数的种类以及各自的背景，优缺点"></a>4.激活函数的种类以及各自的背景，优缺点</h3><h5 id="1-定义"><a href="#1-定义" class="headerlink" title="1,定义"></a>1,定义</h5><p>在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数 Activation Function。</p>
<p>使用激活函数对线性值进行去线性化。</p>
<h5 id="2-为什么要使用激活函数-线性模型的局限性"><a href="#2-为什么要使用激活函数-线性模型的局限性" class="headerlink" title="2.为什么要使用激活函数(线性模型的局限性)"></a>2.为什么要使用激活函数(线性模型的局限性)</h5><p>如果不用激励函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中</p>
<h5 id="3-分类定义"><a href="#3-分类定义" class="headerlink" title="3.分类定义"></a>3.分类定义</h5><h6 id="1-sigmoid函数"><a href="#1-sigmoid函数" class="headerlink" title="(1).sigmoid函数"></a>(1).sigmoid函数</h6><p>公式：<br><img src="/2019/04/21/神经网络基础/nn10.PNG" alt=""></p>
<p>曲线:</p>
<p><img src="/2019/04/21/神经网络基础/nn11.PNG" alt=""></p>
<p>取值范围为(0,1)，用于<strong>隐层神经元输出</strong>。<br>它可以将一个实数映射到(0,1)的区间，可以用来做二分类。<br><strong>在特征相差比较复杂或是相差不是特别大时效果比较好。</strong><br><strong>sigmoid缺点</strong>：</p>
<p>激活函数计算量大，反向传播求误差梯度时，求导涉及除法。反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。</p>
<h4 id="2-Tanh函数"><a href="#2-Tanh函数" class="headerlink" title="(2) Tanh函数"></a>(2) Tanh函数</h4><p>公式：</p>
<p><img src="/2019/04/21/神经网络基础/nn12.PNG" alt=""></p>
<p>曲线:</p>
<p><img src="/2019/04/21/神经网络基础/nn13.PNG" alt=""></p>
<p>也称为双切正切函数<br>取值范围为[-1,1]。<br>tanh在特征相差明显时的效果会很好，在循环过程中会不断扩大特征果。与 sigmoid 的区别是，tanh 是 0 均值的，因此实际应用中 tanh 会比 sigmoid 更好。</p>
<h4 id="3-ReLU"><a href="#3-ReLU" class="headerlink" title="3) ReLU"></a>3) ReLU</h4><p>Rectified Linear Unit(ReLU) - 用于隐层神经元输出</p>
<p>公式：</p>
<p><img src="/2019/04/21/神经网络基础/nn14.PNG" alt=""></p>
<p>曲线</p>
<p><img src="/2019/04/21/神经网络基础/nn15.PNG" alt=""></p>
<p>输入信号 <0 时，输出都是0，="">0 的情况下，输出等于输入</0></p>
<p><strong>ReLU 的优点</strong>：使用 ReLU 得到的 SGD 的收敛速度会比 sigmoid/tanh 快很多</p>
<p><strong>ReLU 的缺点</strong>：<br> 训练的时候很”脆弱”，很容易就”die”了<br> 例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.<br> 如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。</p>
<h3 id="5-深度学习中的正则化"><a href="#5-深度学习中的正则化" class="headerlink" title="5.深度学习中的正则化"></a>5.深度学习中的正则化</h3><h5 id="1-定义-1"><a href="#1-定义-1" class="headerlink" title="1.定义"></a>1.定义</h5><p>《统计学习方法》中认为正则化是<strong>选择模型</strong>的一种方法。</p>
<p>正则化通过对学习算法的修改，旨在减少泛化误差而不是训练误差。目前有很多正则化策略，有些是向机器学习模型中添加限制参数值的额外约束，有些是向目标函数添加额外项来对参数值进行软约束。</p>
<h5 id="2-L1正则化"><a href="#2-L1正则化" class="headerlink" title="2.L1正则化"></a>2.L1正则化</h5><p>L1正则化对梯度的影响不是线性地缩放每个wi而是添加了一项与sign(wi)同号的常数。这种形式的梯度不一定能得到直接算术解。 </p>
<p><img src="/2019/04/21/神经网络基础/nn16.PNG" alt=""></p>
<p>相比L2正则化，L1正则化会产生更稀疏的解。这种稀疏性广泛用于特征选择机制，可以从可用的特征子集中选择出有意义的特征，化简机器学习问题。</p>
<p><strong>L1正则化可通过假设权重w的先验分布为拉普拉斯分布，由最大后验概率估计导出. </strong></p>
<h5 id="3-L2正则化"><a href="#3-L2正则化" class="headerlink" title="3.L2正则化"></a>3.L2正则化</h5><p>通过向目标函数添加一个L2范数平方项，使权重更加接近原点。<br><img src="/2019/04/21/神经网络基础/C:/blog\source\_posts\神经网络基础\nn17.PNG" alt=""></p>
<p>《Deep Learning》书中也提到：<strong>L2正则化能让学习算法“感知”到具有较高方差的输入xx，因此与输出目标的协方差较小的特征的权重将会收缩。</strong>L2正则化能够很好的防止过拟合：</p>
<ol>
<li>通过引入L2正则，使模型参数偏好比较小的值，这其实也限制的函数空间的大小，有针对的减小了模型容量。一般来说，大的参数值对应于波动剧烈的函数，小的参数值对应于比较平缓的参数，因为小参数对于输入的改变不会那么敏感。发生过拟合往往是因为顾及到了所有样本点，所以此时的函数波动会比较大，如下面右图所示。过拟合的模型往往具有比较大的参数，如果将这部分模型族从假设空间中剔除掉，发生过拟合的可能就变小。</li>
</ol>
<p>​       <img src="/2019/04/21/神经网络基础/C:/blog\source\_posts\神经网络基础\nn18.PNG" alt=""></p>
<ol start="2">
<li>L2正则化能让学习算法“感知”到具有较高方差的输入xx，因此与输出目标的协方差较小的特征的权重将会收缩。因此L2正则化总是倾向于对那些训练集样本共用的特征产生较大的响应，而减小对个别样本独有的特征产生的响应。因此L2正则有抑制那些“独有特征”的作用，这在一定程度上也减小了过拟合的风险。<br>　　<strong>L2正则化可通过假设权重ww的先验分布为高斯分布，由最大后验概率估计导出.</strong></li>
</ol>
<h5 id="4-数据集增强"><a href="#4-数据集增强" class="headerlink" title="4.数据集增强"></a>4.数据集增强</h5><ul>
<li>原因：一般而言，比较成功的神经网络需要大量的参数，许许多多的神经网路的参数都是数以百万计，而使得这些参数可以正确工作则需要大量的数据进行训练，而实际情况中数据并没有我们想象中的那么多</li>
<li>作用：<ol>
<li>增加训练的数据量，提高模型的泛化能力</li>
<li>增加噪声数据，提升模型的鲁棒性</li>
</ol>
</li>
<li>方法：<ol>
<li>增加数据集，一般较难实现</li>
<li>利用已有的数据比如翻转、平移或旋转，创造出更多的数据，来使得神经网络具有更好的泛化效果。</li>
</ol>
</li>
<li>分类：<ol>
<li><strong>离线增强</strong> ： 直接对数据集进行处理，数据的数目会变成增强因子 x 原数据集的数目 ，这种方法常常用于数据集很小的时候</li>
<li><strong>在线增强</strong> ： 这种增强的方法用于，获得 batch 数据之后，然后对这个 batch 的数据进行增强，如旋转、平移、翻折等相应的变化，由于有些数据集不能接受线性级别的增长，这种方法长用于大的数据集，很多机器学习框架已经支持了这种数据增强方式，并且可以使用 GPU 优化计算。</li>
</ol>
</li>
</ul>
<h5 id="5-噪声增加"><a href="#5-噪声增加" class="headerlink" title="5.噪声增加"></a>5.噪声增加</h5><p>​    简单提高网络抗噪能力的方法，就是在训练中加入随机噪声一起训练。我们可以在网络的不同位置加入噪声：输入层，隐藏层，输出层。<br>　　数据集增强在某种意义上也能看做是在输入层加入噪声，通过随机旋转、翻转，色彩变换，裁剪等操作人工扩充训练集大小。这样可以使得网络对于输入更加鲁棒。<br>　　当然如果你能保证数据集没错误，不向输出层加入噪声也没关系。解决这个问题常见的办法是<strong>标签平滑</strong>，通过把确切的分类目标从0和1替换成ϵk−1和1−ϵ，正则化具有k个输出的softmax函数的模型。标签平滑的优势是能够防止模型追求确切的概率而不能学习正确分类。<br>　　从优化过程的角度来看，对权重叠加方差极小噪声等价于对权重是假范数惩罚，可以被解释为关于权重的贝叶斯推断的随即实现（这一点我不是很理解）。换个角度就很结论就很明朗了，因为我们在权重中添加了一些随机扰动，这鼓励优化过程找到一个参数空间，该空间对微小参数变化引起的输出变化影像很小。将模型送入了一个对于微小变化不敏感的区域，不仅找到了最小值，还找到了一个宽扁的最小值区域。</p>
<h5 id="early-stop（提前终止"><a href="#early-stop（提前终止" class="headerlink" title="early stop（提前终止)"></a>early stop（提前终止)</h5><p>模型过拟合一般是发生在训练次数过多的情况下，那么只要我们在过拟合之前停止训练即可。这也是深度学习中最常用的正则化形式，主要是因为它的有效性和简单性。提前终止需要验证集损失作为观测指标，当验证集损失开始持续上升时，这时就该停止训练过程了。 </p>
<h5 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h5><p>先介绍集成学习的概念</p>
<ul>
<li><p>集成学习通过结合几个模型降低泛化误差的技术。分别训练几个不同的模型，然后让所有模型表决测试样例的输出，也被称为模型平均。模型平均奏效的原因是不同的模型通常不会再测试集上产生完全相同的误差。 </p>
<p>  假设我们有k个回归模型，每个模型在每个例子上的误差为ϵi，这个误差服从均值为0，方差E[ϵi^2]=v且协方差E[ϵiϵj]=c的多维正态分布。通过所有集成模型的平均预测所得误差是1k∑iϵi，则该集成预测期的平方误差的期望为：</p>
<p>  <img src="/2019/04/21/神经网络基础/C:/blog\source\_posts\神经网络基础\nn19.PNG" alt=""></p>
</li>
</ul>
<p>​      在误差完全相关即c=v的情况下，E[(1/k*∑iϵi)^2]=v，模型平均对提升结果没有任何帮       助；在误差完全不想关即c=0的情况下，集成模型的误差期望为E[(1/k∑iϵi)^2]=(1/k)v。用一  句话说，平均上，集成至少与它任何成员表现的一样好，并且如果成员误差是独立的，集成将显著地比其成员表现的更好。神经网络中随机初始化的差异、小批量的随机选择、超参数的差异或不同输出的非确定性实现往往足以使得集成中的不同成员具有部分独立的误差。 </p>
<ul>
<li><p>Dropout</p>
<p>Dropout是每次训练过程中随机舍弃一些神经元之间的连接</p>
<p><img src="/2019/04/21/神经网络基础/C:/blog\source\_posts\神经网络基础\nn20.PNG" alt=""></p>
</li>
</ul>
<p>Dropout提供了一种廉价的Bagging集成近似，能够训练和评估指数级数量的神经网络。Dropout和Bagging训练不太一样，Bagging所有模型都是独立的，而Dropout是所有模型共享参数，每个模型集成父神经网络参数的不同子集，这中参数共享的方式使得在有限可用内存下表示指数级数量的模型变得可能。<br>　　隐藏单元经过Dropout训练后，它必须学习与不同采样神经元的合作，使得神经元具有更强的健壮性，并驱使神经元通过自身获取到有用的特征，而不是依赖其他神经元去纠正自身的错误。这可以看错对输入内容的信息高度智能化，自适应破坏的一种形式，而不是对输入原始值的破坏。<br>　　Dropout的另一个重要方面是噪声是乘性的。如果是固定规模的加性噪声，那么加了噪声ϵ的ReLU可以简单的学会使hi变得很大（使增加的噪声ϵϵ变得不显著）。乘性噪声就不允许这样病态的去解决噪声鲁棒问题。</p>
<h3 id="深度模型的优化"><a href="#深度模型的优化" class="headerlink" title="深度模型的优化"></a>深度模型的优化</h3><h5 id="1-参数初始化策略"><a href="#1-参数初始化策略" class="headerlink" title="1.参数初始化策略"></a>1.参数初始化策略</h5><p>1.梯度下降算法</p>
<ul>
<li><p>批量梯度下降<strong>(Batch gradient descent)</strong> </p>
<p>批量梯度下降每次学习都使用整个训练集，因此其优点在于每次更新都会朝着正确的方向进行，最后能够保证收敛于极值点(凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点)，但是其缺点在于每次学习时间过长，并且如果训练集很大以至于需要消耗大量的内存，并且全量梯度下降不能进行在线模型参数更新。</p>
</li>
<li><p>随机梯度下降<strong>(Stochastic gradient descent)</strong></p>
<p>随机梯度下降算法每次从训练集中随机选择一个样本来进行学习，即： θ=θ−η⋅∇θJ(θ;xi;yi)</p>
<p>批量梯度下降算法每次都会使用全部训练样本，因此这些计算是冗余的，因为每次都使用完全相同的样本集。而随机梯度下降算法每次只随机选择一个样本来更新模型参数，因此每次的学习是非常快速的，并且可以进行在线更新。</p>
<p>随机梯度下降最大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来优化波动(扰动)</p>
<p><img src="/2019/04/21/神经网络基础/n27.PNG" alt=""></p>
</li>
<li><p>小批量梯度下降<strong>(Mini-batch gradient descent)</strong></p>
<p>Mini-batch 梯度下降综合了 batch 梯度下降与 stochastic 梯度下降，在每次更新速度与更新次数中间取得一个平衡，其每次更新从训练集中随机选择 m,m&lt;n 个样本进行学习，mini-batch梯度下降可以保证收敛性，常用于神经网络中。</p>
</li>
</ul>
<p>以下摘抄自网络:<a href="https://blog.csdn.net/qq_29462849/article/details/80626772" target="_blank" rel="noopener">参数初始化策略</a><br><strong>2.AdaGrad</strong></p>
<p>AdaGrad 算法，如下图所示，独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和的平方根 (Duchi et al., 2011)。具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。在凸优化背景中， AdaGrad 算法具有一些令人满意的理论性质。然而，经验上已经发现，对于训练深度神经网络模型而言， 从训练开始时积累梯度平方会导致有效学习率过早和过量的减小。 AdaGrad 在某些深度学习模型上效果不错，但不是全部</p>
<p><img src="/2019/04/21/神经网络基础/nn23.PNG" alt=""></p>
<p><strong>1.RMSProp</strong></p>
<p>RMSProp 算法 (Hinton, 2012) 修改 AdaGrad 以在非凸设定下效果更好，改<br>变梯度积累为指数加权的移动平均。 AdaGrad 旨在应用于凸问题时快速收敛。当应用于非凸函数训练神经网络时，学习轨迹可能穿过了很多不同的结构，最终到达一个局部是凸碗的区域。 AdaGrad 根据平方梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就变得太小了。 RMSProp 使用指数衰减平均以丢弃遥远过去的历史，使其能够在找到凸碗状结构后快速收敛，它就像一个初始化于该碗状结构的 AdaGrad 算法实例。RMSProp 的标准形式如算法 8.5 所示，结合 Nesterov 动量的形式如算法 8.6 所示。相比于 AdaGrad，使用移动平均引入了一个新的超参数ρ，用来控制移动平均的长度范围。经验上， RMSProp 已被证明是一种有效且实用的深度神经网络优化算法。目前它是深度学习从业者经常采用的优化方法之一。<br><img src="/2019/04/21/神经网络基础/n24.PNG" alt=""></p>
<p><img src="/2019/04/21/神经网络基础/n23.PNG" alt=""></p>
<p><strong>4.Adam</strong></p>
<p>Adam (Kingma and Ba, 2014) 是另一种学习率自适应的优化算法，如算法 8.7 所示。 “Adam’’ 这个名字派生自短语 “adaptive moments’’。早期算法背景下，它也许最好被看作结合 RMSProp 和具有一些重要区别的动量的变种。首先，在 Adam 中，动量直接并入了梯度一阶矩（指数加权）的估计。将动量加入 RMSProp 最直观的方法是将动量应用于缩放后的梯度。结合缩放的动量使用没有明确的理论动机。其次， Adam 包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩的估计（算法 8.7 ）。 RMSProp 也采用了（非中心的）二阶矩估计，然而缺失了修正因子。因此，不像 Adam，RMSProp 二阶矩估计可能在训练初期有很高的偏置。Adam 通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。<br><img src="/2019/04/21/神经网络基础/n26.PNG" alt=""></p>
<h5 id="2-batch-normal层"><a href="#2-batch-normal层" class="headerlink" title="2.batch normal层"></a>2.batch normal层</h5><h5 id="3-layer-normal层"><a href="#3-layer-normal层" class="headerlink" title="3.layer normal层"></a>3.layer normal层</h5>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/17/传统机器学习-LDA/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yu-shui">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yu-shui">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/17/传统机器学习-LDA/" itemprop="url">传统机器学习--LDA</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-17T23:02:46+08:00">
                2019-04-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1-共轭先验分布"><a href="#1-共轭先验分布" class="headerlink" title="1.共轭先验分布"></a>1.共轭先验分布</h3><p>共轭先验分布的提出：某观测数据服从概率分布p(θ），当观测到新的数据时，思考下列问题：</p>
<blockquote>
<p>1.能否根据新观测数据X更新参数θ；<br>2.根据新观测的数据可以在多大的程度上改变参数θ：θ=θ+rθ；<br>3.当重新估计得到θ时，给出的新参数数值θ的新概率分布p(θ|x)；</p>
</blockquote>
<p>分析：根据贝叶斯公式：p(θ|x)=p(x|θ)p(θ)<br>p(x),其中p(x|θ)是在已知θ的情况下估计x的概率分布，又称似然函数；p(θ)是原有的θ的概率分布；要想利用观测到的数据更新参数θ，就要使更新后的p(θ|x)和p(θ)服从相同的分布，所以p(θ)和p(θ|x)形成共轭分布，p(θ)叫做p(θ|x)的共轭先验分布。<br>举个投硬币的例子：使用参数θ的伯努利模型，θ为正面的概率，则结果为x的概率分布为：p(x|θ)=θx(1−θ)1-x</p>
<p>在贝叶斯概率理论中，如果后验概率P(θ|X)和先验概率P(θ)满足同样的分布律(形式相同，参数不同)。那么，先验分布和后验分布被叫做共轭分布，同时，先验分布叫做似然函数的共轭先验分布。</p>
<p><img src="/2019/04/17/传统机器学习-LDA/kk.PNG" alt=""></p>
<h3 id="2-pLSA"><a href="#2-pLSA" class="headerlink" title="2.pLSA"></a>2.pLSA</h3><p><a href="https://baike.baidu.com/item/LSA" target="_blank" rel="noopener"><strong>LSA</strong></a>是处理这类问题的著名技术。其主要思想就是映射高维向量到潜在语义空间，使其降维。LSA的目标就是要寻找到能够很好解决实体间词法和语义关系的数据映射。正是由于这些特性，使得LSA成为相当有价值并被广泛应用的分析工具。PLSA是以统计学的角度来看待<a href="https://baike.baidu.com/item/LSA" target="_blank" rel="noopener">LSA</a>，相比于标准的LSA，他的概率学变种有着更巨大的影响。</p>
<p>概率潜在语义分析（pLSA）  基于双模式和共现的数据分析方法延伸的经典的统计学方法。 </p>
<p>概率潜在语义分析应用于信息检索，过滤，自然语言处理，文本的机器学习或者其他相关领域。概率潜在语义分析与标准潜在语义分析的不同是，标准潜在语义分析是以共现表（就是共现的矩阵）的奇异值分解的形式表现的，而概率潜在语义分析却是基于派生自LCM的混合矩阵分解。考虑到word和doc共现形式，概率潜在语义分析基于多项式分布和条件分布的混合来建模共现的概率。所谓共现其实就是W和D的一个矩阵，所谓双模式就是在W和D上同时进行考虑。</p>
<p>PLSA有时会出现过拟合的现象。所谓过拟合（Overfit），是这样一种现象：一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集上却不能很好的拟合数据。此时我们就叫这个假设出现了overfit的现象。出现这种现象的主要原因是训练数据中存在噪音或者训练数据太少。</p>
<p>解决办法，要避免过拟合的问题，PLSA使用了一种广泛应用的最大似然估计的方法，期望最大化。PLSA中训练参数的值会随着文档的数目线性递增。PLSA可以生成其所在数据集的的文档的模型，但却不能生成新文档的模型。</p>
<p>数学推导待补充（看不懂…..)</p>
<h3 id="3-LDA主题模型原理"><a href="#3-LDA主题模型原理" class="headerlink" title="3.LDA主题模型原理"></a>3.LDA主题模型原理</h3><p>待补充（看不懂…..)</p>
<h3 id="4-LDA参数学习–Gibbs采样训练流程"><a href="#4-LDA参数学习–Gibbs采样训练流程" class="headerlink" title="4.LDA参数学习–Gibbs采样训练流程"></a>4.LDA参数学习–Gibbs采样训练流程</h3><ol>
<li>选择合适的主题数K，选择合适的超参数α、η；</li>
<li>对于语料库中每一篇文档的每一个词，随机的赋予一个主题编号z；</li>
<li>重新扫描语料库，对于每一个词，利用Gibbs采样公式更新它的topic的编号，并更新语料库中该词的编号。</li>
<li>重复第三步中基于坐标轴轮询的Gibbs采样，直到Gibbs采样收敛。</li>
<li>统计语料库中各个文档各个词的主题，得到文档主题分布</li>
</ol>
<h3 id="5-LDA生成主题特征，在之前的特征的基础上加入主题特征实现文本分类"><a href="#5-LDA生成主题特征，在之前的特征的基础上加入主题特征实现文本分类" class="headerlink" title="5.LDA生成主题特征，在之前的特征的基础上加入主题特征实现文本分类"></a>5.LDA生成主题特征，在之前的特征的基础上加入主题特征实现文本分类</h3><h5 id="1-先来了解数据集"><a href="#1-先来了解数据集" class="headerlink" title="1.先来了解数据集"></a>1.先来了解数据集</h5><p>数据集位于lda安装目录的tests文件夹中，包含三个文件：reuters.ldac, reuters.titles, reuters.tokens。<br>reuters.titles包含了395个文档的标题<br>reuters.tokens包含了这395个文档中出现的所有单词，总共是4258个<br>reuters.ldac有395行，第i行代表第i个文档中各个词汇出现的频率。以第0行为例，第0行代表的是第0个文档，从reuters.titles中可查到该文档的标题为“UK: Prince Charles spearheads British royal revolution. LONDON 1996-08-20”。<br>第0行的数据为：<br>159 0:1 2:1 6:1 9:1 12:5 13:2 20:1 21:4 24:2 29:1 ……<br>第一个数字159表示第0个文档里总共出现了159个单词（每个单词出现一或多次），<br>0:1表示第0个单词出现了1次，从reuters.tokens查到第0个单词为church<br>2:1表示第2个单词出现了1次，从reuters.tokens查到第2个单词为years<br>6:1表示第6个单词出现了1次，从reuters.tokens查到第6个单词为told<br>9:1表示第9个单词出现了1次，从reuters.tokens查到第9个单词为year<br>12:5表示第12个单词出现了5次，从reuters.tokens查到第12个单词为charles<br>……<br>这里第1、3、4、5、7、8、10、11……个单词序号和次数没列出来，表示出现的次数为0<br>注意：<br>395个文档的原文是没有的。上述三个文档是根据这395个文档处理之后得到的。</p>
<p><img src="/2019/04/17/传统机器学习-LDA/kk2.PNG" alt=""></p>
<h5 id="2代码实现"><a href="#2代码实现" class="headerlink" title="2代码实现"></a>2代码实现</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#查看文本中词出先的频率</span><br><span class="line">import numpy as np</span><br><span class="line">import lda</span><br><span class="line">import lda.datasets</span><br><span class="line"></span><br><span class="line">X=lda.datasets.load_reuters()</span><br><span class="line">print(&quot;type(X):&#123;&#125;&quot;.format(type(X)))</span><br><span class="line">print(&quot;shape:&#123;&#125;\n&quot;.format(X.shape))</span><br><span class="line"></span><br><span class="line">print(X[:5,:5])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/kk3.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#查看词</span><br><span class="line">vocab=lda.datasets.load_reuters_vocab()</span><br><span class="line">print(&quot;type(vocab):&#123;&#125;&quot;.format(type(vocab)))</span><br><span class="line">print(&quot;len(vocab):&#123;&#125;\n&quot;.format(len(vocab)))</span><br><span class="line">print(vocab[:5])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/kk4.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#查看文档标题</span><br><span class="line">titles=lda.datasets.load_reuters_titles()</span><br><span class="line">print(&quot;type(title):&#123;&#125;&quot;.format(type(titles)))</span><br><span class="line">print(&quot;len(titles):&#123;&#125;&quot;.format(len(titles)))</span><br><span class="line">print(titles[:5])</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/kk5.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#查看前五个文档第0个词出现的次数</span><br><span class="line">doc_id=0</span><br><span class="line">word_id=0</span><br><span class="line">while doc_id &lt;5:</span><br><span class="line">    print(&quot;doc id:&#123;&#125; word id:&#123;&#125;&quot;.format(doc_id,word_id))</span><br><span class="line">    print(&quot;--count:&#123;&#125;&quot;.format(X[doc_id,word_id]))#doc_id是文档的标记</span><br><span class="line">    print(&quot;--word:&#123;&#125;&quot;.format(vocab[word_id]))</span><br><span class="line">    print(&quot;--doc :&#123;&#125;\n&quot;.format(titles[doc_id]))</span><br><span class="line">    doc_id+=1</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/kk6.PNG" alt=""></p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#训练模型</span><br><span class="line">model=lda.LDA(n_topics=20,n_iter=500,random_state=1)</span><br><span class="line">model.fit(X)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/kk7.PNG" alt=""></p>
<p>#主题-单词分布</p>
<p>#计算前三个单词在所有的主题中所占的权重<br>topic_word=model.topic_word_<br>print(“type(topic_word):{}”.format(type(topic_word)))<br>print(“shape:{}”.format(topic_word.shape))<br>print(vocab[:3])<br>print(topic_word[:,:3])</p>
<p><img src="/2019/04/17/传统机器学习-LDA/kk8.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#计算所有行的比重之和</span><br><span class="line">for n in range(20):</span><br><span class="line">    sum_pr=sum(topic_word[n,:])</span><br><span class="line">    print(&quot;topic:&#123;&#125; sum:&#123;&#125;&quot;.format(n,sum_pr))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/kk9.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#计算各个主题的topN个词</span><br><span class="line">n=5</span><br><span class="line">for i,topic_dist in enumerate(topic_word):</span><br><span class="line">    topic_words=np.array(vocab)[np.argsort(topic_dist)[:-(n+1):-1]]</span><br><span class="line">    print(&quot;top &#123;&#125;\n- &#123;&#125;&quot;.format(i,&apos; &apos;.join(topic_words)))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/kk10.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#计算前十篇文章最有可能的主题</span><br><span class="line"></span><br><span class="line">doc_topic=model.doc_topic_</span><br><span class="line">print(&quot;ytpr(doc_topic):&#123;&#125;&quot;.format(type(doc_topic)))</span><br><span class="line">print(&quot;shape:&#123;&#125;&quot;.format(doc_topic.shape))</span><br><span class="line">for n in range(10):</span><br><span class="line">    topic_most_pr=doc_topic[n].argmax()</span><br><span class="line">    print(&quot;doc:&#123;&#125; topic :&#123;&#125;&quot;.format(n,topic_most_pr))</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/C:/blog\source\_posts\传统机器学习-LDA\kk11.PNG" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#可视化分析</span><br><span class="line">#绘制主题0，5，9，14的词的出现次数</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">f,ax=plt.subplots(5,1,figsize=(8,6),sharex=True)</span><br><span class="line">for i,k in enumerate([0,5,9,14,19]):#枚举</span><br><span class="line">    print(i,k)</span><br><span class="line">    ax[i].stem(topic_word[k,:],linefmt=&apos;b-&apos;, markerfmt=&apos;bo&apos;, basefmt=&apos;w-&apos;)</span><br><span class="line">    </span><br><span class="line">    ax[i].set_xlim(-50, 4350)</span><br><span class="line">    ax[i].set_ylim(0, 0.08)</span><br><span class="line">    ax[i].set_ylabel(&quot;Prob&quot;)</span><br><span class="line">    ax[i].set_title(&quot;topic &#123;&#125;&quot;.format(k))</span><br><span class="line">ax[4].set_xlabel(&quot;word&quot;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/04/17/传统机器学习-LDA/C:/blog\source\_posts\传统机器学习-LDA\kk12.PNG" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Yu-shui</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">40</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Yu-shui" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yu-shui</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count"></span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
